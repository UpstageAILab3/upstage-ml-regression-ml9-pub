{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library + Data Setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 나눔 폰트 경로 설정\n",
    "font_path = '/Library/Fonts/NanumGothic.ttf'\n",
    "\n",
    "# 나눔 폰트 설정\n",
    "fe = fm.FontEntry(\n",
    "    fname=font_path,  # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumGothic'  # 이 폰트의 원하는 이름 설정\n",
    ")\n",
    "fm.fontManager.ttflist.insert(0, fe)  # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumGothic'})  # 폰트 설정\n",
    "plt.rc('font', family='NanumGothic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train_cho.csv')\n",
    "test_data = pd.read_csv('./data/test_cho.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train_final_1.csv')\n",
    "test_data = pd.read_csv('./data/test_final_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['아파트명', '전용면적', '계약년월', '계약일', '층', '건축년도', '도로명', 'k-단지분류(아파트,주상복합등등)',\n",
       "       'k-세대타입(분양형태)', 'k-관리방식', 'k-복도유형', 'k-난방방식', 'k-전체동수', 'k-전체세대수',\n",
       "       'k-건설사(시공사)', 'k-시행사', 'k-연면적', 'k-주거전용면적', 'k-전용면적별세대현황(60㎡이하)',\n",
       "       'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하', 'k-135㎡초과', '건축면적', '주차대수',\n",
       "       '기타/의무/임대/임의=1/2/3/4', '좌표X', '좌표Y', 'target', 'distance_score', '구',\n",
       "       '동', '계약연도', '계약월', '급지', 'address', '신축여부', '이자율', 'is_top20',\n",
       "       '대장아파트_거리'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 특수 문자 제거 또는 대체 함수\n",
    "def clean_column_name(name):\n",
    "   return re.sub(r'[^a-zA-Z0-9가-힣]', '_', name)\n",
    "\n",
    "# 모든 열 이름을 정리\n",
    "train_data.columns = [clean_column_name(col) for col in train_data.columns]\n",
    "test_data.columns = [clean_column_name(col) for col in test_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['아파트명', '전용면적', '계약년월', '계약일', '층', '건축년도', '도로명', 'k_단지분류_아파트_주상복합등등_',\n",
       "       'k_세대타입_분양형태_', 'k_관리방식', 'k_복도유형', 'k_난방방식', 'k_전체동수', 'k_전체세대수',\n",
       "       'k_건설사_시공사_', 'k_시행사', 'k_연면적', 'k_주거전용면적', 'k_전용면적별세대현황_60_이하_',\n",
       "       'k_전용면적별세대현황_60__85_이하_', 'k_85__135_이하', 'k_135_초과', '건축면적', '주차대수',\n",
       "       '기타_의무_임대_임의_1_2_3_4', '좌표X', '좌표Y', 'target', 'distance_score', '구',\n",
       "       '동', '계약연도', '계약월', '급지', 'address', '신축여부', '이자율', 'is_top20',\n",
       "       '대장아파트_거리'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1118822, 39)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 범주형/연속형/전용면적 로그 처리등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 변수: ['전용면적', '계약년월', '계약일', '층', '건축년도', 'k_전체동수', 'k_전체세대수', 'k_연면적', 'k_주거전용면적', 'k_전용면적별세대현황_60_이하_', 'k_전용면적별세대현황_60__85_이하_', 'k_85__135_이하', 'k_135_초과', '건축면적', '주차대수', '좌표X', '좌표Y', 'target', 'distance_score', '계약연도', '계약월', '신축여부', '이자율', 'is_top20', '대장아파트_거리']\n",
      "범주형 변수: ['아파트명', '도로명', 'k_단지분류_아파트_주상복합등등_', 'k_세대타입_분양형태_', 'k_관리방식', 'k_복도유형', 'k_난방방식', 'k_건설사_시공사_', 'k_시행사', '기타_의무_임대_임의_1_2_3_4', '구', '동', '급지', 'address']\n"
     ]
    }
   ],
   "source": [
    "continuous_columns_v2 = []\n",
    "categorical_columns_v2 = []\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if pd.api.types.is_numeric_dtype(train_data[column]):\n",
    "        continuous_columns_v2.append(column)\n",
    "    else:\n",
    "        categorical_columns_v2.append(column)\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns_v2)\n",
    "print(\"범주형 변수:\", categorical_columns_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test 데이터를 합쳐서 하나의 데이터프레임으로 만듭니다.\n",
    "combined_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "#전용면적 로그 변환\n",
    "combined_data['전용면적'] = np.log1p(combined_data['전용면적'])\n",
    "\n",
    "# 각 범주형 변수에 대해 LabelEncoder를 적용합니다.\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns_v2:\n",
    "    lbl = LabelEncoder()\n",
    "    combined_data[col] = combined_data[col].astype(str)  # 데이터 타입을 문자열로 변환\n",
    "    combined_data[col] = lbl.fit_transform(combined_data[col])\n",
    "    label_encoders[col] = lbl  # 나중에 후처리를 위해 레이블인코더를 저장해둡니다.\n",
    "\n",
    "# 인코딩된 데이터를 다시 train과 test 데이터로 나눕니다.\n",
    "train_data_encoded = combined_data.iloc[:len(train_data)]\n",
    "test_data_encoded = combined_data.iloc[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9272, 39)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and test sets...\n",
      "Creating LightGBM dataset...\n",
      "Training the final model...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5331\n",
      "[LightGBM] [Info] Number of data points in the train set: 895057, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 58003.281782\n",
      "Iteration 1: Train RMSE = 42946.20594246705, Valid RMSE = 43217.78038602651\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's rmse: 24956.1\tvalid's rmse: 25226.3\n",
      "[20]\ttrain's rmse: 17755.7\tvalid's rmse: 18009.3\n",
      "[30]\ttrain's rmse: 14640.6\tvalid's rmse: 14880.3\n",
      "[40]\ttrain's rmse: 13085.1\tvalid's rmse: 13321\n",
      "[50]\ttrain's rmse: 12145\tvalid's rmse: 12382.2\n",
      "[60]\ttrain's rmse: 11499\tvalid's rmse: 11743.8\n",
      "[70]\ttrain's rmse: 11002.5\tvalid's rmse: 11257.8\n",
      "[80]\ttrain's rmse: 10627.4\tvalid's rmse: 10899\n",
      "[90]\ttrain's rmse: 10285.1\tvalid's rmse: 10575.1\n",
      "[100]\ttrain's rmse: 10029.8\tvalid's rmse: 10335.5\n",
      "[110]\ttrain's rmse: 9790.61\tvalid's rmse: 10110.4\n",
      "[120]\ttrain's rmse: 9615.75\tvalid's rmse: 9949.39\n",
      "[130]\ttrain's rmse: 9423.99\tvalid's rmse: 9776.89\n",
      "[140]\ttrain's rmse: 9260.63\tvalid's rmse: 9626.28\n",
      "[150]\ttrain's rmse: 9108.18\tvalid's rmse: 9487.37\n",
      "[160]\ttrain's rmse: 8968.24\tvalid's rmse: 9354.67\n",
      "[170]\ttrain's rmse: 8855.4\tvalid's rmse: 9248.77\n",
      "[180]\ttrain's rmse: 8745.77\tvalid's rmse: 9149.1\n",
      "[190]\ttrain's rmse: 8651.17\tvalid's rmse: 9064.67\n",
      "[200]\ttrain's rmse: 8544.27\tvalid's rmse: 8968.16\n",
      "[210]\ttrain's rmse: 8454.82\tvalid's rmse: 8889.96\n",
      "[220]\ttrain's rmse: 8361.44\tvalid's rmse: 8807.85\n",
      "[230]\ttrain's rmse: 8284.63\tvalid's rmse: 8740.75\n",
      "[240]\ttrain's rmse: 8208.77\tvalid's rmse: 8673.08\n",
      "[250]\ttrain's rmse: 8121.98\tvalid's rmse: 8596.74\n",
      "[260]\ttrain's rmse: 8052.72\tvalid's rmse: 8540.47\n",
      "[270]\ttrain's rmse: 7978.46\tvalid's rmse: 8473.66\n",
      "[280]\ttrain's rmse: 7931.19\tvalid's rmse: 8434.36\n",
      "[290]\ttrain's rmse: 7871.72\tvalid's rmse: 8382.34\n",
      "[300]\ttrain's rmse: 7811.51\tvalid's rmse: 8330.62\n",
      "[310]\ttrain's rmse: 7743.32\tvalid's rmse: 8268.6\n",
      "[320]\ttrain's rmse: 7692.57\tvalid's rmse: 8225.24\n",
      "[330]\ttrain's rmse: 7636.86\tvalid's rmse: 8174.35\n",
      "[340]\ttrain's rmse: 7585.2\tvalid's rmse: 8129.92\n",
      "[350]\ttrain's rmse: 7527.2\tvalid's rmse: 8083.08\n",
      "[360]\ttrain's rmse: 7475.93\tvalid's rmse: 8040.73\n",
      "[370]\ttrain's rmse: 7435.54\tvalid's rmse: 8004.49\n",
      "[380]\ttrain's rmse: 7378.86\tvalid's rmse: 7953.44\n",
      "[390]\ttrain's rmse: 7337.91\tvalid's rmse: 7918.19\n",
      "[400]\ttrain's rmse: 7297.04\tvalid's rmse: 7883.8\n",
      "[410]\ttrain's rmse: 7257.04\tvalid's rmse: 7851.51\n",
      "[420]\ttrain's rmse: 7224.17\tvalid's rmse: 7824.52\n",
      "[430]\ttrain's rmse: 7187.46\tvalid's rmse: 7794.32\n",
      "[440]\ttrain's rmse: 7150.42\tvalid's rmse: 7763.24\n",
      "[450]\ttrain's rmse: 7108.3\tvalid's rmse: 7727.21\n",
      "[460]\ttrain's rmse: 7073.85\tvalid's rmse: 7699.72\n",
      "[470]\ttrain's rmse: 7040.48\tvalid's rmse: 7669.81\n",
      "[480]\ttrain's rmse: 7003.35\tvalid's rmse: 7639.11\n",
      "[490]\ttrain's rmse: 6969.34\tvalid's rmse: 7609.48\n",
      "[500]\ttrain's rmse: 6943.59\tvalid's rmse: 7589.65\n",
      "[510]\ttrain's rmse: 6909.52\tvalid's rmse: 7560.34\n",
      "[520]\ttrain's rmse: 6875.22\tvalid's rmse: 7534.53\n",
      "[530]\ttrain's rmse: 6839.58\tvalid's rmse: 7502.67\n",
      "[540]\ttrain's rmse: 6808.74\tvalid's rmse: 7477.66\n",
      "[550]\ttrain's rmse: 6776.66\tvalid's rmse: 7452.5\n",
      "[560]\ttrain's rmse: 6747.46\tvalid's rmse: 7430.46\n",
      "[570]\ttrain's rmse: 6720.88\tvalid's rmse: 7408.37\n",
      "[580]\ttrain's rmse: 6691.18\tvalid's rmse: 7385.51\n",
      "[590]\ttrain's rmse: 6659.6\tvalid's rmse: 7361.06\n",
      "[600]\ttrain's rmse: 6630.21\tvalid's rmse: 7336.77\n",
      "[610]\ttrain's rmse: 6603.82\tvalid's rmse: 7316.53\n",
      "[620]\ttrain's rmse: 6581.79\tvalid's rmse: 7299.27\n",
      "[630]\ttrain's rmse: 6558.59\tvalid's rmse: 7280.25\n",
      "[640]\ttrain's rmse: 6540.14\tvalid's rmse: 7267.14\n",
      "[650]\ttrain's rmse: 6520.25\tvalid's rmse: 7251.76\n",
      "[660]\ttrain's rmse: 6501.7\tvalid's rmse: 7238.3\n",
      "[670]\ttrain's rmse: 6478.48\tvalid's rmse: 7221.38\n",
      "[680]\ttrain's rmse: 6459.97\tvalid's rmse: 7209.67\n",
      "[690]\ttrain's rmse: 6440.33\tvalid's rmse: 7197.29\n",
      "[700]\ttrain's rmse: 6417.1\tvalid's rmse: 7178.74\n",
      "[710]\ttrain's rmse: 6396.98\tvalid's rmse: 7165.74\n",
      "[720]\ttrain's rmse: 6373.39\tvalid's rmse: 7147.77\n",
      "[730]\ttrain's rmse: 6351.43\tvalid's rmse: 7132.06\n",
      "[740]\ttrain's rmse: 6333.94\tvalid's rmse: 7120.16\n",
      "[750]\ttrain's rmse: 6310.19\tvalid's rmse: 7102.49\n",
      "[760]\ttrain's rmse: 6283.11\tvalid's rmse: 7081.47\n",
      "[770]\ttrain's rmse: 6266.5\tvalid's rmse: 7071.41\n",
      "[780]\ttrain's rmse: 6250.02\tvalid's rmse: 7060.73\n",
      "[790]\ttrain's rmse: 6228.59\tvalid's rmse: 7042.88\n",
      "[800]\ttrain's rmse: 6212.25\tvalid's rmse: 7033.22\n",
      "[810]\ttrain's rmse: 6197.1\tvalid's rmse: 7020.32\n",
      "[820]\ttrain's rmse: 6174.25\tvalid's rmse: 7001.98\n",
      "[830]\ttrain's rmse: 6160.35\tvalid's rmse: 6991.42\n",
      "[840]\ttrain's rmse: 6144.05\tvalid's rmse: 6979.54\n",
      "[850]\ttrain's rmse: 6128.2\tvalid's rmse: 6967.32\n",
      "[860]\ttrain's rmse: 6111.39\tvalid's rmse: 6955.62\n",
      "[870]\ttrain's rmse: 6094.71\tvalid's rmse: 6942.86\n",
      "[880]\ttrain's rmse: 6076.31\tvalid's rmse: 6933.83\n",
      "[890]\ttrain's rmse: 6061.33\tvalid's rmse: 6923.12\n",
      "[900]\ttrain's rmse: 6043.63\tvalid's rmse: 6910.22\n",
      "[910]\ttrain's rmse: 6026.22\tvalid's rmse: 6900.94\n",
      "[920]\ttrain's rmse: 6010.68\tvalid's rmse: 6889.89\n",
      "[930]\ttrain's rmse: 5991.85\tvalid's rmse: 6880.67\n",
      "[940]\ttrain's rmse: 5974.9\tvalid's rmse: 6869.66\n",
      "[950]\ttrain's rmse: 5961.73\tvalid's rmse: 6859.37\n",
      "[960]\ttrain's rmse: 5947.01\tvalid's rmse: 6848.64\n",
      "[970]\ttrain's rmse: 5929.93\tvalid's rmse: 6835.72\n",
      "[980]\ttrain's rmse: 5917.45\tvalid's rmse: 6828.44\n",
      "[990]\ttrain's rmse: 5903.27\tvalid's rmse: 6818.63\n",
      "[1000]\ttrain's rmse: 5884.24\tvalid's rmse: 6804.2\n",
      "Iteration 1001: Train RMSE = 5883.341265186679, Valid RMSE = 6803.488363975799\n",
      "[1010]\ttrain's rmse: 5872.77\tvalid's rmse: 6796.73\n",
      "[1020]\ttrain's rmse: 5860.15\tvalid's rmse: 6788.5\n",
      "[1030]\ttrain's rmse: 5846.45\tvalid's rmse: 6778.17\n",
      "[1040]\ttrain's rmse: 5830.95\tvalid's rmse: 6766.2\n",
      "[1050]\ttrain's rmse: 5814.47\tvalid's rmse: 6755.43\n",
      "[1060]\ttrain's rmse: 5800.68\tvalid's rmse: 6744.7\n",
      "[1070]\ttrain's rmse: 5788.96\tvalid's rmse: 6736.64\n",
      "[1080]\ttrain's rmse: 5776.29\tvalid's rmse: 6728.08\n",
      "[1090]\ttrain's rmse: 5763.35\tvalid's rmse: 6721.68\n",
      "[1100]\ttrain's rmse: 5750.18\tvalid's rmse: 6711.37\n",
      "[1110]\ttrain's rmse: 5738.22\tvalid's rmse: 6702.04\n",
      "[1120]\ttrain's rmse: 5725.71\tvalid's rmse: 6691.51\n",
      "[1130]\ttrain's rmse: 5717.04\tvalid's rmse: 6685.59\n",
      "[1140]\ttrain's rmse: 5704.79\tvalid's rmse: 6677.58\n",
      "[1150]\ttrain's rmse: 5692.39\tvalid's rmse: 6670.34\n",
      "[1160]\ttrain's rmse: 5679.74\tvalid's rmse: 6663.7\n",
      "[1170]\ttrain's rmse: 5669.18\tvalid's rmse: 6656.3\n",
      "[1180]\ttrain's rmse: 5658.91\tvalid's rmse: 6649.95\n",
      "[1190]\ttrain's rmse: 5647.35\tvalid's rmse: 6642.87\n",
      "[1200]\ttrain's rmse: 5633\tvalid's rmse: 6635.5\n",
      "[1210]\ttrain's rmse: 5620.92\tvalid's rmse: 6628.57\n",
      "[1220]\ttrain's rmse: 5611.04\tvalid's rmse: 6621.01\n",
      "[1230]\ttrain's rmse: 5599.18\tvalid's rmse: 6614.68\n",
      "[1240]\ttrain's rmse: 5593.18\tvalid's rmse: 6610.26\n",
      "[1250]\ttrain's rmse: 5581.48\tvalid's rmse: 6602.51\n",
      "[1260]\ttrain's rmse: 5572.76\tvalid's rmse: 6596.38\n",
      "[1270]\ttrain's rmse: 5563.08\tvalid's rmse: 6592.37\n",
      "[1280]\ttrain's rmse: 5552.68\tvalid's rmse: 6584.59\n",
      "[1290]\ttrain's rmse: 5543.53\tvalid's rmse: 6578.92\n",
      "[1300]\ttrain's rmse: 5536.91\tvalid's rmse: 6575.75\n",
      "[1310]\ttrain's rmse: 5528.2\tvalid's rmse: 6570.43\n",
      "[1320]\ttrain's rmse: 5516.77\tvalid's rmse: 6564.99\n",
      "[1330]\ttrain's rmse: 5509\tvalid's rmse: 6560.1\n",
      "[1340]\ttrain's rmse: 5499.63\tvalid's rmse: 6555.68\n",
      "[1350]\ttrain's rmse: 5491.6\tvalid's rmse: 6549.75\n",
      "[1360]\ttrain's rmse: 5483.02\tvalid's rmse: 6544.41\n",
      "[1370]\ttrain's rmse: 5473.6\tvalid's rmse: 6537.17\n",
      "[1380]\ttrain's rmse: 5462.08\tvalid's rmse: 6529.12\n",
      "[1390]\ttrain's rmse: 5451.61\tvalid's rmse: 6522.99\n",
      "[1400]\ttrain's rmse: 5442.83\tvalid's rmse: 6516.69\n",
      "[1410]\ttrain's rmse: 5432.14\tvalid's rmse: 6509.96\n",
      "[1420]\ttrain's rmse: 5422.86\tvalid's rmse: 6503.74\n",
      "[1430]\ttrain's rmse: 5414.42\tvalid's rmse: 6498.94\n",
      "[1440]\ttrain's rmse: 5405.4\tvalid's rmse: 6492.5\n",
      "[1450]\ttrain's rmse: 5396.48\tvalid's rmse: 6486.68\n",
      "[1460]\ttrain's rmse: 5386.16\tvalid's rmse: 6479.91\n",
      "[1470]\ttrain's rmse: 5376.89\tvalid's rmse: 6476\n",
      "[1480]\ttrain's rmse: 5367.85\tvalid's rmse: 6470.8\n",
      "[1490]\ttrain's rmse: 5358.5\tvalid's rmse: 6464.25\n",
      "[1500]\ttrain's rmse: 5350.02\tvalid's rmse: 6458\n",
      "[1510]\ttrain's rmse: 5340.19\tvalid's rmse: 6452.95\n",
      "[1520]\ttrain's rmse: 5332.58\tvalid's rmse: 6447.42\n",
      "[1530]\ttrain's rmse: 5324.36\tvalid's rmse: 6442.04\n",
      "[1540]\ttrain's rmse: 5317.74\tvalid's rmse: 6439.39\n",
      "[1550]\ttrain's rmse: 5310.67\tvalid's rmse: 6435.63\n",
      "[1560]\ttrain's rmse: 5304.09\tvalid's rmse: 6433.07\n",
      "[1570]\ttrain's rmse: 5296.38\tvalid's rmse: 6427.44\n",
      "[1580]\ttrain's rmse: 5289.26\tvalid's rmse: 6422.81\n",
      "[1590]\ttrain's rmse: 5280.28\tvalid's rmse: 6417.2\n",
      "[1600]\ttrain's rmse: 5272.52\tvalid's rmse: 6412.39\n",
      "[1610]\ttrain's rmse: 5266.67\tvalid's rmse: 6408.1\n",
      "[1620]\ttrain's rmse: 5259.56\tvalid's rmse: 6403.93\n",
      "[1630]\ttrain's rmse: 5249.72\tvalid's rmse: 6398.06\n",
      "[1640]\ttrain's rmse: 5243.11\tvalid's rmse: 6394.16\n",
      "[1650]\ttrain's rmse: 5233.54\tvalid's rmse: 6389.54\n",
      "[1660]\ttrain's rmse: 5227.75\tvalid's rmse: 6385.22\n",
      "[1670]\ttrain's rmse: 5219.41\tvalid's rmse: 6381.69\n",
      "[1680]\ttrain's rmse: 5213.24\tvalid's rmse: 6378.25\n",
      "[1690]\ttrain's rmse: 5207.1\tvalid's rmse: 6375.5\n",
      "[1700]\ttrain's rmse: 5202.14\tvalid's rmse: 6373.91\n",
      "[1710]\ttrain's rmse: 5194.95\tvalid's rmse: 6369.56\n",
      "[1720]\ttrain's rmse: 5187.97\tvalid's rmse: 6364.87\n",
      "[1730]\ttrain's rmse: 5177.27\tvalid's rmse: 6360.15\n",
      "[1740]\ttrain's rmse: 5170.54\tvalid's rmse: 6356.08\n",
      "[1750]\ttrain's rmse: 5163.96\tvalid's rmse: 6352.76\n",
      "[1760]\ttrain's rmse: 5157.92\tvalid's rmse: 6349.54\n",
      "[1770]\ttrain's rmse: 5150.09\tvalid's rmse: 6345.01\n",
      "[1780]\ttrain's rmse: 5142.47\tvalid's rmse: 6340.93\n",
      "[1790]\ttrain's rmse: 5133.06\tvalid's rmse: 6334.99\n",
      "[1800]\ttrain's rmse: 5124.06\tvalid's rmse: 6329.59\n",
      "[1810]\ttrain's rmse: 5117.52\tvalid's rmse: 6325.66\n",
      "[1820]\ttrain's rmse: 5112.17\tvalid's rmse: 6323.33\n",
      "[1830]\ttrain's rmse: 5103.06\tvalid's rmse: 6318.22\n",
      "[1840]\ttrain's rmse: 5097.17\tvalid's rmse: 6314.97\n",
      "[1850]\ttrain's rmse: 5091.94\tvalid's rmse: 6311.76\n",
      "[1860]\ttrain's rmse: 5084.22\tvalid's rmse: 6306.68\n",
      "[1870]\ttrain's rmse: 5076.9\tvalid's rmse: 6303.03\n",
      "[1880]\ttrain's rmse: 5069.41\tvalid's rmse: 6298.32\n",
      "[1890]\ttrain's rmse: 5063.34\tvalid's rmse: 6294.81\n",
      "[1900]\ttrain's rmse: 5055.71\tvalid's rmse: 6290.52\n",
      "[1910]\ttrain's rmse: 5049.61\tvalid's rmse: 6286.69\n",
      "[1920]\ttrain's rmse: 5043.06\tvalid's rmse: 6284.03\n",
      "[1930]\ttrain's rmse: 5036.9\tvalid's rmse: 6281.19\n",
      "[1940]\ttrain's rmse: 5028.8\tvalid's rmse: 6275.9\n",
      "[1950]\ttrain's rmse: 5023.22\tvalid's rmse: 6271.44\n",
      "[1960]\ttrain's rmse: 5017.23\tvalid's rmse: 6267.92\n",
      "[1970]\ttrain's rmse: 5010.13\tvalid's rmse: 6266.5\n",
      "[1980]\ttrain's rmse: 5002.92\tvalid's rmse: 6263.08\n",
      "[1990]\ttrain's rmse: 4996.34\tvalid's rmse: 6259.2\n",
      "[2000]\ttrain's rmse: 4991.04\tvalid's rmse: 6255.69\n",
      "Iteration 2001: Train RMSE = 4990.585412203398, Valid RMSE = 6255.491728666712\n",
      "[2010]\ttrain's rmse: 4984.59\tvalid's rmse: 6251.7\n",
      "[2020]\ttrain's rmse: 4978.95\tvalid's rmse: 6248.5\n",
      "[2030]\ttrain's rmse: 4972.87\tvalid's rmse: 6245.76\n",
      "[2040]\ttrain's rmse: 4966.88\tvalid's rmse: 6243.67\n",
      "[2050]\ttrain's rmse: 4961.96\tvalid's rmse: 6242.01\n",
      "[2060]\ttrain's rmse: 4957.21\tvalid's rmse: 6239.37\n",
      "[2070]\ttrain's rmse: 4950.99\tvalid's rmse: 6234.95\n",
      "[2080]\ttrain's rmse: 4945.88\tvalid's rmse: 6232.28\n",
      "[2090]\ttrain's rmse: 4939.68\tvalid's rmse: 6229.27\n",
      "[2100]\ttrain's rmse: 4934.01\tvalid's rmse: 6225.84\n",
      "[2110]\ttrain's rmse: 4927.5\tvalid's rmse: 6223.54\n",
      "[2120]\ttrain's rmse: 4922.66\tvalid's rmse: 6220.75\n",
      "[2130]\ttrain's rmse: 4917.8\tvalid's rmse: 6216.93\n",
      "[2140]\ttrain's rmse: 4911.74\tvalid's rmse: 6215.15\n",
      "[2150]\ttrain's rmse: 4905.36\tvalid's rmse: 6211.96\n",
      "[2160]\ttrain's rmse: 4899.2\tvalid's rmse: 6207.99\n",
      "[2170]\ttrain's rmse: 4894.47\tvalid's rmse: 6205.45\n",
      "[2180]\ttrain's rmse: 4888.88\tvalid's rmse: 6204.05\n",
      "[2190]\ttrain's rmse: 4883.66\tvalid's rmse: 6202.69\n",
      "[2200]\ttrain's rmse: 4877.49\tvalid's rmse: 6200.11\n",
      "[2210]\ttrain's rmse: 4872.48\tvalid's rmse: 6197.3\n",
      "[2220]\ttrain's rmse: 4867.09\tvalid's rmse: 6194.46\n",
      "[2230]\ttrain's rmse: 4861.55\tvalid's rmse: 6192.21\n",
      "[2240]\ttrain's rmse: 4857.08\tvalid's rmse: 6189.59\n",
      "[2250]\ttrain's rmse: 4852.06\tvalid's rmse: 6187.21\n",
      "[2260]\ttrain's rmse: 4846.65\tvalid's rmse: 6183.71\n",
      "[2270]\ttrain's rmse: 4842.05\tvalid's rmse: 6181.83\n",
      "[2280]\ttrain's rmse: 4836.78\tvalid's rmse: 6179.9\n",
      "[2290]\ttrain's rmse: 4830.63\tvalid's rmse: 6176.91\n",
      "[2300]\ttrain's rmse: 4825.57\tvalid's rmse: 6175.84\n",
      "[2310]\ttrain's rmse: 4820.58\tvalid's rmse: 6173.11\n",
      "[2320]\ttrain's rmse: 4814.63\tvalid's rmse: 6170.6\n",
      "[2330]\ttrain's rmse: 4810.26\tvalid's rmse: 6168.15\n",
      "[2340]\ttrain's rmse: 4805.13\tvalid's rmse: 6165.53\n",
      "[2350]\ttrain's rmse: 4800.46\tvalid's rmse: 6163.25\n",
      "[2360]\ttrain's rmse: 4795.09\tvalid's rmse: 6160.3\n",
      "[2370]\ttrain's rmse: 4790.93\tvalid's rmse: 6157.99\n",
      "[2380]\ttrain's rmse: 4786.05\tvalid's rmse: 6155.72\n",
      "[2390]\ttrain's rmse: 4781.17\tvalid's rmse: 6153.41\n",
      "[2400]\ttrain's rmse: 4775.11\tvalid's rmse: 6151.56\n",
      "[2410]\ttrain's rmse: 4772.24\tvalid's rmse: 6150.3\n",
      "[2420]\ttrain's rmse: 4766.55\tvalid's rmse: 6146.84\n",
      "[2430]\ttrain's rmse: 4761.09\tvalid's rmse: 6143.48\n",
      "[2440]\ttrain's rmse: 4756.8\tvalid's rmse: 6141.29\n",
      "[2450]\ttrain's rmse: 4752.18\tvalid's rmse: 6138.4\n",
      "[2460]\ttrain's rmse: 4747.24\tvalid's rmse: 6134.99\n",
      "[2470]\ttrain's rmse: 4742\tvalid's rmse: 6132.43\n",
      "[2480]\ttrain's rmse: 4737.27\tvalid's rmse: 6129.61\n",
      "[2490]\ttrain's rmse: 4732.96\tvalid's rmse: 6128\n",
      "[2500]\ttrain's rmse: 4729.28\tvalid's rmse: 6126.5\n",
      "[2510]\ttrain's rmse: 4722.92\tvalid's rmse: 6123.08\n",
      "[2520]\ttrain's rmse: 4718.26\tvalid's rmse: 6121.17\n",
      "[2530]\ttrain's rmse: 4711.99\tvalid's rmse: 6119.14\n",
      "[2540]\ttrain's rmse: 4707.04\tvalid's rmse: 6116.72\n",
      "[2550]\ttrain's rmse: 4701.32\tvalid's rmse: 6114.39\n",
      "[2560]\ttrain's rmse: 4696.22\tvalid's rmse: 6111.68\n",
      "[2570]\ttrain's rmse: 4691.41\tvalid's rmse: 6110.83\n",
      "[2580]\ttrain's rmse: 4686.45\tvalid's rmse: 6108.5\n",
      "[2590]\ttrain's rmse: 4681.33\tvalid's rmse: 6106.06\n",
      "[2600]\ttrain's rmse: 4677.41\tvalid's rmse: 6103.93\n",
      "[2610]\ttrain's rmse: 4673.2\tvalid's rmse: 6101.21\n",
      "[2620]\ttrain's rmse: 4669.28\tvalid's rmse: 6099.67\n",
      "[2630]\ttrain's rmse: 4663.7\tvalid's rmse: 6097.26\n",
      "[2640]\ttrain's rmse: 4660.09\tvalid's rmse: 6095.9\n",
      "[2650]\ttrain's rmse: 4656.43\tvalid's rmse: 6095.02\n",
      "[2660]\ttrain's rmse: 4652.33\tvalid's rmse: 6093.47\n",
      "[2670]\ttrain's rmse: 4646.33\tvalid's rmse: 6091.5\n",
      "[2680]\ttrain's rmse: 4642.26\tvalid's rmse: 6090.57\n",
      "[2690]\ttrain's rmse: 4638.89\tvalid's rmse: 6088.94\n",
      "[2700]\ttrain's rmse: 4634.29\tvalid's rmse: 6086.62\n",
      "[2710]\ttrain's rmse: 4630.01\tvalid's rmse: 6084.14\n",
      "[2720]\ttrain's rmse: 4626.21\tvalid's rmse: 6083.2\n",
      "[2730]\ttrain's rmse: 4621.36\tvalid's rmse: 6080.98\n",
      "[2740]\ttrain's rmse: 4617.33\tvalid's rmse: 6079.39\n",
      "[2750]\ttrain's rmse: 4613.65\tvalid's rmse: 6077.74\n",
      "[2760]\ttrain's rmse: 4609.31\tvalid's rmse: 6076.67\n",
      "[2770]\ttrain's rmse: 4605.8\tvalid's rmse: 6075\n",
      "[2780]\ttrain's rmse: 4600.98\tvalid's rmse: 6074.01\n",
      "[2790]\ttrain's rmse: 4597.13\tvalid's rmse: 6072.43\n",
      "[2800]\ttrain's rmse: 4593.52\tvalid's rmse: 6070.55\n",
      "[2810]\ttrain's rmse: 4590.53\tvalid's rmse: 6069.33\n",
      "[2820]\ttrain's rmse: 4586.79\tvalid's rmse: 6067.69\n",
      "[2830]\ttrain's rmse: 4582.45\tvalid's rmse: 6066.53\n",
      "[2840]\ttrain's rmse: 4577.66\tvalid's rmse: 6065.71\n",
      "[2850]\ttrain's rmse: 4573.37\tvalid's rmse: 6064.36\n",
      "[2860]\ttrain's rmse: 4569.56\tvalid's rmse: 6061.53\n",
      "[2870]\ttrain's rmse: 4563.58\tvalid's rmse: 6060.5\n",
      "[2880]\ttrain's rmse: 4559.13\tvalid's rmse: 6058.87\n",
      "[2890]\ttrain's rmse: 4555.17\tvalid's rmse: 6058.39\n",
      "[2900]\ttrain's rmse: 4549.97\tvalid's rmse: 6056.02\n",
      "[2910]\ttrain's rmse: 4545.92\tvalid's rmse: 6054.43\n",
      "[2920]\ttrain's rmse: 4542.07\tvalid's rmse: 6052.45\n",
      "[2930]\ttrain's rmse: 4538.36\tvalid's rmse: 6050.86\n",
      "[2940]\ttrain's rmse: 4533.9\tvalid's rmse: 6048.3\n",
      "[2950]\ttrain's rmse: 4528.83\tvalid's rmse: 6047.04\n",
      "[2960]\ttrain's rmse: 4525.17\tvalid's rmse: 6045.21\n",
      "[2970]\ttrain's rmse: 4521.23\tvalid's rmse: 6043\n",
      "[2980]\ttrain's rmse: 4517.15\tvalid's rmse: 6040.94\n",
      "[2990]\ttrain's rmse: 4512.79\tvalid's rmse: 6039.21\n",
      "[3000]\ttrain's rmse: 4508.88\tvalid's rmse: 6038.35\n",
      "Iteration 3001: Train RMSE = 4508.5076079568635, Valid RMSE = 6038.603796142274\n",
      "[3010]\ttrain's rmse: 4505\tvalid's rmse: 6036.99\n",
      "[3020]\ttrain's rmse: 4501.12\tvalid's rmse: 6034.5\n",
      "[3030]\ttrain's rmse: 4496.98\tvalid's rmse: 6031.65\n",
      "[3040]\ttrain's rmse: 4493.79\tvalid's rmse: 6030.16\n",
      "[3050]\ttrain's rmse: 4490.86\tvalid's rmse: 6028.95\n",
      "[3060]\ttrain's rmse: 4487.6\tvalid's rmse: 6027.72\n",
      "[3070]\ttrain's rmse: 4483.65\tvalid's rmse: 6026.35\n",
      "[3080]\ttrain's rmse: 4479.11\tvalid's rmse: 6023.84\n",
      "[3090]\ttrain's rmse: 4475.05\tvalid's rmse: 6022.28\n",
      "[3100]\ttrain's rmse: 4471.79\tvalid's rmse: 6020.82\n",
      "[3110]\ttrain's rmse: 4468.1\tvalid's rmse: 6019.78\n",
      "[3120]\ttrain's rmse: 4465.54\tvalid's rmse: 6018.48\n",
      "[3130]\ttrain's rmse: 4461.27\tvalid's rmse: 6017.18\n",
      "[3140]\ttrain's rmse: 4457.4\tvalid's rmse: 6015.63\n",
      "[3150]\ttrain's rmse: 4453.8\tvalid's rmse: 6015.52\n",
      "[3160]\ttrain's rmse: 4450.13\tvalid's rmse: 6014.81\n",
      "[3170]\ttrain's rmse: 4446.95\tvalid's rmse: 6013.87\n",
      "[3180]\ttrain's rmse: 4443.22\tvalid's rmse: 6012.75\n",
      "[3190]\ttrain's rmse: 4439.97\tvalid's rmse: 6011.45\n",
      "[3200]\ttrain's rmse: 4436.15\tvalid's rmse: 6009.9\n",
      "[3210]\ttrain's rmse: 4431.97\tvalid's rmse: 6008.02\n",
      "[3220]\ttrain's rmse: 4428.62\tvalid's rmse: 6006.3\n",
      "[3230]\ttrain's rmse: 4424.58\tvalid's rmse: 6004.64\n",
      "[3240]\ttrain's rmse: 4420.9\tvalid's rmse: 6002.91\n",
      "[3250]\ttrain's rmse: 4417.17\tvalid's rmse: 6001.44\n",
      "[3260]\ttrain's rmse: 4413.37\tvalid's rmse: 5999.5\n",
      "[3270]\ttrain's rmse: 4410.19\tvalid's rmse: 5998.35\n",
      "[3280]\ttrain's rmse: 4406.6\tvalid's rmse: 5997.79\n",
      "[3290]\ttrain's rmse: 4403.88\tvalid's rmse: 5996.4\n",
      "[3300]\ttrain's rmse: 4400.86\tvalid's rmse: 5995.7\n",
      "[3310]\ttrain's rmse: 4398.28\tvalid's rmse: 5994.24\n",
      "[3320]\ttrain's rmse: 4395.33\tvalid's rmse: 5993.03\n",
      "[3330]\ttrain's rmse: 4391.26\tvalid's rmse: 5991.52\n",
      "[3340]\ttrain's rmse: 4387.43\tvalid's rmse: 5990.66\n",
      "[3350]\ttrain's rmse: 4383.15\tvalid's rmse: 5989.35\n",
      "[3360]\ttrain's rmse: 4379.11\tvalid's rmse: 5988.15\n",
      "[3370]\ttrain's rmse: 4373.75\tvalid's rmse: 5986.58\n",
      "[3380]\ttrain's rmse: 4367.87\tvalid's rmse: 5985.15\n",
      "[3390]\ttrain's rmse: 4364.69\tvalid's rmse: 5984.41\n",
      "[3400]\ttrain's rmse: 4361.43\tvalid's rmse: 5982.98\n",
      "[3410]\ttrain's rmse: 4358.76\tvalid's rmse: 5982.17\n",
      "[3420]\ttrain's rmse: 4355.28\tvalid's rmse: 5980.38\n",
      "[3430]\ttrain's rmse: 4351.9\tvalid's rmse: 5978.93\n",
      "[3440]\ttrain's rmse: 4349.08\tvalid's rmse: 5977.69\n",
      "[3450]\ttrain's rmse: 4345.34\tvalid's rmse: 5976.52\n",
      "[3460]\ttrain's rmse: 4341.39\tvalid's rmse: 5975.11\n",
      "[3470]\ttrain's rmse: 4338.32\tvalid's rmse: 5973.52\n",
      "[3480]\ttrain's rmse: 4335.57\tvalid's rmse: 5972.59\n",
      "[3490]\ttrain's rmse: 4332.45\tvalid's rmse: 5971.03\n",
      "[3500]\ttrain's rmse: 4328.93\tvalid's rmse: 5969.76\n",
      "[3510]\ttrain's rmse: 4325.7\tvalid's rmse: 5968.35\n",
      "[3520]\ttrain's rmse: 4321.6\tvalid's rmse: 5966.56\n",
      "[3530]\ttrain's rmse: 4317.65\tvalid's rmse: 5964.82\n",
      "[3540]\ttrain's rmse: 4314.34\tvalid's rmse: 5963.39\n",
      "[3550]\ttrain's rmse: 4310.29\tvalid's rmse: 5963.02\n",
      "[3560]\ttrain's rmse: 4307.63\tvalid's rmse: 5962.63\n",
      "[3570]\ttrain's rmse: 4303.5\tvalid's rmse: 5960.96\n",
      "[3580]\ttrain's rmse: 4299.02\tvalid's rmse: 5959.75\n",
      "[3590]\ttrain's rmse: 4294.67\tvalid's rmse: 5958.38\n",
      "[3600]\ttrain's rmse: 4290.78\tvalid's rmse: 5957.66\n",
      "[3610]\ttrain's rmse: 4287.67\tvalid's rmse: 5956.75\n",
      "[3620]\ttrain's rmse: 4284.99\tvalid's rmse: 5955.98\n",
      "[3630]\ttrain's rmse: 4282.14\tvalid's rmse: 5955.03\n",
      "[3640]\ttrain's rmse: 4279.02\tvalid's rmse: 5954\n",
      "[3650]\ttrain's rmse: 4276.33\tvalid's rmse: 5953.73\n",
      "[3660]\ttrain's rmse: 4273.07\tvalid's rmse: 5951.76\n",
      "[3670]\ttrain's rmse: 4269.79\tvalid's rmse: 5950.35\n",
      "[3680]\ttrain's rmse: 4266.48\tvalid's rmse: 5948.86\n",
      "[3690]\ttrain's rmse: 4263.14\tvalid's rmse: 5947.91\n",
      "[3700]\ttrain's rmse: 4260.31\tvalid's rmse: 5946.66\n",
      "[3710]\ttrain's rmse: 4257.54\tvalid's rmse: 5946.56\n",
      "[3720]\ttrain's rmse: 4254.84\tvalid's rmse: 5946.04\n",
      "[3730]\ttrain's rmse: 4251.92\tvalid's rmse: 5944.76\n",
      "[3740]\ttrain's rmse: 4248.4\tvalid's rmse: 5943.46\n",
      "[3750]\ttrain's rmse: 4245.19\tvalid's rmse: 5941.86\n",
      "[3760]\ttrain's rmse: 4242.7\tvalid's rmse: 5941.16\n",
      "[3770]\ttrain's rmse: 4239.76\tvalid's rmse: 5940.67\n",
      "[3780]\ttrain's rmse: 4236.34\tvalid's rmse: 5939.61\n",
      "[3790]\ttrain's rmse: 4232.92\tvalid's rmse: 5938.35\n",
      "[3800]\ttrain's rmse: 4230.55\tvalid's rmse: 5937.32\n",
      "[3810]\ttrain's rmse: 4225.85\tvalid's rmse: 5935.46\n",
      "[3820]\ttrain's rmse: 4223.39\tvalid's rmse: 5934.72\n",
      "[3830]\ttrain's rmse: 4220.35\tvalid's rmse: 5932.83\n",
      "[3840]\ttrain's rmse: 4217.58\tvalid's rmse: 5931.42\n",
      "[3850]\ttrain's rmse: 4213.89\tvalid's rmse: 5930.56\n",
      "[3860]\ttrain's rmse: 4210.57\tvalid's rmse: 5929.11\n",
      "[3870]\ttrain's rmse: 4207.74\tvalid's rmse: 5927.76\n",
      "[3880]\ttrain's rmse: 4204.77\tvalid's rmse: 5926.98\n",
      "[3890]\ttrain's rmse: 4201.84\tvalid's rmse: 5925.87\n",
      "[3900]\ttrain's rmse: 4198.41\tvalid's rmse: 5924.81\n",
      "[3910]\ttrain's rmse: 4195.51\tvalid's rmse: 5923.46\n",
      "[3920]\ttrain's rmse: 4192.85\tvalid's rmse: 5922.47\n",
      "[3930]\ttrain's rmse: 4189.69\tvalid's rmse: 5921.12\n",
      "[3940]\ttrain's rmse: 4186.45\tvalid's rmse: 5920.42\n",
      "[3950]\ttrain's rmse: 4183.06\tvalid's rmse: 5920.49\n",
      "[3960]\ttrain's rmse: 4180.63\tvalid's rmse: 5919.16\n",
      "[3970]\ttrain's rmse: 4177.63\tvalid's rmse: 5917.82\n",
      "[3980]\ttrain's rmse: 4174.5\tvalid's rmse: 5917.01\n",
      "[3990]\ttrain's rmse: 4171.73\tvalid's rmse: 5916.02\n",
      "[4000]\ttrain's rmse: 4168.46\tvalid's rmse: 5915.17\n",
      "Iteration 4001: Train RMSE = 4168.042225073596, Valid RMSE = 5914.962470517495\n",
      "[4010]\ttrain's rmse: 4165.63\tvalid's rmse: 5914.2\n",
      "[4020]\ttrain's rmse: 4162.93\tvalid's rmse: 5913.35\n",
      "[4030]\ttrain's rmse: 4160.6\tvalid's rmse: 5912.52\n",
      "[4040]\ttrain's rmse: 4158.08\tvalid's rmse: 5912\n",
      "[4050]\ttrain's rmse: 4155.67\tvalid's rmse: 5911.47\n",
      "[4060]\ttrain's rmse: 4152.52\tvalid's rmse: 5910.73\n",
      "[4070]\ttrain's rmse: 4149.76\tvalid's rmse: 5909.73\n",
      "[4080]\ttrain's rmse: 4146.32\tvalid's rmse: 5908.1\n",
      "[4090]\ttrain's rmse: 4144.12\tvalid's rmse: 5907.24\n",
      "[4100]\ttrain's rmse: 4140.41\tvalid's rmse: 5906.55\n",
      "[4110]\ttrain's rmse: 4137.59\tvalid's rmse: 5906.57\n",
      "[4120]\ttrain's rmse: 4134.82\tvalid's rmse: 5905.91\n",
      "[4130]\ttrain's rmse: 4132.21\tvalid's rmse: 5905.77\n",
      "[4140]\ttrain's rmse: 4129.78\tvalid's rmse: 5905.24\n",
      "[4150]\ttrain's rmse: 4126.92\tvalid's rmse: 5904.48\n",
      "[4160]\ttrain's rmse: 4123.94\tvalid's rmse: 5902.93\n",
      "[4170]\ttrain's rmse: 4120.65\tvalid's rmse: 5901.85\n",
      "[4180]\ttrain's rmse: 4117.84\tvalid's rmse: 5901\n",
      "[4190]\ttrain's rmse: 4114.53\tvalid's rmse: 5900.6\n",
      "[4200]\ttrain's rmse: 4111.82\tvalid's rmse: 5900.29\n",
      "[4210]\ttrain's rmse: 4109.32\tvalid's rmse: 5899.54\n",
      "[4220]\ttrain's rmse: 4106.18\tvalid's rmse: 5898.1\n",
      "[4230]\ttrain's rmse: 4103.65\tvalid's rmse: 5897.28\n",
      "[4240]\ttrain's rmse: 4101.02\tvalid's rmse: 5896.79\n",
      "[4250]\ttrain's rmse: 4098.23\tvalid's rmse: 5896.14\n",
      "[4260]\ttrain's rmse: 4094.98\tvalid's rmse: 5895.44\n",
      "[4270]\ttrain's rmse: 4092.8\tvalid's rmse: 5894.43\n",
      "[4280]\ttrain's rmse: 4089.56\tvalid's rmse: 5893.5\n",
      "[4290]\ttrain's rmse: 4086.84\tvalid's rmse: 5893.15\n",
      "[4300]\ttrain's rmse: 4084.71\tvalid's rmse: 5892.9\n",
      "[4310]\ttrain's rmse: 4082.37\tvalid's rmse: 5892.35\n",
      "[4320]\ttrain's rmse: 4080.3\tvalid's rmse: 5891.61\n",
      "[4330]\ttrain's rmse: 4078.08\tvalid's rmse: 5890.8\n",
      "[4340]\ttrain's rmse: 4075.68\tvalid's rmse: 5890.37\n",
      "[4350]\ttrain's rmse: 4072.96\tvalid's rmse: 5889.82\n",
      "[4360]\ttrain's rmse: 4070.67\tvalid's rmse: 5889.25\n",
      "[4370]\ttrain's rmse: 4067.44\tvalid's rmse: 5888.15\n",
      "[4380]\ttrain's rmse: 4064.41\tvalid's rmse: 5888.24\n",
      "[4390]\ttrain's rmse: 4062.01\tvalid's rmse: 5888.05\n",
      "[4400]\ttrain's rmse: 4059.8\tvalid's rmse: 5887.97\n",
      "[4410]\ttrain's rmse: 4057.43\tvalid's rmse: 5887.2\n",
      "[4420]\ttrain's rmse: 4054.87\tvalid's rmse: 5887.67\n",
      "[4430]\ttrain's rmse: 4052.98\tvalid's rmse: 5887.32\n",
      "[4440]\ttrain's rmse: 4050.12\tvalid's rmse: 5886.28\n",
      "[4450]\ttrain's rmse: 4047.93\tvalid's rmse: 5885.54\n",
      "[4460]\ttrain's rmse: 4045.58\tvalid's rmse: 5885\n",
      "[4470]\ttrain's rmse: 4043.32\tvalid's rmse: 5884.28\n",
      "[4480]\ttrain's rmse: 4040.83\tvalid's rmse: 5883.64\n",
      "[4490]\ttrain's rmse: 4037.75\tvalid's rmse: 5882.92\n",
      "[4500]\ttrain's rmse: 4035.52\tvalid's rmse: 5881.58\n",
      "[4510]\ttrain's rmse: 4033.6\tvalid's rmse: 5880.37\n",
      "[4520]\ttrain's rmse: 4030.91\tvalid's rmse: 5879.22\n",
      "[4530]\ttrain's rmse: 4028.69\tvalid's rmse: 5878.66\n",
      "[4540]\ttrain's rmse: 4026.46\tvalid's rmse: 5878.39\n",
      "[4550]\ttrain's rmse: 4024.16\tvalid's rmse: 5876.97\n",
      "[4560]\ttrain's rmse: 4022.29\tvalid's rmse: 5876.04\n",
      "[4570]\ttrain's rmse: 4019.88\tvalid's rmse: 5875.46\n",
      "[4580]\ttrain's rmse: 4016.8\tvalid's rmse: 5875.55\n",
      "[4590]\ttrain's rmse: 4014.32\tvalid's rmse: 5875.09\n",
      "[4600]\ttrain's rmse: 4011.67\tvalid's rmse: 5874.57\n",
      "[4610]\ttrain's rmse: 4009.23\tvalid's rmse: 5873.55\n",
      "[4620]\ttrain's rmse: 4006.94\tvalid's rmse: 5872.97\n",
      "[4630]\ttrain's rmse: 4004.13\tvalid's rmse: 5872.47\n",
      "[4640]\ttrain's rmse: 4002.13\tvalid's rmse: 5871.93\n",
      "[4650]\ttrain's rmse: 3999.95\tvalid's rmse: 5871.05\n",
      "[4660]\ttrain's rmse: 3997.11\tvalid's rmse: 5870.51\n",
      "[4670]\ttrain's rmse: 3994.63\tvalid's rmse: 5869.3\n",
      "[4680]\ttrain's rmse: 3992.12\tvalid's rmse: 5869.07\n",
      "[4690]\ttrain's rmse: 3989.93\tvalid's rmse: 5868.43\n",
      "[4700]\ttrain's rmse: 3987.68\tvalid's rmse: 5867.95\n",
      "[4710]\ttrain's rmse: 3985.67\tvalid's rmse: 5867.2\n",
      "[4720]\ttrain's rmse: 3983.3\tvalid's rmse: 5866.91\n",
      "[4730]\ttrain's rmse: 3980.96\tvalid's rmse: 5866.25\n",
      "[4740]\ttrain's rmse: 3978.67\tvalid's rmse: 5866.19\n",
      "[4750]\ttrain's rmse: 3976.05\tvalid's rmse: 5864.85\n",
      "[4760]\ttrain's rmse: 3973.96\tvalid's rmse: 5863.64\n",
      "[4770]\ttrain's rmse: 3971.36\tvalid's rmse: 5863.11\n",
      "[4780]\ttrain's rmse: 3968.96\tvalid's rmse: 5862.15\n",
      "[4790]\ttrain's rmse: 3967.68\tvalid's rmse: 5862.25\n",
      "[4800]\ttrain's rmse: 3965.49\tvalid's rmse: 5861.54\n",
      "[4810]\ttrain's rmse: 3963.18\tvalid's rmse: 5860.73\n",
      "[4820]\ttrain's rmse: 3961.07\tvalid's rmse: 5859.55\n",
      "[4830]\ttrain's rmse: 3958.75\tvalid's rmse: 5859.25\n",
      "[4840]\ttrain's rmse: 3956.39\tvalid's rmse: 5858.7\n",
      "[4850]\ttrain's rmse: 3953.77\tvalid's rmse: 5858.63\n",
      "[4860]\ttrain's rmse: 3951.42\tvalid's rmse: 5859.57\n",
      "[4870]\ttrain's rmse: 3949.05\tvalid's rmse: 5858.6\n",
      "[4880]\ttrain's rmse: 3947.28\tvalid's rmse: 5858.49\n",
      "[4890]\ttrain's rmse: 3945.2\tvalid's rmse: 5857.58\n",
      "[4900]\ttrain's rmse: 3943.09\tvalid's rmse: 5857.38\n",
      "[4910]\ttrain's rmse: 3941.16\tvalid's rmse: 5857.02\n",
      "[4920]\ttrain's rmse: 3938.95\tvalid's rmse: 5856.35\n",
      "[4930]\ttrain's rmse: 3936.52\tvalid's rmse: 5855.88\n",
      "[4940]\ttrain's rmse: 3933.76\tvalid's rmse: 5855.41\n",
      "[4950]\ttrain's rmse: 3931.7\tvalid's rmse: 5854.74\n",
      "[4960]\ttrain's rmse: 3929.52\tvalid's rmse: 5854.4\n",
      "[4970]\ttrain's rmse: 3927.36\tvalid's rmse: 5853.56\n",
      "[4980]\ttrain's rmse: 3924.86\tvalid's rmse: 5852.83\n",
      "[4990]\ttrain's rmse: 3921.89\tvalid's rmse: 5853.14\n",
      "[5000]\ttrain's rmse: 3919.18\tvalid's rmse: 5852.63\n",
      "Iteration 5001: Train RMSE = 3919.013351473857, Valid RMSE = 5852.5451741882325\n",
      "[5010]\ttrain's rmse: 3916.35\tvalid's rmse: 5853.16\n",
      "[5020]\ttrain's rmse: 3914.2\tvalid's rmse: 5853.69\n",
      "[5030]\ttrain's rmse: 3912.05\tvalid's rmse: 5853.64\n",
      "Early stopping, best iteration is:\n",
      "[4988]\ttrain's rmse: 3922.61\tvalid's rmse: 5852.19\n",
      "Calculating Permutation Importance...\n",
      "Calculating baseline score...\n",
      "Evaluating feature: 아파트명\n",
      "Evaluating feature: 전용면적\n",
      "Evaluating feature: 계약년월\n",
      "Evaluating feature: 계약일\n",
      "Evaluating feature: 층\n",
      "Evaluating feature: 건축년도\n",
      "Evaluating feature: 도로명\n",
      "Evaluating feature: k_단지분류_아파트_주상복합등등_\n",
      "Evaluating feature: k_세대타입_분양형태_\n",
      "Evaluating feature: k_관리방식\n",
      "Evaluating feature: k_복도유형\n",
      "Evaluating feature: k_난방방식\n",
      "Evaluating feature: k_전체동수\n",
      "Evaluating feature: k_전체세대수\n",
      "Evaluating feature: k_건설사_시공사_\n",
      "Evaluating feature: k_시행사\n",
      "Evaluating feature: k_연면적\n",
      "Evaluating feature: k_주거전용면적\n",
      "Evaluating feature: k_전용면적별세대현황_60_이하_\n",
      "Evaluating feature: k_전용면적별세대현황_60__85_이하_\n",
      "Evaluating feature: k_85__135_이하\n",
      "Evaluating feature: k_135_초과\n",
      "Evaluating feature: 건축면적\n",
      "Evaluating feature: 주차대수\n",
      "Evaluating feature: 기타_의무_임대_임의_1_2_3_4\n",
      "Evaluating feature: 좌표X\n",
      "Evaluating feature: 좌표Y\n",
      "Evaluating feature: distance_score\n",
      "Evaluating feature: 구\n",
      "Evaluating feature: 동\n",
      "Evaluating feature: 계약연도\n",
      "Evaluating feature: 계약월\n",
      "Evaluating feature: 급지\n",
      "Evaluating feature: address\n",
      "Evaluating feature: 신축여부\n",
      "Evaluating feature: 이자율\n",
      "Evaluating feature: is_top20\n",
      "Evaluating feature: 대장아파트_거리\n",
      "Permutation Feature Importances:\n",
      "                   feature    importance\n",
      "2                     계약년월  1.045655e+09\n",
      "1                     전용면적  9.756646e+08\n",
      "32                      급지  6.113884e+08\n",
      "5                     건축년도  1.695819e+08\n",
      "26                     좌표Y  1.136631e+08\n",
      "25                     좌표X  6.300580e+07\n",
      "33                 address  5.447845e+07\n",
      "28                       구  2.346532e+07\n",
      "37                대장아파트_거리  2.164844e+07\n",
      "4                        층  1.891177e+07\n",
      "16                   k_연면적  1.570995e+07\n",
      "12                  k_전체동수  1.222237e+07\n",
      "29                       동  1.209701e+07\n",
      "23                    주차대수  1.160945e+07\n",
      "7       k_단지분류_아파트_주상복합등등_  1.124049e+07\n",
      "11                  k_난방방식  1.095110e+07\n",
      "17                k_주거전용면적  1.078178e+07\n",
      "27          distance_score  8.790410e+06\n",
      "6                      도로명  7.312892e+06\n",
      "0                     아파트명  6.965256e+06\n",
      "35                     이자율  5.912753e+06\n",
      "13                 k_전체세대수  5.691434e+06\n",
      "15                   k_시행사  5.517553e+06\n",
      "19  k_전용면적별세대현황_60__85_이하_  3.954117e+06\n",
      "20            k_85__135_이하  3.815404e+06\n",
      "14              k_건설사_시공사_  3.456251e+06\n",
      "10                  k_복도유형  3.350095e+06\n",
      "8             k_세대타입_분양형태_  3.336379e+06\n",
      "18      k_전용면적별세대현황_60_이하_  3.019038e+06\n",
      "22                    건축면적  1.852536e+06\n",
      "24     기타_의무_임대_임의_1_2_3_4  1.585483e+06\n",
      "36                is_top20  1.393460e+06\n",
      "31                     계약월  1.083496e+06\n",
      "3                      계약일  9.880150e+05\n",
      "21                k_135_초과  4.552280e+05\n",
      "9                   k_관리방식  2.713488e+05\n",
      "30                    계약연도  1.213199e+05\n",
      "34                    신축여부  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Permutation Importance 함수 정의\n",
    "def permutation_importance(model, X, y, metric, num_rounds=5):\n",
    "    print(\"Calculating baseline score...\")\n",
    "    baseline_score = metric(y, model.predict(X, num_iteration=model.best_iteration))\n",
    "    importances = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        print(f\"Evaluating feature: {col}\")\n",
    "        scores = []\n",
    "        for _ in range(num_rounds):\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[col] = np.random.permutation(X_permuted[col])\n",
    "            score = metric(y, model.predict(X_permuted, num_iteration=model.best_iteration))\n",
    "            scores.append(score)\n",
    "        importances.append(np.mean(scores) - baseline_score)\n",
    "\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "# 데이터 나누기\n",
    "print(\"Splitting data into train and test sets...\")\n",
    "X_train_perm, X_test_perm, y_train_perm, y_test_perm = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM 데이터셋 생성\n",
    "print(\"Creating LightGBM dataset...\")\n",
    "dtrain_perm = lgb.Dataset(X_train_perm, label=y_train_perm)\n",
    "dtest_perm = lgb.Dataset(X_test_perm, label=y_test_perm, reference=dtrain_perm)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "}\n",
    "\n",
    "# num_boost_round 설정\n",
    "num_boost_round = 10000\n",
    "\n",
    "# Detailed logging callback\n",
    "def log_callback(env):\n",
    "    if env.iteration % 1000 == 0 or env.iteration + 1 == env.end_iteration:\n",
    "        print(f\"Iteration {env.iteration + 1}: Train RMSE = {env.evaluation_result_list[0][2]}, Valid RMSE = {env.evaluation_result_list[1][2]}\")\n",
    "\n",
    "print(\"Training the final model...\")\n",
    "final_model_perm = lgb.train(\n",
    "    params,\n",
    "    dtrain_perm,\n",
    "    num_boost_round=num_boost_round,\n",
    "    valid_sets=[dtrain_perm, dtest_perm],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), \n",
    "               lgb.log_evaluation(period=10),\n",
    "               log_callback]\n",
    ")\n",
    "\n",
    "# Permutation Importance 계산\n",
    "print(\"Calculating Permutation Importance...\")\n",
    "feature_importances = permutation_importance(final_model_perm, X_test_perm, y_test_perm, mean_squared_error)\n",
    "\n",
    "print(\"Permutation Feature Importances:\")\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature        object\n",
       "importance    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame columns: Index(['feature', 'importance'], dtype='object')\n",
      "Permutation Feature Importances:\n",
      "                   feature    importance\n",
      "0                     계약년월  1.045655e+09\n",
      "1                     전용면적  9.756646e+08\n",
      "2                       급지  6.113884e+08\n",
      "3                     건축년도  1.695819e+08\n",
      "4                      좌표Y  1.136631e+08\n",
      "5                      좌표X  6.300580e+07\n",
      "6                  address  5.447845e+07\n",
      "7                        구  2.346532e+07\n",
      "8                 대장아파트_거리  2.164844e+07\n",
      "9                        층  1.891177e+07\n",
      "10                   k_연면적  1.570995e+07\n",
      "11                  k_전체동수  1.222237e+07\n",
      "12                       동  1.209701e+07\n",
      "13                    주차대수  1.160945e+07\n",
      "14      k_단지분류_아파트_주상복합등등_  1.124049e+07\n",
      "15                  k_난방방식  1.095110e+07\n",
      "16                k_주거전용면적  1.078178e+07\n",
      "17          distance_score  8.790410e+06\n",
      "18                     도로명  7.312892e+06\n",
      "19                    아파트명  6.965256e+06\n",
      "20                     이자율  5.912753e+06\n",
      "21                 k_전체세대수  5.691434e+06\n",
      "22                   k_시행사  5.517553e+06\n",
      "23  k_전용면적별세대현황_60__85_이하_  3.954117e+06\n",
      "24            k_85__135_이하  3.815404e+06\n",
      "25              k_건설사_시공사_  3.456251e+06\n",
      "26                  k_복도유형  3.350095e+06\n",
      "27            k_세대타입_분양형태_  3.336379e+06\n",
      "28      k_전용면적별세대현황_60_이하_  3.019038e+06\n",
      "29                    건축면적  1.852536e+06\n",
      "30     기타_의무_임대_임의_1_2_3_4  1.585483e+06\n",
      "31                is_top20  1.393460e+06\n",
      "32                     계약월  1.083496e+06\n",
      "33                     계약일  9.880150e+05\n",
      "34                k_135_초과  4.552280e+05\n",
      "35                  k_관리방식  2.713488e+05\n",
      "36                    계약연도  1.213199e+05\n",
      "37                    신축여부  0.000000e+00\n",
      "Feature importances saved to 'feature_importances.csv'\n",
      "Visualizing feature importances...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLgAAAK6CAYAAAA6veVnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3jN9///8cfJOhEy7BmbBLGiVNWqTZWqlpQqqkZRe6WoLTFqtqhRalVRSqld1PgoarRBSkraorXlmBlyfn/4Or8e5yROIjL0fruu93V5v+bzfZL0us6zr9frbTCbzWYBAAAAAAAAGZRTWgcAAAAAAAAAPA0SXAAAAAAAAMjQSHABAAAAAAAgQyPBBQAAAAAAgAyNBBcAAAAAAAAyNBJcAAAAAAAAyNBIcAEAAAAAACBDI8EFAAAAAACADI0EFwAAAAAAADI0ElwAAABIV2bOnKmKFSumdRgAACADIcEFAACQRr766isZDAbL5ePjo6pVq2rlypVpHVqSLF26VGFhYcnqu2HDBu3du9eqLFeuXCpRokRKhOawzp07W/0sHl1dunRJ8bnOnz+vTz/9NMXHTY69e/cqS5YsaR1Gkt2+fVtjx45N6zAAAOkICS4AAIA0Eh0drRIlSujGjRu6ceOGTp06pT59+qhr1676/PPP0zo8h82fP1+HDx9OVt/Vq1dr+/btVmWtW7dO9SRfbGysOnbsaPlZPLpmzpyZ4nNFRERo8uTJKT5ucsTFxSkuLi6tw0iyq1evavjw4WkdBgAgHXFJ6wAAAAD+y5ycnOTj4yNJ8vHxUVBQkP7880/NmDFDXbt2Tdvg/mPc3NwsPwsAAJCxsIILAAAgnSlXrpz++usvy/3SpUtVqlQpGY1GlS5dWl999ZVV+/fff1+ffvqpPvnkE2XNmlU1a9aUJL3zzjv67LPP1LNnT2XLlk3ZsmVT9+7dFRMTo19++UW1atWSh4eHAgICdODAAasxQ0ND1a1bN5vYevbsqffff1+SNH78eBkMBu3evVsdO3aUwWDQ+PHjJUk3btzQBx98oIIFC8rd3V1FihRRcHCwYmJiJEnLly+XwWDQl19+qVGjRlltB1y6dKkCAgKs5v3nn3/UsWNH5cqVS+7u7qpYsaKWLVtm1Wbnzp0qW7astmzZogoVKihTpkwqUaKEpk2bltQfgV2XLl1SmzZt5OXlJR8fH7Vt21ZXrlyxarNgwQJVrFhRnp6eyp49uxo3bqzTp09b6t3c3PTKK6/ojz/+kMFgkJubm6UuU6ZMunjxotV4V65ckZOTk86fP281xp9//qnGjRvLw8NDc+fOlfRwFdrQoUOVL18+ZcqUSTVr1tSRI0eS9Izbt29XjRo1tGrVKvn5+cnDw0PVq1fXr7/+qujoaH344YfKkSOHsmXLpnHjxslsNlv6HjhwQC+88IKOHDli+d3KmzevgoODFRsbazWP2WzWjBkzLL/X+fPnV+/evWUymazaFS9eXEeOHFHbtm2VOXNmDR06VCVLllSRIkUkybKV9MKFC5KkPXv2qH79+sqRI4eyZMmiSpUqafPmzVZj1qpVS4sXL1bnzp2VI0cOeXt7q1GjRoqMjLT5PE6cOKEWLVooa9ascnNzU5EiRXT8+HFL/ZP+NgEAqYcEFwAAQDrzxx9/qHDhwpKkL7/8Ur169dLHH3+sM2fOaMSIEerWrZu2bt1qaR8XF6etW7fq2LFjOnbsmOVLtouLi0aPHq0HDx7owIED2r17tw4dOqThw4frtddeU+fOnRUREaGePXuqcePGunr1qmXM+/fv6/79+zax/XtL28CBA3Xjxg29/PLL+uyzz3Tjxg0NHDhQknTkyBF5eXlp9erVOnfunObNm6dFixZpxowZkh5uQ7xx44befvttDR482Go74OPb5qKiolSjRg2dO3dOW7ZsUWRkpPr06aPu3btbbeU0GAw6f/68unbtquDgYJ0+fVozZszQ2LFjtXr16qf6mdy/f1916tTRgwcPtHfvXu3bt0/3799XixYtrNpt375dY8aM0a+//qr//e9/cnd311tvvaX4+HhJ0rVr1/Tdd9/J19dXN27c0LVr16zmeJQAfOTBgwcym81Wn0dsbKz69++vd955R2fPntVbb70lSeratau+++47rVixQqdOnVK9evVUr149myRcYlxcXHTy5EmNGTNGc+fO1W+//aZXXnlFb7zxhvr27aubN2/q559/1qZNm7Rw4UItWLDAKv4rV66oefPm6tmzp86dO6f58+drzpw5lt+LRwYNGqQRI0ZoxIgR+vPPP7VmzRodPnxYr776qh48eGBpFxcXp1GjRumFF17Q77//rj59+ujo0aOWJNONGzd08+ZN5c+fX5K0adMmvfPOO9qzZ49OnTqlFi1aqEWLFlYJQoPBoODgYN2/f18//PCDjh07prx586pZs2ZWCbuDBw/qpZdeUsmSJbV//36dP39eK1asUKFChSQ59rcJAEhFZgAAAKSJhQsXmv38/Cz39+/fN69fv96cNWtW8/z5880xMTHmXLlymVeuXGnVb/LkyebatWtb7tu3b2/Onj27+d69e1bt2rdvb65QoYI5Pj7eUrZ582azwWAwjxs3zqpt1apVzQsXLrTcjxgxwty+fXubmLt27WpTXqtWLau+CRk9erS5Zs2aNjGOGDHCquzxz+Wjjz4ylyhRwhwTE2PVbsmSJWZPT0+zyWQym81m886dO82SzKtXr7aZt0mTJonG1r59e3PXrl0TrJ8+fbo5ICDA6rO8d++eOUeOHOZdu3Yl2O/PP/80SzKfPXvWUrZz505zoUKFbNpKMp87d86q7O+//7Ypl2Tu3r27Vbvjx4+bjUaj+a+//rIqb9q0qXnkyJEJxrdz506z0Wi0updkPnr0qKXswYMH5nz58plLlSplfvDggaV8zpw55lq1atn0/fzzz63mWLFihdnNzc188+ZNs9lsNoeHh5udnJxsPjeTyWTOmjWr1e9SoUKF7P7szp07Z3b0q0zRokXNX3zxheW+Vq1a5nLlyln9LG/fvm329PQ0Hzx40Gw2m83x8fHmgIAA86BBg+yO6ejfJgAg9bCCCwAAIA2dOXNGPj4+ypIlizJlyqQBAwZoxowZ6tSpkw4fPqzbt2/rjTfesOrzyiuv6KeffrIqq1u3rtzd3W3Gr127tgwGg+W+WLFiMpvNat26tVW7YsWK6c8//0zBJ7NVtGhRq5U0jlq7dq26desmV1dXq/KgoCC5uLhoy5YtVuUNGjSwui9XrpzOnTv3xHkWLlwoHx8fq2vVqlWSpI0bN6pNmzZWn6W7u7teeuklm+2d/+br6ytXV9dkPXdimjZtanW/ceNG1apVSwUKFLAqf+WVVxKNzx4fHx9VqFDBcu/k5KQiRYqoZcuWcnL6/18f7P3OGAwGtW3b1qrs9ddflyTLmzbXrVunMmXKqFatWlbtPD091bZtW5vVdo8/a1LZ+72rV6+e1c8yc+bMKlasmOX35JdfflFYWJgGDx5sd8yk/G0CAFIHh8wDAACkocKFC2vHjh0yGAzKkSOHMmfObKmLjIzU/fv3lT17dqs+8fHxunfvnm7cuKGsWbNKkgoWLGh3fG9vb6t7FxcXu+1dXFws2+hSgtls1urVq/XVV18pPDxc165d0507d5QjR44kj3X27FmVLl3aptzFxUUlSpTQ77//bikzGo3y9PS0auft7a27d+8+cZ4333xT48aNsyrLmzevpIc/i7Fjx2rChAlW9Xfv3rWcByVJv/32m6ZPn65Dhw7p77//1r179xQbG2u17S4lPP7zi4yM1K5du2wOyY+JiVGJEiWSNPbjvzPSw8/akd+ZnDlzWv0OSw9/Jnny5LGck5XQz1OSSpcubfNWzYR+t+25ceOGZsyYoR9++EF//vmnbt26paioKL388ss2cT7u378n4eHhypMnj7Jly2Z3nqT8bQIAUgcJLgAAgDTk6upqOW/Lnpw5c9pdgWMwGKy+QD+eVHBk3uS4d++e1cqXhAwbNkwzZ85Ujx491KVLF+XLl08//PBDsg58f9J8jsTjCE9Pz0R/FsOHD1dQUJBN+aOfw/Hjx1WtWjXVr19fvXv3lp+fn7JmzZpgMscR9+7ds1tu7+fduHFju59vpkyZkj3/vznyO2Pv3DbJ+vcmqT9PR3+379y5o6pVqypTpkzq0qWLypcvr+zZs6tz584O9X/ck5KSjv5tAgBSBwkuAACAdCp//vy6cuWK8uTJY3f74bOUKVMm3bp1y6Y8IiLCZkXQ4wmJmJgYTZkyRStWrFDz5s0t5Zs2bbIZz5HkVPHixXXy5Ek1atTIqjwuLk4REREqWbLkE8d4Wvnz55fJZEo0ATZ9+nQ1aNBAa9eutZRdunTJ5g2CCT2zu7u7zWceERHhcHy//vprovGlBpPJZLN6yWQy6erVqypevLikhz/PL7/80m7/U6dOOfTztPcZfvPNN7p165aOHTtmldT798sTHOXv768rV67o/PnzNts+pbT92wQA2McZXAAAAOnUCy+8IA8PD82dOzfV5/b19dUvv/xi9Va5iIgIuytW3N3drZI4N2/e1P3791WmTBmrdt9+++0T+9rTvn17zZkzx6bd8uXLZTQabRJfz8Irr7yixYsXJ7iiSpL++ecfm2f+d7LrkYSe2dfX1/J2wEeWLFnicHwHDhyw6Z8Wli1bZnW/dOlS5cmTR+XLl5f08Oy006dPa9euXVbtoqKitHTpUnXs2PGJczxKKv37c/znn39UtGhRq+TWiRMndObMmSQ/Q7ly5eTn56fRo0fbrU/Lv00AgH0kuAAAANIpDw8PBQcHa9CgQZo6daouXryoyMhIrVixQjt37nymczdu3FiXL1/WmDFjFBUVpVOnTqlFixaqV6+eTduCBQtqzZo1On/+vI4ePaqcOXOqZMmSGjlypM6dO6cTJ06oQ4cOds/4KliwoL7//ntFRkbq8OHDVgm1Rz744AMZDAa9/vrrOnbsmP755x/NmTNHXbt21fjx41NlBU337t0VHx+vunXr6tChQ7p8+bKOHDmikJAQS5vq1atr8eLF2rdvn/766y/Nnz9fU6ZMsTlDqmDBgvr777+1adMmRURE6O+//5b0MPEzatQoHT9+XHfu3NGkSZP0999/y9nZ+Ynx1ahRQ/Xr11eTJk20bt06Xb58Wb/99pumTp1qOfsqNWTNmlVTp07V6tWrdenSJX377bcaNmyYRo4caTmgPn/+/BowYIDefvttS7v9+/erRo0aqlChgpo1a/bEebJnzy4PDw99+eWX+uOPPxQZGanq1avr4MGDWr58uf7++29t2bJFLVu2VMWKFZP8HAaDQZ999pkWLVqkzp076/Tp07p69aoOHz6smzdvpunfJgDAPhJcAAAAacTd3d1y6HtCPvroI02dOlULFixQkSJFVL58eX322WdW5xK5uLjYHcfFxcUm+ePm5mY3IWQ0Gq3Ks2bNqg0bNmj9+vXKmzevXnvtNfXv31916tSxmWvAgAH6559/VKJECQ0cOFAGg0Hff/+9rl69qsDAQNWtW1e5cuXSzJkzbZJc77//vjw9PVWqVCl17NhRMTExNs+TOXNm7dmzR3ny5FGDBg1UuHBhzZs3T0uWLFGHDh2e+Dm4u7s/MQnm7u4uo9GYYH327Nm1d+9eFShQQI0aNZKvr69atGihy5cvW9oMHDhQbdq0Udu2beXv76+VK1dq3bp1ypYtm9V5Tnnz5tVHH32kd955R9WqVdPhw4clScHBwWrUqJEaN26sPHnyaN++fVq2bJlcXV2tniuh51yzZo2CgoL04YcfqkCBAqpRo4Z2794tDw+PBJ/r8Z+7vd+ZR5/P4+WP95UkLy8vrVy5UtOmTVPhwoXVu3dvjR49Wl26dLFqN3bsWA0bNkwff/yxChYsqFatWqlBgwb67rvvrLYfJva7PWnSJA0bNkzly5fXli1bVK1aNX3xxRcaN26cihUrpsGDB2vSpEmqXr261efv6O9J3bp1tXPnTv3+++8KDAxUnjx51LRpU8tLDRz52wQApB6D2d7/JgMAAACAJNi1a5c6dOigyMjItA4FAPAfxAouAAAAAE/N1dU10VVwAAA8S6zgAgAAAAAAQIbGCi4AAAAAAABkaCS4AAAAAAAAkKGR4AIAAAAAAECGlvh7qQHg/8THx+vixYvy9PS0en03AAAAAADPgtls1q1bt5QvXz45OSW+RosEFwCHXLx4Ub6+vmkdBgAAAADgP+avv/5SgQIFEm1DgguAQzw9PSU9/A+Ll5dXGkcDAAAAAHjemUwm+fr6Wr6PJoYEFwCHPNqW6OXlRYILAAAAAJBqHDkmh0PmAQAAAAAAkKGR4AIAAAAAAECGRoILAAAAAAAAGRoJLgAAAAAAAGRoJLgAAAAAAACQoZHgAgAAAAAAQIZGggsAAAAAAAAZGgkuAAAAAAAAZGgkuAAAAAAAAJChkeACAAAAAABAhkaCCwAAAAAAABkaCS4AAAAAAABkaCS4AAAAAAAAkKGR4AIAAAAAAECGRoILAAAAAAAAGRoJLgAAAAAAAGRoJLgAAAAAAACQoZHgAgAAAAAAQIZGggsAAAAAAAAZGgkuAAAAAAAAZGgkuAAAAAAAAJChkeACAAAAAABAhkaCCwAAAAAAABkaCS4AAAAAAABkaCS4AAAAAAAAkKG5pHUAwPOqfv36+vTTT/XHH39o1apVmjdv3hP77Nu3T5MnT9batWuTNFelSpW0fv165c+f31L2/fffa/To0bpz547MZrPq1aun8ePHy8PDQ5JUuXJlrV69WoUKFUrSXAEjtsjJ6JGkPgAAAACA9CEy9NW0DuGZYAUX8BTGjBkjf39/yxUYGKhffvlFkhQbG6vY2FjFxMQoNjbW0mfq1Kny8/NTvnz5VKlSJX377beWukd9/m3dunXq3LlzonFER0db9YuIiFDfvn21YsUK/frrrzp+/Ljy5s2rbt26JToXAAAAAAAZEQku4CkMHz5c4eHhlitv3rz666+/Emw/e/Zsbdy4UXv27NHFixf11VdfaejQodqxY4elzY8//qiAgAC1atVKkv1E1IoVKxQQEGC5IiIirOrDwsJUo0YNFS5cWJLk7OysDh066H//+18KPTkAAAAAAOkHCS4gBYWHh6t3794KCAjQoUOHbOq//vprjRs3Trly5ZIklSxZUkOHDrXavlizZk2FhYVp5cqVCc4TFBSksLAwy1W8eHGr+pdffln79+/Xli1bdP/+ff3+++/68MMP1aZNmxR6UgAAAAAA0g/O4AJSyM6dO1WiRAlt3rxZklS7dm2bNgaDQfHx8VZlcXFxcnL6/7nmqKgoHT58WJ6envLz87M714oVKzR27FjL/eMruHLmzKmNGzdq4sSJGj16tLJmzaq33npL7777rsxmc3IfEQAAAACAdIkEF5ACzGazgoODNW7cuETbvfvuuxo8eLBWrlypPHny6Ndff9Xo0aP1xRdfWNqcOXNGY8eOVdGiRTVlyhQZDAabcYKCgtSoUSNlyZJFLi4uCggIsNQdPXpUc+fOldlslqurq/Lnz6+sWbPq6NGjOnjwoGJjY9W3b98nPlN0dLSio6Mt9yaTyZGPAgAAAACAVEeCC0gBgwYNUqlSpVS3bt1E23Xs2FHx8fFq3Lixbt68qbx582rmzJmqWbOmpc0LL7xgdfB8sWLFtHHjRvn7+8vZ2VmxsbFydnZW1qxZ9eWXX6pEiRJWc1SsWFGzZ8+23Ddt2lTdu3dX7dq19cEHH2j//v3av3+/fv/990RjDQkJ0ahRo5LwKQAAAAAAkDZIcAFPwWw2a9y4cTp06JA2bdqkTz75RAsXLpQknTt3zm6fTp06qVOnTnbr/P39rd50KEkVKlTQ5cuXdf/+fRkMBrm7uycpRoPBYFkF9u/E1wcffCAvL68E+wUHB6tfv36We5PJJF9f3yTNDQAAAABAaiDBBSST2WxWixYt5OzsrA0bNihTpkzq37+/+vfvL8n+GVyStGbNGn300Ud265ycnGwSXNLDJFWmTJmSFN+tW7c0evRo5c2bV99++62+++47q/oiRYooR44cCfY3Go0yGo1JmhMAAAAAgLRAggtIJoPBoA8//PCJ2xIf98Ybb+iNN96wW7d9+3ZNnz5dvXr1sqkbOHCg6tWrp4YNG9rUjRw5Urlz57Yqy5Qpk958880ED5Vv3bq13n77bVZlAQAAAAAyPBJcwFNIanLrSVxcXBJMSF25ckW3bt2yW/fmm2/alDk5OWnUqFH666+/7I4ZFxcnHx+fp4oXAAAAAID0gAQX8Iy4uLjI1dU1xcaz9zbFxNy4cUO//PKLzp8/n2IxAAAAAACQHpHgAp6R7du3S5IiIyMdTnRlyZJFefPmtVvn5+envn37auTIkXbrO3XqpL59+1ruvby85OzsrICAgATnGzlypN3VX4kJG9Uw0cPpAQAAAABIbQZzQvuhAOBfTCaTvL29FRUVRYILAAAAAPDMJeV7qFMqxQQAAAAAAAA8E2xRBJ6xwMBAfffdd8qfP7+lbPXq1VqwYIF+++03SZKrq6tKly6tTp06qWnTpkmeo379+po8ebLKly//TPtIUsCILXIyeiQ1RAAAAABIlyJDX03rEJACSHABT2HTpk1W515J0uXLl7V//375+/tLkmJiYhQbG2upnzBhgjZt2qTp06erXLlyMhgMMpvN+vXXX9W/f3/98ssv+uijjyzt16xZo27duilXrlyWsn/++UfTp09X27ZtJUmxsbFWc+zbt0+dOnWyiuv8+fPatm2bXnrpJbt9AAAAAADIqNiiCDyFxo0bKzw83HL98MMPcnJyUp48eRLs8/3332vo0KEqX7685c2IBoNB5cqV0+jRo7VmzRqr9mfPnlX37t0VFhZmuXr06KEBAwYoICBAAQEBOnTokFWfl19+2SquDRs2qHDhwqpYsWLKfwgAAAAAAKQxElxAChoyZIg6dOggHx+fBNu88847Gjx4sLZs2aJbt25Jkm7fvq1t27apT58+eu+99544j8FgUP/+/S0Jr8qVKyfY9vr163rnnXe0ePFizZ07V/7+/vL399fBgweT/HwAAAAAAKRHbFEEUsiiRYu0ZcsWnT17VqNGjdKqVaskSREREVbtOnfuLHd3d7Vt21a+vr66deuWsmTJoosXL2rKlCl65513HJrvypUrCg8PlyTdvXvXbpvffvtNHTp00IMHD7R06VJNnDhRvXr1kiTVrl070fGjo6MVHR1tuTeZTA7FBQAAAABAamMFF5ACvvnmG40YMULFixfX1KlTNWLECMvqquLFi9u0L1KkiCpUqKCjR48qIiJCx44dU5UqVVSgQAGH5jObzdq0aZOGDRumYcOG6dy5c1b19+/f16RJk/TGG2/o888/1969exUdHa2AgABt377doTlCQkLk7e1tuXx9fR3qBwAAAABAamMFF/AUYmNjFRISomXLlmnnzp3KkyeP3njjDQUFBemzzz5T9uzZLW23bNmi//3vf5KkP//8U2fPntXIkSMt9adPn9aiRYu0a9cuSVKlSpX02muvydfXVz169NCKFSssba9du6a5c+eqRYsWkmxXY9WvX181a9bUgQMH5OnpKUn67LPPdPDgQTk5OZbXDg4OVr9+/Sz3JpOJJBcAAAAAIF0iwQU8hZdfflmlS5fWgQMHlDVrVkkPD5GfMmWKdu3apZYtW1ralihRQkaj0XL/7rvvWo31eJIqf/78kqTWrVurdevWSYpr586dcnGx/fOuUqWK5d/t27dPNGFlNBqt4gUAAAAAIL0iwQU8hY0bNypnzpxWZU5OThowYIBN26JFi6po0aKSpL1792rJkiU6evSobty4IU9PT1WsWFHvvvuuatWqZXeuO3fuyGg02k1cPe5Rm6ZNmyoyMtJum6xZs+r1119/4lgAAAAAAKR3nMEFPIVHya327dsn+FbCYcOGKVeuXJb76dOnq3fv3mratKk2btyoU6dOafPmzWrWrJl69+6t6dOn2x3nww8/1Pr16+3WzZkzR2XLlrUp37Bhg+UssMcvs9mss2fPJvWRAQAAAABId1jBBaSAa9euJfgmw6CgIKv7r776StOmTVONGjUsZbly5VLz5s2VNWtWDRw4UL1797YZJz4+XvHx8Xbn8Pf3T3LMLi4uMpvNSe4HAAAAAEB6Q4ILSAEGg8Hhtg0bNtS4ceM0efJklSlTRgaDQWazWb/88ovGjBmj+vXrP/Ucz1LYqIby8vJK6zAAAAAAALAgwQWkgJIlS6p9+/aWNxY+rlOnTurbt68kacSIEVqwYIG6d++uc+fOWdqUKFFCHTt2VLt27eyO4efnp759+1q9efHf3njjDY0ePTpJMZOoAgAAAAA8Dwxm9igBcIDJZJK3t7eioqJIjAEAAAAAnrmkfA9lBReAJAkYsUVORo+0DgMAAKSxyNBX0zoEAAAseIsikEH06tVL/v7+dq/8+fNr8ODBSRqvfv36On78+DOKFgAAAACA1MMKLiCDmDFjRoJ1u3bt0rRp0yz3+/fvV5cuXaza3Lt3TzNmzNCrrz78v62xsbGKjY19JrECAAAAAJCaSHABz4F79+4pS5Yslvtq1aopLCzMqs0LL7yQ4CH4AAAAAABkZGxRBJ4D165dU44cORKsP3HihK5fv64aNWqkYlQAAAAAAKQOVnABz4GLFy+qaNGiCdaPGjVKgwYNksFgcHjM6OhoRUdHW+5NJtNTxQgAAAAAwLNCggtI5yZMmKCFCxcm2ubKlStyd3fXrFmzFBQUpJEjR1rq1q1bp++//16LFy9O0rwhISEaNWpUckIGAAAAACBVGcxmszmtgwDwbBw+fFhvvfWWKleurHz58lkdRF+7dm1NnjxZL7zwgt2+9lZw+fr6yrfPSjkZPZ516AAAIJ2LDH01rUMAADznTCaTvL29FRUVJS8vr0TbcgYXkEGcOnVKN2/etFt38+ZNnTp1yqrsf//7n1q1aqWVK1dq2bJlOn36tEJCQhyez2g0ysvLy+oCAAAAACA9IsEFZBDDhw/XgQMH7NYdOHBAwcHBlvv58+frnXfe0erVq1W5cmW5urpqzZo1+umnnzRhwoTUChkAAAAAgFTBGVxABtK5c2dlzpzZpvzOnTuqVKmS5f7atWs6cOCAcubMaSlzd3fX2rVrFRUVlSqxAgAAAACQWkhwARnIvHnz1KhRI5vyzZs3a86cOZb7wYMH2+1vMBjk4+PzrMIDAAAAACBNkOACMpCE3gmRmu+KCBvVkPO4AAAAAADpCgkuIIMoVKiQ3nvvPXl7e9vUmUwmBQUFJWk8V1dXubq6plR4AAAAAACkGYM5NZd+AMiwkvJ6VgAAAAAAnlZSvoeyggtAkgSM2CIno0dah4EMKDL01bQOAQAAAMBzyimtAwCed/Xr19fx48dtyidMmKDixYtbri+//DLZc1SqVEkXLlx4mjABAAAAAMiwWMEFPIVDhw6pXbt2VmVXrlzR2LFj9cEHH0iSYmNjFRsba9N38ODBCb7t8N/WrVunDRs2aN68eQm2iY6OtpmjZcuWOnLkiDJnzmy3T8WKFbVkyZInzg8AAAAAQHpHggt4CpUrV1Z4eLhVWbdu3WQwGBLss3btWg0fPtym/NKlS8qSJYtOnTold3d3S7m9BNmKFSs0duxYy31ERITNeL/99pu2bt2qEiVKOPw8AAAAAABkRGxRBFLQ/fv3tXHjRjVv3jzBNi1atFBYWJjl2r17t9q0aaOsWbNq/PjxVsmthAQFBVmNUbx4cbvtEku0AQAAAADwvGAFF5CCPv74YzVr1kxDhgzRTz/9JEn6888/bdqdP39eW7du1fr167V7927duXNHQ4cOlb+/v2JiYuTm5pboPI6s4Hpa0dHRio6OttybTKYUnwMAAAAAgJRAggtIIaGhodq8ebP+97//WZ17Vbt2bcu/IyIi1LRpU2XPnl0vv/yyBg0apLVr1yo8PFy7du3SlClTFBYWpmXLlql06dKS7K/CCgoKUqNGjZQlSxa5uLgoICDApo3BYFBcXFyynyckJESjRo1Kdn8AAAAAAFILCS7gKV28eFEDBgxQZGSktm/fnuCh7pJUvHhxnThxQs7OzlblpUqVUqlSpSwH0/9bsWLFtHHjRvn7+8vZ2VmxsbFydnZW1qxZ9eWXXyZ4xlbdunXVvHlzS4Ls7NmzKlq0qKW+YMGC2rp1a4KxBgcHq1+/fpZ7k8kkX1/fBNsDAAAAAJBWSHABT2HRokUKDg5W79699eWXX8rV1TXBthMmTNDChQttyk+fPq2SJUvalAcFBWnkyJGqUKGCLl++rPv378tgMDh0RpckTZs2TdOmTbPcu7u72xyInxij0Sij0ehwewAAAAAA0goJLuApvPLKKwoPD5e3t/cT2w4ePFiDBw+2KXck8WQwGJQpU6ZkxwkAAAAAwPOMtygCT6FQoULy9vZWly5dtG7dOrttxo8fL39/f8t91apV5e/vb7myZs1q+XeRIkU0btw4u+MMHDhQW7ZssVs3cuRI5c6d++kfCAAAAACADIgVXEAKiImJUWxsrN26atWqWd0fOHAgwXE2bdqkuXPn2q27cuWKbt26ZbfuzTfftPw7oa2QhQsXtkq0PfJoKyQAAAAAABkVCS4gBdh702FyxzGbzU81R0JbIVNK2KiG8vLyembjAwAAAACQVCS4gBTg5+envn37JrgS6o033tDo0aOfOI6Tk5OcnOzvHH7SHJ06dVLfvn0dDRkAAAAAgOeGwZzQchEAqe7Bgwe6d++esmTJktah2DCZTPL29lZUVBQruAAAAAAAz1xSvoeyggtIR5ydndNlcuvfAkZskZPRI63D+E+KDH01rUMAAAAAgHSJtygC6dy+ffv0xhtvpHUYAAAAAACkWyS4gDQ2a9Ys+fv7W13lypXTN998I0mKjY1VTEyMpX1cXJwqVKggX19fZcuWTQEBAZbLx8dHhQoVUsOGDS3tv/jiC73yyit25549e7bat2//bB8QAAAAAIBnjAQXkMa6d++u8PBwy7V3716ZTCa5urrabe/i4qJjx44pJCRErVq1UlhYmOVq2rSpPvvsM23ZssXSvkOHDrp9+7ZWrFhhNc7ly5cVEhKi0NDQZ/p8AAAAAAA8ayS4gHTkwoULql+/vvr27atmzZqlyJhOTk6aNWuWhgwZort371rKBw0apN69eytv3rwpMg8AAAAAAGmFBBeQTmzdulWVK1eW0WhUu3btUnTsypUrq1GjRgoJCZH08Fyvn3/+Wb17906wT3R0tEwmk9UFAAAAAEB6RIILSGPh4eF6++23NWrUKK1du1YtW7ZU1apV1bt3b505cybF5hk/frwWLVqkiIgI9ejRQzNnzpSLS8IvUg0JCZG3t7fl8vX1TbFYAAAAAABISQaz2WxO6yCA/6o//vhDb7/9tvr27as333xTLVu21KJFi+Tu7q5vv/1W+fLlU1xcnJo2baoCBQqoYsWK6tGjh7p166bo6GhdvHhRRYoUsYx35swZFSlSRK6urlq/fr1VnSTNmzdPH3/8sWrXrq2vvvoq0diio6MVHR1tuTeZTPL19ZVvn5VyMnqk7AcBh0SGvprWIQAAAABAqjGZTPL29lZUVJS8vLwSbUuCC0hHChcurMOHDytHjhyWsl27dmny5MnasGGDVduwsDB169ZNe/futZT5+/tr165dypMnj93xzWaz8ubNq23btqls2bJJiu3Rf1hIcKUdElwAAAAA/kuSkuBKeH8SgDRjNpt19epVRUREKDY2NsXGNRgMcnd3V6ZMmVJsTAAAAAAA0hoJLiANzZw5U8uWLZPZbFZsbKwuXryoKlWqyGg0Kk+ePCpRooSaNGmS1mECAAAAAJCukeAC0tD777+vVq1aSZJcXV2VJUsWubm5WbXZtWuX5d8TJkzQwoULrer9/f2t7mvXrm35d1BQkEaOHJmiMQMAAAAAkN6Q4ALSUKZMmZK0XXDw4MEaPHjwU83p5uYmV1fXZPcPG9XwiXufAQAAAABITSS4gHTO1dX1qRJSjzt9+nSKjQUAAAAAQHrAWxQBOCQpb68AAAAAAOBp8RZF4Dmyb98+ffLJJ1qzZk1ahyJJChixRU5Gj7QOI8VEhr6a1iEAAAAAAJ6SU1oHAPzXzZo1S/7+/lZXuXLl9M0330iSYmNjFRMTY2kfFxenChUqyNfXV9myZVNAQIDl8vHxUaFChdSwYUNL+6tXryp//vz67bffbObu0KGDPvnkk2f/kAAAAAAAPEOs4ALSWPfu3dW9e3fL/dWrV/XCCy8keO6Wi4uLjh07pqVLl2rv3r2aM2eOpe6dd95RUFCQmjZtainLkSOHQkJC1LFjR+3du1dOTg/z2uvXr9epU6e0YMGCZ/RkAAAAAACkDlZwAenIhQsXVL9+ffXt21fNmjVLsXHfffddZc+eXVOnTpUkXb9+Xb1799aiRYvk7OycYvMAAAAAAJAWSHAB6cTWrVtVuXJlGY1GtWvXLsXH//zzz/XJJ5/ozJkz6tWrlz744AOVKlUqxecBAAAAACC1sUURSGPh4eEaNWqU/vzzT61du1Y//vijqlatqsaNG6tnz54pNk++fPk0fvx4NWjQQLlz59aXX36ZaPvo6GhFR0db7k0mU4rFAgAAAABASmIFF5CG/vjjD7333nt64403tHfvXk2YMEFdu3ZVWFiYXn75ZV26dEmStGvXLvn7++vtt9/W3r17FRAQoFGjRmnJkiVWh8yvWrVKAwYMUNmyZXXu3Dmb+V5++WX99ddfql69+hO3JoaEhMjb29ty+fr6PpPPAAAAAACAp2Uwm83mtA4CwEOFCxfW4cOHlSNHDkvZrl27NHnyZG3YsMGqbVhYmLp166a9e/dayvz9/bVr1y7lyZPHZmyz2axatWqpdevWGj9+vL7//nuVL18+wVjsreDy9fWVb5+VcjJ6PM1jpiuRoa+mdQgAAAAAADtMJpO8vb0VFRUlLy+vRNuyRRFIh8xms65evaqIiAjFxsamyJgzZsxQ1qxZ1aNHD/n4+KhTp0766aefElzJZTQaZTQaU2RuAAAAAACeJRJcQBqaOXOmli1bJrPZrNjYWF28eFFVqlSR0WhUnjx5VKJECTVp0uSp5/n99981ceJEHTp0SJLUtm1bLV68WFOmTNHAgQOfenwAAAAAANISCS4gDb3//vtq1aqVJMnV1VVZsmSRm5ubVZtdu3ZZ/j1hwgQtXLjQqt7f39/qvnbt2pZ/BwUFacSIEXrvvfc0ZswY5cuXz1I3e/ZsValSRS1atFDx4sVT6IkAAAAAAEh9JLiANJQpUyZlypTJ4faDBw/W4MGDkzTHnDlzlClTJr333ntW5UWLFtXQoUPVrVs3bd++PUljAgAAAACQnnDIPJDO7du3T5MnT9batWvTNI6kHO4HAAAAAMDTSsr3UBJcABxCggsAAAAAkJqS8j3UKZViAgAAAAAAAJ4JzuACkCQBI7bIyeiR1mEkSWToq2kdAgAAAADgGWIFF5BGSpcunWh9586dtW/fvlSKBgAAAACAjIsEF5BG7t69m2h9bGysYmNjUykaAAAAAAAyLhJcAAAAAAAAyNBIcAEpZMuWLapSpYrKli2rgIAAdevWTXFxcZKkQ4cO6aWXXlL58uVVoUIFffPNN1Z9b9++rffee08BAQEqU6aMevbsqZiYGEv98uXL1bdvX/Xq1UuBgYFavXq1JGnHjh2qWLGiSpUqpcDAQG3fvt3SZ+nSpSpbtqwqVqyoypUr6/Lly5KkyZMnq0yZMgoMDFS1atXEi1QBAAAAABkdh8wDKSRr1qzauHGjcubMqbi4OL322mtatGiR2rRpo6ZNm2rJkiVq0KCBTCaTmjZtatV34MCBMpvNOn78uJydnTVp0iR9/vnn6tKliyQpJiZGq1ev1qxZszRjxgxJ0vnz59WzZ09t3LhRRYsW1W+//aYGDRroyJEjypw5s4YPH67jx4/Ly8tLZrNZBoNBZ8+e1VdffaWjR4/Kzc3NUm5PdHS0oqOjLfcmk+kZfXIAAAAAADwdVnABKaRKlSrKmTOnJMnFxUVNmzbVkSNHtGnTJlWsWFENGjSQJHl5eWnkyJFWfZctW6Zx48bJ2dlZktS/f3/lz5/fqk2WLFn02muvWe5nz56tnj17qmjRopIkPz8/NWrUSBs2bFB8fLwMBoMePHggSZYkltlsltlsVnx8vFW5PSEhIfL29rZcvr6+yf1oAAAAAAB4pkhwASnkwoUL6t27t6pUqaJSpUopNDRUd+/e1R9//KEyZcpYta1UqZLl39evX5erq6vy5ctnKXNyclLFihWt+pQqVcrq/uTJk5oyZYoqVKhguXbs2CGTySQPDw+NHj1aL774osaPH69bt25JkooVK6Y2bdqoQoUKmjVrltU2yMcFBwcrKirKcv3111/J/mwAAAAAAHiWSHABKSA2NlY1a9ZUjhw5tHbtWp06dUpDhgyR9DBZ9fg5V49WUCVU/3gbSfLw8LC6v3fvnkJCQnTs2DHLFRERoQ8//FCS9M477+jw4cO6ffu2AgMDLWdwDRgwQLt379aRI0dUo0YN3bt3z+4zGY1GeXl5WV0AAAAAAKRHJLiAFHD8+HF5eHho+PDhlq2FJ06ckCSVLFlSYWFhVu337t1r+bePj4/c3Nx04cIFS1lsbKx++umnROcsUaKEDh06lGgbLy8vjR8/XrVq1dLixYst5blz59b8+fPl6empjRs3OvaQAAAAAACkUyS4gBSQK1cuXb582bJK6sCBA9qwYYMkqWHDhoqMjNTWrVslSVeuXNHEiROt+nft2lVDhw7VgwcPZDabNXTo0ES3D0pShw4dNH/+fKtkWWRkpKSHh9LfuXNH0sPD4s+cOaP8+fPr7t27lnEfbTt8/KwvAAAAAAAyGt6iCKSAggULKiQkRHXr1pUkFS1aVJMmTdKGDRvk7Oysb775Rl26dFH//v2VOXNmTZgwQd27d7f0Hzp0qHr16iU/Pz9lyZJFTZs2VcuWLeXq6irp4XZBo9FoNWelSpW0atUq9e/fX7dv35abm5vKlCmjpUuX6ty5c2rUqJHc3d1lNpvVvHlzBQUFadeuXerQoYOyZMmi+Ph4de3aVS+99FKSnjVsVEO2KwIAAAAA0hWD2d7hPwDwGJPJJG9vb0VFRZHgAgAAAAA8c0n5HsoWRQAAAAAAAGRobFEEkCQBI7bIyejx5IYOiAx9NUXGAQAAAAD8t7GCCwAAAAAAABkaK7iA51idOnV08eJFu3W3b9/WO++8o9DQ0FSOCgAAAACAlEWCC3iO/fDDDwnW7dq1S7NmzUrFaAAAAAAAeDbYogj8R0VFRSlbtmxpHQYAAAAAAE+NFVzAf9TZs2dVrFixBOujo6MVHR1tuTeZTKkRFgAAAAAAScYKLuA/6vDhw3rhhRcSrA8JCZG3t7fl8vX1TcXoAAAAAABwnMFsNpvTOggAKWvChAlauHBhom3++OMP5cuXT66urgoKCtLIkSOt6u2t4PL19ZVvn5VyMnqkSJyRoa+myDgAAAAAgOePyWSSt7e3oqKi5OXllWhbElzAf4i7u7vu37+frL6P/sNCggsAAAAAkBqSkuBiiyIAAAAAAAAyNBJcAAAAAAAAyNBIcAEAAAAAACBDc0nrAABkLGGjGj5x7zMAAAAAAKmJFVzAf4jRaEzrEAAAAAAASHEkuID/kKioqLQOAQAAAACAFMcWRQBJEjBii5yMHk81RmToqykUDQAAAAAArOACAAAAAABABkeCC+lawYIFJUk7duxQ586dHeqzbt06denSJUnzrFu3zuHxH9m3b59atGiRpD7PwoULFxQYGJikPsePH1f9+vWfUUQAAAAAAKQutigizU2ZMkWLFi1SfHy8pWzu3LmqVq2a7t69K0mKjY1VbGysJGnBggWaNGmSpe2DBw907do1/f7778qaNatiY2MVExNjM8+cOXM0ceJE3b17V2XLltXnn3+uokWL2owvSbt371aPHj1kMpnk4eEhF5eHfyrXr19Xjhw5tGvXLps+j7z++us6evSoMmXKZPd5AwMDtXz5coc/n2bNmuns2bOW+/j4eOXOnVs7d+60xP7487Zv314///yz5d5sNitv3rzavn273ecFAAAAACAjI8GFNLdz505NnTpVdevWdah9p06d1KlTJ8v9gQMH9M4778jb2zvBPps3b9akSZP0448/qkCBApo9e7aaNm2qsLAwOTnZLmSsVauWwsLC1KhRIw0bNkzVq1fX/fv3VbhwYf3yyy+JxhcREaHt27erRIkSDj3Pk6xfv97qPjY2VtmyZUu0z5dffmlTljlz5hSJBwAAAACA9IYtikhzZrNZzs7OyeobHR2tHj16aPz48XYTVY/Mnj1boaGhKlCggCTpgw8+UO7cubVp06ZkzfskBoPhmYwrSadOnZK/v3+S+yU1pujoaJlMJqsLAAAAAID0iAQXMqy4uDi1bdtWFy9eVO3ata3q1q5dK39/f73//vuSpJ9//lm1atWyatOwYUMdOHDgmcTWpEkTBQQE2L3GjBnzVGN/++23atKkSZL6mM1mqy2gjggJCZG3t7fl8vX1TVJ/AAAAAABSC1sUka7dvHlT/v7+unPnjtUWxosXL+rtt99W9erV1bRpU1WrVk0hISF66623JEktWrTQokWLLO1v3LihrFmzWo2dM2dORUZGWu7Xrl2rAwcOqHr16po/f75D8e3atUv+/v4qU6aMvvnmG0v5999/r+LFiyfjiRN3+/ZtLViwQF9//bVKlSols9ms2NhYm/O+3nrrLR0+fFhGo1HSw9VbjRs3TtJcwcHB6tevn+XeZDKR5AIAAAAApEskuJCu+fj4KDw8XJs3b9aKFSsUFxenTz75RF988YXGjRunN998U9LDg9t79eqluLg4ubq62oyTPXt2Xb9+Xblz57aUXbhwQXnz5rXcP54Uc0Tt2rW1YcMGq7LixYurXr16cnd3t9unYMGC2rp1a5LmeWTIkCF6++23VbVqVZ06dUqSFBkZqaZNm1q1O3XqlHbu3KnChQsnax5JMhqNlgQZAAAAAADpGQkupDmDwSCz2Wy5j46OVkREhPLnz2/TNi4uTmazWT///LOyZMliKS9Xrpx27dolSfrpp58UGBho1a969erasmWL3n33XUvZ+vXrNXXq1CTHe/PmTV2+fDnB+m+//TbJYzpixowZOn78uH744YdnMj4AAAAAABkVCS6kuUqVKqlNmzby9PSUi4uLDAaDihUrprFjx9q0dXd315AhQyQ93KYYGhqqHTt2KD4+XmazWXnz5lW3bt3Uq1cvq359+/bVW2+9pYCAAPn7+2vcuHFyd3dXjRo17Ma0atUqTZs2TSdPnlSnTp1kMBgUFxcnd3d3vfbaa6pWrVqSt/wlV0xMjIKDg/XTTz9p48aNdleoPc5gMOjOnTsymUy6e/eubt26pd9//12nTp1SXFycXnnllVSIHAAAAACA1EGCC2lu5MiRGjlyZJL63L17V7Vq1VLfvn01ceJEy3bAEydOqFu3bvr777/Vp08fS/vKlStr3rx5+vDDD3X58mXVrFlT69evT3D85s2bW5JAzs7O8vDwsNmu92jF2LMUFRWlF198UQ0aNNAPP/wgNzc3h/q9/vrrCgoKkiR5eHgoe/bsKliwoEqWLKnq1as/y5ABAAAAAEh1JLiQIZ08eVLu7u7q3r27VXmZMmUUEhKijz76yCrBJUn169dX/fr1HRrfzc1NOXLkSFJMEyZM0MKFCx1uHxQU9MTEnre3t7Zs2aJChQolKZYxY8Yk+rbGw4cPJ2m8fwsb1VBeXl7J7g8AAAAAQEojwYV0zcPDw2556dKlFR0drUWLFqlNmzaWlU1nzpzRqFGjLIfPp6bBgwdr8ODBKT5uUpNbAAAAAAD815DgQrr2559/SpJcXV2tzp7y8PDQzp07NW7cOE2ePFnx8fEyGAzKlSuXunXrptatWydpnsfHf1Z9ngUXF5cMGzsAAAAAACnBYP736+sApKrXX39dkZGRduvee+89m8Py05LJZJK3t7d8+6yUk9H+yroniQx9NYWjAgAAAAA8rx59D42KinriUTms4ALS0Lfffpsq80yYMEHz5s2z3A8fPlzt27dPlbkBAAAAAHjWSHAB/wHP6nwwAAAAAADSAxJcwHNs7dq1Gj58uE35pUuXlCVLFp06dUru7u5pEBkAAAAAACnHKa0DAPDstGjRQmFhYZZr3759ateunbJly6aQkBCSWwAAAACA5wIruIDn3O3bt7Vt2zZ98803Wr9+veLi4hQaGqqXXnop0X7R0dGKjo623JtMpmcdKgAAAAAAycIKLuA5dfbsWVWuXFkvv/yytm3bpo4dO+rq1av64YcfdOfOHfXo0UNlypTRyZMn7fYPCQmRt7e35fL19U3lJwAAAAAAwDEGs9lsTusgAKQ8s9lseaVqcthbweXr6yvfPivlZPRI1piRoa8mqx8AAAAA4L/n0XfaqKgoeXl5JdqWLYrAc2jSpEn68ssvHW7fqlUrffzxx1ZlRqNRRqMxpUMDAAAAACDFkeACnkMDBw7UwIED0zoMAAAAAABSBQku4Dl2+/ZtTZo0SRs3btSVK1ckPdy6mCtXLr366qsaOHCgsmTJksZRAgAAAADwdDhkHniOvf/++7p165Y2bdqkP/74Q3/88Yf+/PNPbdy4UVFRUerSpUtahwgAAAAAwFNjBRfwHDMYDInWOzklPccdNqrhEw/3AwAAAAAgNbGCC3iOzZs3T56enmrUqJEKFiyoAgUKyNfXV6+++qq8vb01Z86ctA4RAAAAAICnZjCbzea0DgJA+peU17MCAAAAAPC0kvI9lC2KAJIkYMQWORk9HG4fGfrqM4wGAAAAAAC2KAIAAAAAACCDI8EFpDN+fn66efNmuuwDAAAAAEB6RIILSIYGDRroxx9/TFbfqVOnys/PT3nz5lXp0qVtDnqPjo5WXFyc5X7atGnKly+fAgICLFfWrFm1Z8+eBPusXLlS/v7+lsvPz0+enp7666+/EuwDAAAAAEBGxRlcQDLExMQoJiYmyf1mzpypLVu2aO/evcqZM6fCw8PVvHlzTZw4UR4eD8+1unjxolWfiIgIjR8/Xh06dLCUtW/fXm3btrUcsvd4n1atWqlVq1aW+71792rIkCEqUKBAkmMGAAAAACC9I8EFpKIFCxZo0aJFypkzpyTJ399fw4cP18aNG/XVV19JkgoXLvzEcQwGg6ZMmaI333zziX0iIyPVu3dvrV27VkOGDNG6deskSRcuXEh0jujoaEVHR1vuTSbTE+MCAAAAACAtsEUReEoxMTGqU6eOli9f7lD7+Ph4m3tPT88kz3vx4kWFh4crPDxcsbGxdtv89NNPevPNNxUTE6NVq1YpNDTU0id//vyJjh8SEiJvb2/L5evrm+QYAQAAAABIDazgAp5Sjx499OKLL6pNmzZPbNu5c2cNGTJEK1asULZs2RQeHq6xY8fK29tbtWvXliT9888/TxzHbDbrq6++spwD9vhh8VFRUQoJCdGOHTu0Zs0aeXl5qXPnzqpcubLmzZunihUrPnGO4OBg9evXz3JvMplIcgEAAAAA0iUSXMBTmDVrlq5du6a5c+c61L5Hjx6Ki4vTyy+/rNu3bytHjhxasGCBatSoYWnz+HbDQoUKafDgwQoNDbWUXblyRZs3b1blypVt+sTExKhatWp67733tG/fPrm5uUmSVq1apW3btilLliwOxWo0GmU0Gh1qCwAAAABAWiLBBSTTnj179OmnnyoyMlIGg8Hhfr1791bv3r0dbj9w4EANHDjQ4fZubm46fvy4XFxs/7zr169v+XefPn2StTUSAAAAAID0hgQXkExffPGFqlWrpjlz5iQpAZVcJpNJnp6eDiXTHiW3AgMDE3zbo6+vr3r16pWiMQIAAAAAkBZIcAHJNG3aNNWtW1dVqlRR/fr1VaFCBYf7tmjRQgMGDNDLL79sU7d69Wply5bNpvz111/X5MmTFRgYaFP33XffKXv27DblR44csTu/2WxWwYIFdePGDbv9AAAAAADISEhwAcnk7e0tHx8fLViwQO3atdPBgweVKVMmh/rGxsYm+ObDF154wW55fHy8zRsYHylbtqxjQf8fg8EgZ2dnmc3mJPWTpLBRDeXl5ZXkfgAAAAAAPCtOaR0AkBG5ublZDm+vUaOGWrZsqeDgYIf7J+XMrqfpAwAAAADAfwEruIBk2Lp1q9X9yJEjk9Tfz89P7du3T/CQ9z59+uj999+3KvP399dbb72lzJkz2+3z4YcfqmvXrg7HUK5cOd6SCAAAAAB4LhjMydmjBMDG2LFjtXr1art15cqV0+LFi1M5opRlMpnk7e0t3z4r5WT0SLRtZOirqRQVAAAAAOB59eh7aFRU1BOPyiHBBcAhJLgAAAAAAKkpKQkuzuACnqHcuXM73Nbf3193797VggULNHr0aKu6vn37Kn/+/PL397d7ffLJJ4mO/fvvv6tq1arJegYAAAAAANI7zuDCf16DBg00bNgw1axZM0n95s+fr8mTJ1vuHzx4IJPJpNOnT8vb21uSdOfOHZt+Xbp00Y8//ihJ+uabb1SmTBlJ0v379xUfH6/Y2FjFxMRY9QkPD9eKFStUo0aNJMX471jPnj2r69evK1u2bMkaAwAAAACA9IoEF/7zYmJibBJKjnj//fetDoI/cuSIXn/99QQPjn9k7ty5SZ7LbDYn+y2KW7Zs0apVq/Txxx8rKChIa9asUZYsWZI1FgAAAAAA6REJLiAFmM1m9evXT4MGDZKTU8I7f9u3b6+ff/5Z0sMVXxcuXFBkZOQTV1UZDAZFR0cnOaZp06bpk08+0ffff69y5crJbDarWrVqmjlzpmrVqpVo/+joaKs5TSZTkuYHAAAAACC1cAYX8C8xMTGqU6eOli9fnqR+o0aN0vXr19W9e3d16dJFAQEBCggI0L1796zaffnllwoLC1NYWJgWLlyowMBAh7YM1qtXTx9++KH8/f1VvHhxubm5WZ3BFRwcbNV+7dq1KleunPbs2aODBw+qXLlykqQPP/xQc+fO1eDBg/Xiiy9qx44dCc4ZEhIib29vy+Xr65ukzwQAAAAAgNTCCi7gX3r06KEXX3xRbdq0cbjPzJkztWLFChmNRi1dutRqC2JCWwFjY2P10Ucf6bPPPlOHDh10+PBhXbx4McE5+vfvr/79+0uSzp8/r9q1ays8PDzB9s7Ozvryyy8VGBhoU1e1alUdOHBAx48fT/QtFMHBwerXr5/l3mQykeQCAAAAAKRLJLiA/zNr1ixdu3bN4TOybt++rUGDBunnn3/W3r17ZTab1bhxYx0+fFihoaHy8PCw2y8+Pl5du3bVmTNn9M8//2jRokWSpMKFC9u0XbJkiU6cOGFVduvWLV27dk1Dhgyxad+wYUO98soratas2RPjL1++fKL1RqNRRqPxieMAAAAAAJDWSHABkvbs2aNPP/1UkZGRDh3mfufOHVWsWFHNmzfX7t275e7uLknat2+fRowYoWPHjqlatWo2/Uwmk9577z3Vrl1bQ4YMUZMmTTR79mzVr1/f7jxVqlRRkSJFbMrffvttu+2LFCmi3r17a8uWLU98hkf69++vzp07O9weAAAAAID0hgQXIOmLL75QtWrVNGfOHA0cOPCJ7TNnzqz9+/crZ86cVuVGo1GhoaF2+/z111969dVXNXToULVu3VqS9N1336lZs2b64Ycf7Pbx8/OTn5+fJGnjxo1auXKlfv31V92+fVs+Pj6qUqWK3n//fVWoUMHSZ/r06TbjREZGqlGjRoluawQAAAAAIKMiwQVImjZtmurWrasqVaqofv36VgmjhDxKbtWpU0fLli1T3rx5bdrMmDHD8u8CBQpo69atypMnj6WsVKlSOnnypFxdXROdq1+/fjp27JiGDRumihUrytPTU5cuXdKuXbvUqlUrhYSEqGXLlg4+LQAAAAAAzxcSXIAkb29v+fj4aMGCBWrXrp0OHjyoTJkyOdT38uXLio6Otlv33nvvWf5tMBiskluPPCm5JUnLly/XoUOHrA55z58/v9q2basHDx5o5cqVqZbgChvVMNHD6QEAAAAASG1OaR0AkNbc3Nzk5uYmSapRo4Zatmyp4OBgh/s7cmaXI9zd3eXkZP9PsmHDhgoODtbZs2ctZQ8ePND+/fs1Y8aMBM/wAgAAAADgv4AVXPjP27p1q9X9yJEjk9Tfz89P9erVsxw0/7iPP/5YrVq1euI4j87HcnV1tVnVNXfuXM2cOVNBQUH6+++/JT1MrJUtW1ajR49WkyZNEh3b2dlZLi4p8+ceMGKLnIz23xD5SGToqykyFwAAAAAAjjCYzWZzWgcBpDdjx47V6tWr7daVK1dOixcvTuWIkmfatGkKDQ2Vj4+P3fomTZpoypQpDo1lMpnk7e0t3z4rSXABAAAAAJ65R99Do6KinnhUDiu4ADuGDRumYcOGpXUYT+23337TpEmT1K5du7QOBQAAAACAZ4YzuIDnmNlsTrEzwgAAAAAASK9IcAHPMYPBkOAbHgEAAAAAeF6Q4AKeYzVr1tSUKVPk7+8vf39/OTk5Wf7t7++vd999N8G+0dHRMplMVhcAAAAAAOkRZ3ABz7G3335bb7/9tuXe3d3d8rbGJwkJCdGoUaOeVWgAAAAAAKQYElzAc2jDhg3au3evTXlcXJyGDBliU16lShW98cYbVmXBwcHq16+f5d5kMsnX1zflgwUAAAAA4CmR4AKeQwEBAfLx8bEpb9q0qd32+fLlsykzGo0yGo0pHRoAAAAAACmOBBfwHCpcuLAKFy4sSdq7d6+WLFmio0eP6saNG/Ly8lLFihX17rvvqmbNmmkbKAAAAAAAKYBD5oHn2PTp09W7d281bdpUGzdu1KlTp7Rp0ya99tpr6tWrl6ZPn57WIQIAAAAA8NRIcAHPsa+++krTpk3Ta6+9ppw5c8rFxUW5cuVS8+bNNWPGDC1fvjytQwQAAAAA4KmxRRF4jjVs2FDjxo3T5MmTVaZMGRkMBpnNZv3yyy8aM2aM6tevn+Qxw0Y1lJeX1zOIFgAAAACA5DGYzWZzWgcB4NmIj4/XggULtGTJEp07d85SXrJkSXXo0EHt2rVzeCyTySRvb29FRUWR4AIAAAAAPHNJ+R5KgguAQ0hwAQAAAABSU1K+h7JFEUCSBIzYIiejR6JtIkNfTaVoAAAAAADgkHngqU2fPl0BAQE2V5EiRZQnTx5FRkY6PNa+ffvUokULh9sXLFhQkrRjxw517tw5wXb37t2ztAUAAAAA4HlDggt4Sr1791ZYWJjVtWvXLpUpU0a1a9e2SSxNnDhR/v7+luvEiROWutjYWMXGxlq1nzJlisqVK2eVPNu/f78k6e7duwn2+7fFixfr0qVLioiISKnHBgAAAAAg3SDBBaSgBw8eaP78+apUqZJq1Kihr776Sk5O1n9mgwYNUnh4uOUqU6ZMomPu3LlTU6dOtUqgVatWzeGYjh07pnHjxunzzz9XUFCQLl++nKxnAwAAAAAgveIMLiAF3L59W4sXL9ann36qIkWKKHPmzLp06ZJOnjxplcAaNGiQvv/+e8t9bGysrl69qosXL8poNNod22w2y9nZOVlxLV++XP369dPy5ctVp04deXp6qkaNGgoNDU3SVkgAAAAAANIzVnABTyE8PFxvvPGGAgMD9fvvv2vDhg3auHGjjh8/rkqVKmnAgAEKCAjQkiVLJD3cnvjvlViTJ09W5cqVE0xuJdfu3btVrVo1ff7559q9e7fq1KkjSWrZsqW+/fZbff755woICNDXX3+d4BjR0dEymUxWFwAAAAAA6REruICnkDNnTg0YMEAvvfSSDAaDpdzV1VVt27ZV27ZtdefOHd25c8em79WrVzVo0CB9++23VuW7du2Sv7+/ypQpo2+++SbR+W/evCl/f3/duXNHdevWtaobN26cXnnlFZs+pUqV0ubNmxUREaH79+8nOHZISIhGjRqV6PwAAAAAAKQHJLiAp5A9e/YnnoeVOXNmZc6c2aosKipKTZo0kSTlypXLqq527drasGGDQ/P7+PgoPDxcmzdv1ooVKyzltWrVemLf4sWLJ1ofHBysfv36We5NJpN8fX0digsAAAAAgNREggtIpkmTJunLL790uH2rVq308ccf68SJE2rTpo2GDx+uuLg4NWzYUGvXrlX+/Pnt9jMYDDKbzZb76OhoRUREJNh+4sSJ+uKLLxyOKygoSCNHjrQpNxqNKb51EgAAAACAZ4EEF5BMAwcO1MCBA23K3d3d7W79u3v3rgYMGKAdO3Zozpw5eumllyRJ3t7eql27thYtWmR3nkqVKqlNmzby9PSUi4uLDAaDihUrprFjx9ptP2jQIA0aNMjhuAAAAAAAyOhIcAGpxGw2q2jRogoJCZGrq6ulvHHjxtqzZ488PDx05MgRm34jR460u8IKAAAAAAA8RIILSCWZM2dW9+7d7dblyZMnlaMBAAAAAOD5QYILyMA8PDxSfc6wUQ3l5eWV6vMCAAAAAJAQp7QOAHjePM3B7K6urlbbF5/kzz//dLgfB8YDAAAAAJ5XBvO/X88GAAkwmUzy9vZWVFQUK7gAAAAAAM9cUr6HskURQJIEjNgiJ2PiWyMjQ19NpWgAAAAAAGCLIv6DWLQIAAAAAMDzhQRXEixbtkx9+/ZNcj8/Pz/dvHnTqqx58+bau3ev3fZTp05VSEiITfkrr7yigIAAu1fBggUVGhqa5NgSExgYqAsXLiSpT/369XX8+PEUjcOe1atX2/0c/Pz85OPjo//9738J9i1evLguX75sVXb8+HE1aNDApu3UqVM1ZswYu+McPXpUDRs2tCnftGmTihcvbrlatGhhqbtw4YICAwNt+qxZs0Y5cuSQv7+/3at169Yp0gcAAAAAgOfRc7lFsUGDBho2bJhq1qyZpH5jxozRnDlz5OnpaVU+a9Ys1alTR7GxsYqNjbWqW716tUaOHGlVdv/+fa1atUoVK1aUJEVHRysuLs6qzYMHD2zK/l334MEDm/KdO3cmGPt3332nlStXJlhvz/nz59W1a1f9/PPPcnd3V69evdSvXz9LfUxMjNXztm7dWsePH9fdu3cte1/v3r0rJycndezYUUOHDrX7GR04cEBNmjRRrly5Eozliy++ULVq1RyO/c0339Sbb75pVXbnzh316tVLERERqlChQoJ9//nnH2XLls2qLDY2VjExMTZtE/pZ2Ku7ceOGFi9eLLPZrJ49e1q1nTZtmrJnz64aNWrYnefMmTPq3bu3hg8fnmDcKdEHAAAAAIDn0XOZ4IqJibGbRHiS33//XdOnT7dJnCTm8UTLgwcPVKBAAZsEij0dO3ZU5syZbcqvXbumHj16OByD9DBBYzAYktTnzTffVPPmzbVhwwZduXJFdevWVd68efX222/bbf/111/rwIEDGjZsmLZv3y5JGjJkiAoXLqxu3bolOM/58+fVuHFjLVu2LEnxJcU333yj4OBgNW3aVNu2bZObm5vddqdPn5bZbJaTk+OLF2fNmqXVq1fblN+/f1+FCxe23Lu4uChPnjyWLZA3btyQh4eH5e2FPj4+Cc5hNpuT/PNLTh8AAAAAAJ5Hz2WCKy1t3rxZfn5+KlSo0BPbLly4ULVr17Ypnzx5su7fv29TXq1aNd24cUPOzs42dc7Ozho4cKDDcR49elS3bt1ScHCwJClXrlz69NNPNWDAgAQTXE/jWSRiYmJitHLlSk2fPl1eXl4yGo26deuWjh07pipVqtjts3z5ct27d0979+51eIVf9+7dbVbpSdLhw4c1ZMgQy72np6dat26tPXv2qF+/fsqePbtMJpPy58+vL774Qp6enoqMjNSZM2fk7+8vHx8fHThwQNLDzyehlWIJSU6fpIiOjlZ0dLTl3mQyPbO5AAAAAAB4Gs99gismJkaNGjXS+++/rzZt2jz1eMuWLdP27dvVoEEDzZgxw6ouPj5eY8aMsZsMeVzx4sXVvn17m+2QknT16lVNmjTJpvz06dM6f/683N3dkx3/Iz///LNq1aplVVazZk2dOnVKsbGxcnV1feo5/m3Dhg0KCAiwW2cwGLRr1y5lz57dobH++ecfDRw4UP/73/9Ut25dLViwQOXKlVN8fLw2bNigCRMm6MSJE+rUqZNV0u/8+fOaPn263n33XQ0dOlQ//vijVeLt4MGD8vf3V5EiRbRp0yZJUuHChdWjRw+7K7hu376t+vXrW5U9ePBAHTp00M6dO1WwYEFJ0syZMxUcHKxPP/1UklSiRAmFhYVZ9atYsaJ69uypVatW2X3mfPnyaevWrU/dJylCQkI0atSoZPcHAAAAACC1PPcJrh49eujFF190KLllMBhs3rAXGxurP//807Kiqm3btpZExeNmzpypq1evqlGjRk+ca9q0aZo2bdqTH+BfSpQooQoVKsjFxf6PLXv27Nq9e7dDY924cUNZs2a1KjMYDMqaNauuXbumPHnySJLq1q0rV1dXrVixItFzrf7tzTfflLu7u2bPnq1XXnlFktS0aVMtXbrUof5P4uPjo44dO2rhwoVWn4WTk5OaNWumZs2aKSYmRlevXrXU3bp1S23btlXz5s21aNEidezYUR07dtTcuXMt2xmrVKmiXbt22TzLv7egFi5cWIcPH1aOHDnsxnblyhV5eXlZklvSw8ThmjVrEn2mBg0a6PTp0w5/BsntkxTBwcFWZ7KZTCb5+vo+s/kAAAAAAEiu5zrBNWvWLF27dk1z5851qH25cuXUo0cPDRs2TM7OzjIYDHJxcVGBAgX0+uuvJ7qqacOGDZo1a5aKFSumadOmqU+fPin0FP9fYm8GTKrs2bPr7NmzVmVxcXG6fv26cubMaSnbsWOH1TlTjli9erVeeOEFy32BAgW0efNm+fv7J9hn4sSJatasmUPju7u7q06dOom2cXNzU758+SRJN2/eVK1atVSlShXNnj1bkjR//nwNHjxYVatW1c8//+zQvI7IkyePPDw8NGvWLLVr1043btzQxx9/nOC5bhs3btTChQsdHr98+fIKDAxMcp/kHERvNBot54cBAAAAAJCePbcJrj179ujTTz9VZGSkw+c/9e3bV71795bBYLDbZ9GiRXb7rV+/Xv369dO2bduUPXt21atXTzly5NA777xjd45t27bZlJ88eVKlS5e2Ke/Tp4/ef/99h+JPiurVq2v8+PFWB5Vv3rxZVatWtXvGV2Li4uJ09uzZBA/Wr1q1qtVqqqexfPlyjR8/3uH2L7/8smbNmqXJkydbbSd0dnbW5MmTdenSJRkMBvn4+FhtoVy6dKnGjh1rM567u7uqV69uU16jRg3NmzdP0sM3WoaEhKhJkybKkiWLgoKC1L59e0lS/vz5Le2khz+HYsWKOfw8np6eypIlS5L7AAAAAADwPHtuE1xffPGFqlWrpjlz5iTp8PWkvF1PkkaPHq1Vq1Zp69atKlKkiKSHiaLXXntNzs7ONge2T5061e447u7uNucyPdK7d29t2bJFknTnzh1FRUVZVidJD8/mKlmypOW+f//+6ty5c6JxlyxZUuXLl1ffvn01duxY/fHHH+rZs6dV8uVxjRo10sWLFxUZGany5cvrzp07iouLk5eXl7Zv366PPvoo0TlTQps2bexuNy1cuLAOHDhg2Vr5uEfJLZPJpNDQUG3YsEGxsbEym83KmjWr3n33Xc2cOdPS/p133rGboHSEk5OTLl26pD179tjU3bhxQ4sWLdJLL70kSfL29pa3t7ckKSIiQgsWLNDBgwd15coVeXp6KiAgQK1atVLdunWtxklOHwAAAAAAnlfPbYJr2rRpqlu3rqpUqaL69es7fH7U5cuX1bNnT61cudKmrmrVqlbb9eLi4mQwGHTgwAFlzpzZUp4tWzbt3LlTsbGxCc5TpUoV/fjjjw4dGD99+nTLvzds2KClS5dqxYoVljJ3d3eFh4c/cZzHLVu2TH379lVgYKCyZcumKVOm2Bya/m8rVqywPLObm5uyZMnyTN6O+KyYzWY1bNhQDRo00N69e+Xl5SVJioyMVL9+/fTbb7/ZnIvWsmVLnThxwu549+/f18svv6xly5bZlH/11Vc6ePCgTZ/Y2FhlypTJpnzbtm3q2rWrhg0bph49eihPnjyKiorS//73P3300Udq2LChRo8e/dR9AAAAAAB4Hj23CS5vb2/5+PhowYIFateunQ4ePGg3sfC4u3fv6uTJk3br/P39rc6RcnFxSfBsIzc3N8vh5fb8+eefiouLs9xXrlz5ibGltEcHwTvKx8cnSePfvn1bVapUUXx8vEPtDQaDfvzxR6szwFLStWvXdOLECZuzzAoXLqwZM2aoWrVqNgmub775JsHxwsPDbVboPZI/f367ScfIyEg1bdrUpvzLL7/UiBEjLFsZpYfnpDVt2lRly5ZVjRo1bJJVyemTEsJGNbQkBwEAAAAASA+eywTXv5NLNWrUUMuWLRUcHOzQWwtTa0XS4/PY286W0WXJkiXBZGFayJEjh8qUKaPQ0FB9+OGHllV3Fy5c0ODBgxM8CD4xj79180nlCSX7GjRooOnTp6tkyZJ68cUXLVtlIyIi9PHHH9t9M2dy+gAAAAAA8Dx6LhNcW7dutbofOXKkw31z5MihmzdvWh04/ri1a9eqRIkSDo9pNBptDm4vW7asKlWqlOCbGd3c3HTkyBGbcmdnZ5uxElsp9jRcXV3l4pK0XxFXV9dE3zb5rLi4uDh0OP7mzZs1fvx4VatWTbGxsTIYDPLy8lK7du30wQcfJGnOhF5G4O3trbt376pMmTI29Xfu3FHZsmVt+rz77rvKli2bxo4dq1OnTsnJyUkPHjxQvnz51Lp1a7uxJacPAAAAAADPI4M5oaUmz5mxY8dq9erVduvKlSunxYsXp3JEz1aVKlUUExNjt27YsGHJWq2UGjZv3qwhQ4bYrXt03pnRaEzlqOyLj4/X+fPnVbBgwbQO5Ym6d++u/fv3261r1KiRQkNDnziGyWSSt7e3fPuslJPRI8F2kaGvJjtOAAAAAAAeefQ9NCoq6olH5fxnElwAng4JLgAAAABAakpKgssplWIC/nMevaUyqX369euXon0SymH7+fnp5s2bSZoLAAAAAID0iAQX/vMaNGigH3/8Mcn9pk+frnHjxiVYHxsbq9jYWKuyTp066bvvvrMpW7t2raXP41tLk9PnkVu3biW4hTI6OtrqTZ4AAAAAAGRUJLjwnxcTE5Nggigx9hJYT/LgwQM9ePDgiWVP2+eRP//8U7lz505SjAAAAAAAZDTP5VsUgfRs2LBhmjZtmuU+PDxcTZs2TfE+kvTtt9/q9OnTioqKkre3d5LijI6OVnR0tOXeZDIlqT8AAAAAAKmFFVzAv8TExKhOnTpavnx5ioy3bNky+fv7q1evXpayYcOGacOGDZarXr16dvv07t37qfpcu3ZNCxYsUFBQkAYOHJjk2ENCQuTt7W25fH19kzwGAAAAAACpgQQX8C89evTQiy++qDZt2jyxrcFgsDnA/ebNmzpy5IhOnz4tSWrbtq3Cw8M1Y8YMSx8XFxdlyZLFcj0qf+RRn+nTpye7z507d9SqVSv17dtXs2bN0l9//aUhQ4Y4tK3xkeDgYEVFRVmuv/76y+G+AAAAAACkJrYoAv9n1qxZunbtmubOnetQ+9KlS+vdd9/Vl19+qbi4OLm4uMjT01PFixdXx44d7fZ58cUXNXr0aI0cOdJS5u7urlKlSiU4T1L7nDlzRq+//ro6deqkDz/8UJK0du1adevWTa1bt9bq1asdej6j0Sij0ehQWwAAAAAA0hIJLkDSnj179OmnnyoyMtJqZVRiGjdurCtXrujBgwdydna2qV+0aJFNWbdu3dStW7cEx8yTJ4+KFSv2VH2yZcum+fPn66WXXrKUubu7a9GiRYqKirKUffzxx/Ly8kpwXAAAAAAAMgoSXICkL774QtWqVdOcOXOSfF6VveTWk5w/f14hISHauXOnzGazzGazChQooB49eqhv375P1Sd79ux66aWXFB0drcDAwAS3JRYsWFAdOnRIcuwAAAAAAKQ3JLgASdOmTVPdunVVpUoV1a9fXxUqVHCo36+//qohQ4Zo48aNNnX16tVTYGCgTfnt27dVq1YtDR48WJ988onc3d0lSceOHVOXLl109epVde7c+an7GI1GnThxwm7cZrNZBQsW1I0bN5Q9e3aHnhUAAAAAgPSKBBcgydvbWz4+PlqwYIHatWungwcPKlOmTE/sFx0drTt37titK1CggAoUKGBTHhYWJi8vL3Xp0sWqvEKFCho9erRmzJhhk6xKTp/EGAwGOTs72xyS74iwUQ3Z2ggAAAAASFd4iyL+89zc3OTm5iZJqlGjhlq2bKng4GCH+jp6Xte/BQQE6NatW1qyZIliY2Mt5eHh4Ro3bpzefPPNFOkDAAAAAMB/hcGcnCUcACRJf//9typWrKgcOXIk2Gbt2rUqUaKEVdmff/6pcePGad++fYqPj5ck5c2bVz179lSLFi3sjpOcPompXbu21q9f7/BqLJPJJG9vb0VFRbGCCwAAAADwzCXleygJLsCOsWPHavXq1XbrypUrp8WLF6dyRGnv0X9YfPuslJPRI8F2kaGvpmJUAAAAAIDnFQkuACmOBBcAAAAAIDUlJcHFGVx4Lg0ePFgBAQE2V4ECBVSkSBHdvXs3yWP6+fnp5s2bSeqTO3dumzKz2ax58+apdu3aKlq0qIoVKyZ/f3+1bdtW+/btS3JcyY0tOX0AAAAAAEiPSHAh3WrQoIF+/PHHZPWdMGGCwsLCrK4NGzaoQIECeuutt+ThYb0CacyYMTbJsNKlSysyMtLSJjo6WnFxcZb7+fPny9/f33KVKFFCuXPnVlRUlKWNvTcsdu/eXVu2bNH8+fN19uxZ/f777zp58qR69OihPn36aMmSJVbtp02bpnz58lnFljVrVu3ZsyfB2FauXGkVm5+fnzw9PfXXX38l2AcAAAAAgIyKBBfSrZiYGMXExDz1ONHR0Zo4caJq1Kihbt26aeLEiTZthg8fbpMQy549uy5fvpzguO+//77Cw8Mt19dffy2j0ShPT89E4/n+++8VEhKi4sWLW8qcnJxUrVo19evXT+vWrbNqHxERofHjx1vF1qxZM7Vt29aS8Lp48aJVn1atWlnFtmDBApUvX14FChRw5CMDAAAAACBDIcGF59a1a9cUGhqqMmXKaN++ffL09NS5c+esVmUlJD4+XmfOnJG/v79Dc5nNZvXr10+DBg2Sk1Pif1bt2rVTly5dtHfvXt27d0+SdOPGDa1du1bjxo1Tu3btnjifwWDQlClTLAmvfPnyJdg2MjJSvXv31vLlyzVkyBDLqq4LFy4kOkd0dLRMJpPVBQAAAABAekSCCxlCTEyM6tSpo+XLlz+x7f79+9WoUSPVrFlT9+/f1759+7Ru3TodOnRIefPmVYcOHRQYGKgtW7YkOMbevXvl5+f3xEPsHhk1apSuX7+u7t27q0uXLpaVVY8SWP82duxY1alTR0FBQapSpYqKFy+u+vXrq3379pozZ46aN2/u0JwXL160rNCKjY212+ann37Sm2++qZiYGK1atUqhoaGWPvnz5090/JCQEHl7e1suX19fh+ICAAAAACC1uaR1AIAjevTooRdffFFt2rR5YtsCBQooNDRUFSpUsCrPnDmzunXrpm7duunGjRsyGAwJjjFp0iR1795d9erV0/nz5yUpwRVPM2fO1IoVK2Q0GrV06VLNnTvXUpclSxa7fYoUKaJGjRpp/vz5lrJHh+A7wmw266uvvrKcUfb4YfFRUVEKCQnRjh07tGbNGnl5ealz586qXLmy5s2bp4oVKz5xjuDgYPXr189ybzKZSHIBAAAAANIlElxI92bNmqVr165ZJY4SU7BgQRUsWDDRNlmzZk2wbvXq1bp27ZpatWql1q1bW8oLFy5s1e727dsaNGiQfv75Z+3du1dms1mNGzfW4cOHFRoaanOQ/ddff61Tp05Jkn755RdFRkZq5MiRlvrLly9r2rRp8vHxkSTVrVtXNWrUUKFChTR48GCFhoZa2l65ckWbN29W5cqVbWKLiYlRtWrV9N5772nfvn1yc3OTJK1atUrbtm1LMOn2OKPRKKPR6FBbAAAAAADSEgkupGt79uzRp59+qsjIyERXXD3St29fbdu2zeHx+/Tpo/fff99yv3//fg0ePFg//PBDovPduXNHFStWVPPmzbV79265u7tLkvbt26cRI0bo2LFjqlatmlWfsmXLKnfu3JKk2rVr24z5eNmjpNXAgQM1cOBAh5/Jzc1Nx48fl4uL7Z93/fr1Lf/u06fPEw/EBwAAAAAgIyDBhXTtiy++ULVq1TRnzhyHkjxTp061KYuMjFSjRo0UHh6eaN/Fixdr9OjR+vbbb1WoUKFE22bOnFn79+9Xzpw5rcqNRqPVSqt/K126tEqXLi1J2rhxo1auXKlff/1Vt27dUtasWVWlShW9//77NlsrHzGZTPL09HQo0fcouRUYGJjgmyh9fX3Vq1evJ44FAAAAAEB6R4IL6dq0adNUt25dValSRfXr108w+fO0WrVqpRs3bujHH39M9I2E//YouVWnTh0tW7ZMefPmtWkzY8YMm7J+/frp2LFjGjZsmCpWrChPT09dunRJu3btUqtWrRQSEqKWLVva9Hv99dc1efJkBQYG2tR99913yp49u035kSNH7MZuNptVsGBB3bhxw24/AAAAAAAyEhJcSNe8vb3l4+OjBQsWqF27djp48KAyZcqU4vNMmjTpiau2EnL58mVFR0fbrXvvvfdsypYvX65Dhw5ZHdieP39+tW3bVg8ePNDKlSvtJrji4+MVHx9vd56yZcsmKWaDwSBnZ2eZzeYk9ZOksFENHX67JAAAAAAAqcEprQMAEuLm5mY5IL1GjRpq2bKlgoODn8lcyU1uSXJoy+C/NWzYUMHBwTp79qyl7MGDB9q/f79mzJhhdU7W08wDAAAAAMB/BSu4kG5t3brV6v7fbxxMCmdnZ7sHrieV0WiUs7OzTbmfn5/q1atnOWj+cR9//LFatWpluZ87d65mzpypoKAg/f3335bycuXKafTo0WrSpIndcfz9/fXWW28pc+bMdus//PBDde3a1eHnKVeuHG9JBAAAAAA8Fwzm5OxRAtLI2LFjtXr1art15cqV0+LFi1M5ov8Ok8kkb29v+fZZKSejh019ZOiraRAVAAAAAOB59eh7aFRU1BOPyiHBBcAhJLgAAAAAAKkpKQkuzuBChtG5c2ft27dPW7Zs0ejRo9M6HAAAAAAAkE5wBhcyjNjYWMXGxqphw4Zq2LDhE9vv27dPDx48UM2aNVMhOgAAAAAAkFZIcOG5tW3bNsXFxZHgSqbo6GhFR0db7k0mUxpGAwAAAABAwtiiiHTp9u3beu+99xQQEKAyZcqoZ8+eiomJkSQtX75c7733niQpJiZG7dq1U0BAgCpVqqQePXpIkl5++WXNmTNH8+fPV4UKFXTy5ElJ0ieffKKAgACVK1dOZcqU0ezZsy1zLly4UD169NAbb7yhUqVKqVSpUvroo4+s4oqOjtbgwYNVpEgRlS1bVuXKlVNcXJwkaceOHapYsaJKlSqlwMBAbd++3aFnTegZJCkqKkpdunRR4cKFVa5cOdWvX99St3XrVr3wwgvy8/NT8eLFNWzYMD148MBS7+/vrx07duill15SvXr1JEn3799X165dVbx4cfn5+alLly66d++e3bhCQkLk7e1tuXx9fR16HgAAAAAAUhsruJAuDRw4UGazWcePH5ezs7MmTZqkzz//XF26dFFMTIwl2bVs2TL5+PgoLCxMkvTonQn79u3TyJEjFRcXp7Fjx1rGLV++vH7++WcZjUZdvXpV5cuXV926dVWyZEkZDAbNmzdP33zzjV577TXdvXtXNWrU0IsvvqjmzZtLklq3bi1/f3+dOXNGLi7//8/n/Pnz6tmzpzZu3KiiRYvqt99+U4MGDXTkyBFlz5490WdN6BkePHigOnXqqGPHjvr8889lMBgsfX755Rd17NhR69evV6VKlXT37l298847Gj58uMaPHy/pYTJr3rx52rlzp9zd3S2fa+7cuXXmzBlJUq9evTRmzBhLn38LDg5Wv379LPcmk4kkFwAAAAAgXWIFF9KlZcuWady4cXJ2dpYk9e/fX/nz57dpFx8fb1lBJckqCWRPvXr1ZDQaJUk5cuRQtWrVdOzYMUt91apV9dprr0mSPDw89NZbb+nHH3+U9DBpFhkZqZCQEKvkliTNnj1bPXv2VNGiRSVJfn5+atSokTZs2PDEZ03oGVasWKH8+fOrZ8+eNs81efJkDRgwQJUqVbLEOnv2bM2ePVt37961tGvRooUluXX79m2tW7dOI0eOlMFgkMFg0EcffaSvvvrKblxGo1FeXl5WFwAAAAAA6VGyE1zz589XnTp1VKtWLUvZzZs3derUqRQJDP9d169fl6urq/Lly2cpc3JyUsWKFW3atm3bVnfu3FHVqlX1/fffP3HsgwcPqm3btqpQoYLKlCmjHTt2WCWEHl+hlCNHDl2/fl2StH//flWvXt1uEu3kyZOaMmWKKlSoYLl27Njh0LlVCT3D/v37Ezw/7Ndff1X16tWtynLnzq18+fIpIiLCUlaqVCnLv3///Xddu3ZNgYGBlhgbN26s+Pj4J8YIAAAAAEB6lqwtikOHDlV4eLjGjx+vrl27WsqdnJz0/vvva9++fSkWIP57nJycLNv0/s1eIsbd3V2LFy/W8ePH1b17d23atEkzZ860O+7JkyfVtGlTzZgxQ7Nnz5aXl5feeustqzb2klf/juXfK63+7d69ewoJCVGrVq0SfTZ7EnuGhOZ7tLLNXqz/rvPw8LCKsVChQlYr1gAAAAAAeB4kawXXmjVrtHLlSlWtWtXqy7SXl1eCB1YDjvLx8ZGbm5suXLhgKYuNjdVPP/2UYJ/y5ctr27ZtWrx4sa5cuSLJNgn0/fffq3Xr1goKCrJst3t0+LwjXnrpJe3cudPqIPdHSpQooUOHDjk8lj2PP8NLL72kbdu22W0bGBioPXv2WJVdunRJly9fVvHixe32KV68uCIjI3Xt2rWnihMAAAAAgPQmWQmuBw8e2F1BYjabdf/+/acOCujatauGDh2qBw8eyGw2a+jQoZaD5f/t5s2blhVWv//+u5ycnOTj4yNJyp49u/744w9L27x58+rEiROWVVGffvqpLl265HBM1atXV6FChdS/f3+bJFeHDh00f/587d2711IWGRnp0LgJPUNQUJD++ecfffLJJzZ9evfurU8++USHDx+WJN29e1ddu3ZV9+7dLWeMPS5Hjhxq0KCBevbsqejoaEkPV3Ul5TOQpLBRDRUZ+qrNBQAAAABAWklWgqtixYpatGiRVdmDBw80ePBgBQYGpkRc+I8bOnSo3N3d5efnp4oVK8rNzU0tW7aUq6ur3Nzc5ObmJkmaM2eOChcurLJly6pDhw76+uuv5erqKunhAesnTpxQpUqVNGPGDLVu3Vply5ZVhQoVFBAQoIiICH3wwQeWZNW/x33EaDRalX377beKjY1VqVKlVKFCBZUtW1ZxcXGqVKmSVq1apf79+6tMmTKqWLGihg0b5tCzJvQMLi4u2r17tw4fPqySJUuqYsWKqlOnjiSpTJkyWrZsmXr06CE/Pz+VL19elSpV0siRIxOMXZKWLl2qbNmyqXz58qpQoYJq1qyZpFVsAAAAAACkRwazvcOOnuD69evq0KGDLl26pDNnzqhatWo6evSo8uTJoy1btihHjhzPIlYAachkMsnb21tRUVG8UREAAAAA8Mwl5XtoshJcj0REROjkyZO6d++eSpQoweotwI7u3btr//79dusaNWqk0NDQVI4oeR79h8W3z0o5GT2s6tiiCAAAAABIaUlJcCXrLYodOnTQokWLVLx48QQPtAbw0KxZs9I6BLvq16+vyZMnq3z58mkdCgAAAAAATyVZCS7O7AGeTsuWLXXkyBFlzpzZbn3FihW1ZMkSy/0bb7yR4N/dvXv3VKtWLS1evNhStmbNGnXt2lXZs2e3lF25ckUzZsxQ27ZtJT18M2VsbGxKPA4AAAAAAGkqWQmuMWPGqFevXnr33Xfl7+8vD4//v13JYDDIYDCkWIDA8+i3337T1q1bVaJECYfar1mzJsG68PBwtWvXzqrs7Nmz6tGjh9Wh8yNGjNCAAQMUEhIiSTp37lzSAwcAAAAAIB1K9hbFmzdv6tNPP5UkS0LLbDYrS5YsMplMKRch8JxKqUSwo6uwDAaD+vfvrwEDBkiSateunSLzAwAAAACQ1pKV4Pr7779TOg4AiXjrrbd09OhRubu729Q5OTmpRYsWDo1z5MgRLV26VJJ06dKlRNtGR0crOjrack/iGgAAAACQXiUrwQXg6RgMBsXFxTnc/tSpU9q8eXOSXurw+AtS4+PjFRUVpfPnz0uSVfLKnpCQEI0aNcrh+QAAAAAASCvJSnB17do1wW1Rbm5umjNnzlMFBTzv6tatq+bNm1u2KZ49e1ZFixa11BcsWFBbt2613Cc1IVayZEn16dNH33zzjaXs/v37+uyzz9SwYUNJ0ubNmxMdIzg4WP369bPcm0wm+fr6OhwDAAAAAACpJVkJrho1alit/rh7966OHj2qPXv2sOIDcMC0adM0bdo0y727u7vCw8MTbN+kSRO1aNHCsirr9OnTKlmypKXeyclJP/30kzw9PSVJzZo1U7NmzRKNoVixYpb29hiNRhmNRkceBwAAAACANGUwP76P6SkcOHBAoaGh+vbbb1NqSOA/wd3dXffv338m7VesWKHZs2frypUrMpvNMhgMqlOnjoYMGaICBQo4PKfJZJK3t7d8+6yUk9HDqi4y9FWHxwEAAAAAwBGPvodGRUXJy8sr0bYpegZX1apVdeHChZQcEsBTmDVrlpYvX64FCxbIz89P0sMVl0uWLFHt2rV16NAhZc2aNY2jBAAAAADg6aRoguuPP/7Q3bt3U3JI4LkyYcIELVy40Ka8cOHC8vf3tyn/448/VKhQIYfbBwUFaeTIkZb77777TgMHDrQktyTJw8NDXbt21dq1a3Xo0CE1aNAgmU8DAAAAAED6kKwEV6dOnWwOmb9y5YoOHDhgda4QAGuDBw/W4MGDU22+1157TdOnT1e5cuVUpEgRSQ/fnvj1118rMjJSVapUSfKYYaMaPnFpKAAAAAAAqSlZCa569eopJibGqszHx0dz587lLWtAOtK9e3d5eXmpffv2un79uuLj4+Xk5KRatWppx44d8vHxSesQAQAAAAB4ail6yLwknT17VkWLFk3JIQGkA0k53A8AAAAAgKf1zA+Zb9WqlVauXGm3rn379tqzZ09yhgWQAQSM2MJbFAEAAAAA6YpTUhrfvn1bZ8+e1cmTJ3Xu3DmdPXvW6vrhhx94iyLwf9atW6fOnTs73H7fvn1q0aLFM4zo/6tfv76OHz+eKnMBAAAAAPCsJWkF1+LFizVp0iT9/fffqlOnjlWds7OzcuTIoXHjxqVogEB6NmfOHE2cOFF3795V2bJl9fnnn1u26MbGxlpexrB792716NFDJpNJHh4ecnF5+Kd3/fp15ciRQ7t27bJq/2+vv/66jh49qkyZMtmNITAwUMuXL7fcr1mzRl27dlX27NktZVeuXNGMGTPUtm1bm9gAAAAAAMjokpTg6t69u7p3766KFSvq6NGjzyomIEPYvHmzJk2apB9//FEFChTQ7Nmz1bRpU4WFhcnJyXpxZK1atRQWFqZGjRpp2LBhql69uu7fv6/ChQvrl19+SXSeiIgIbd++XSVKlHAorrNnz6pHjx4aOXKkpWzEiBEaMGCAQkJCJEnnzp1L2sMCAAAAAJCOJWmL4iOffPJJSscBZDizZ89WaGioChQoIEn64IMPlDt3bm3atCnF5zIYDE/dv3///goLC1NYWJgqV66cQpEBAAAAAJD2knXIfJ06dXTr1i399ttvunv3rlXdgwcP9Morr6RIcEB69vPPP2vevHlWZQ0bNtSBAwf06qspe+h6kyZN5ObmZreudevWGj58+BPHOHLkiJYuXSpJunTp0hPbR0dHKzo62nJvMpkcjBYAAAAAgNSVrATX2rVr1aVLFxUpUkTh4eHy9/dXRESEXFxc1KxZMxJc+E+4ceOGsmbNalWWM2dORUZGWu7Xrl2rAwcOqHr16po/f/4Tx9y1a5f8/f1VpkwZffPNN5by77//XsWLF3c4NrPZbHUfHx+vqKgonT9/XpKsElcJCQkJ0ahRoxyeEwAAAACAtJKsBNfo0aP1008/qWjRoipbtqwOHjyomJgYDR06VNmyZUvpGIF0KXv27Lp+/bpy585tKbtw4YLy5s1ruW/RooUWLVrk8Ji1a9fWhg0brMqKFy+uevXqyd3d3W6fggULauvWrZb7kiVLqk+fPlYJsvv37+uzzz5Tw4YNJT08P+xJgoOD1a9fP8u9yWSSr6+vw88CAAAAAEBqSVaCy2w2W94UZzAYdP/+fbm7u2vixImqUKGCgoODUzRIID2qXr26tmzZonfffddStn79ek2dOjVJ49y8eVOXL19OsP7bb79N0njNmjVTs2bNEm1TrFgxeXp6JtrGaDTKaDQmaW4AAAAAANJCshJcsbGxMpvNMhgMKlmypHbv3q2GDRvKYDDYbI0Cnld9+/bVW2+9pYCAAPn7+2vcuHFyd3dXjRo1bNquWrVK06ZN08mTJ9WpUycZDAbFxcXJ3d1dr732mqpVq6bGjRuneIwrVqzQ7NmzdeXKFcvfbJ06dTRq1CjL4fgAAAAAAGR0yUpwvfrqq9q6dasaNmyozp07q0uXLurTp49++eUXVahQIYVDBNKnypUra968efrwww91+fJl1axZU+vXr7fbtnnz5paz6ZydneXh4WGzOmrXrl0pGt+sWbO0fPlyLViwQH5+fpKku3fvasmSJapdu7YOHTpkc4YYAAAAAAAZUbISXBMnTrT8u2HDhlqwYIHWrVunEiVKqHfv3ikWHJDe1a9fX/Xr139iOzc3N+XIkSNJY0+YMEELFy50uH1QUJBGjhxpuf/uu+80cOBAS3JLkjw8PNS1a1etXbtWhw4dUoMGDZIUEwAAAAAA6VGyElyPq1evnurVq5cSQwH4P4MHD9bgwYOT3f+1117T9OnTVa5cORUpUkTSw7cnfv3114qMjFSVKlWSNW7YqIby8vJKdlwAAAAAAKS0ZCW4oqOjNXr0aH311VeSpLNnz0qSLl++rFOnTqlWrVopFyGQQbm6usrV1fWZtX+S7t27y8vLS+3bt9f169cVHx8vJycn1apVSzt27JCPj0+KzQUAAAAAQFoymJNxKny3bt1kNBo1ZMgQNWnSREePHpUk3b9/X7Vq1dJPP/2U4oECSFsmk0ne3t6KiopiBRcAAAAA4JlLyvfQZK3g2r17t06dOiVJMhgMlnJ3d3fFxsYmZ0gAGUTAiC1yMnpY7iNDX03DaAAAAAAAkJyS0ykuLs5ueUxMjO7du/dUAQGQAgMDdeHCBYfa7tu3Ty1atLApj4+P17x581SvXj2VLl1aJUuWVIkSJVS9enV99tlnio+PT+mwAQAAAABIE8lKcNWtW1ejRo2yKrtx44Y6duyounXrpkhgwPNsz549qlKlisqUKaNKlSpp69atVvUxMTGW1ZDDhw9XQECA5SpVqpTKlStnSSbHxsbaXTk5ePBgbd68WV988YVOnjyp06dP68yZM1q+fLl27typ/v37P/sHBQAAAAAgFSQrwTV9+nRdu3ZNhQoV0unTpxUQECBfX1/du3dPEydOTOkYgefK5cuX1bp1a82aNUsnTpzQkiVL9N5771le1vC4MWPGKCwszHKtW7dOJpNJ7u7uic4TERGhoKAgFSxY0Kq8YMGCatOmjX7//fcUeyYAAAAAANKSw2dwrVy5Uq1atZIkGY1GzZgxQ61atZKXl5fu3bun4sWLK3v27M8sUOB5sWzZMr311lt64YUXJEmlS5dW9+7dNXv2bE2aNOmJ/RcsWKBOnTpZnX9nz7hx49SpUydt3rxZgYGB8vT01K1bt3TkyBGdOHFCCxYsSJHnAQAAAAAgrTmc4AoNDbUkuB7p1auXjhw5kuJBAc+zn3/+Wa+//rpVWd26dTVgwIAn9j137pxWrlxpeXNpYkqXLq3//e9/OnnypMLCwmQymZQjRw7169dPZcqUeWL/6OhoRUdHW+5NJtMT+wAAAAAAkBYcTnCZzWaHygAk7saNG/Lx8bEqy5Ejh65cuZJov9jYWL333nsKDAy06b9r1y75+/urTJkyqlKlihYuXOhwPEFBQRo5cqRNeUhIiM1ZewAAAAAApEcOJ7jsbYd60hYpALZy5Miha9euWZVdvHhRefPmTbBPfHy83n33XVWoUEHHjh2zbFN8pHbt2tqwYYPlfvDgwZKkyMhINW3aVGFhYZa6woUL68CBA8qTJ0+icQYHB6tfv36We5PJJF9fX8ceEgAAAACAVORwguvWrVvauXOn1aqt27dv64cffrBq5+bmpurVq6dchMBzpkaNGtq+fbtat25tKduwYYPq1atnt/2dO3fUsWNHeXl5acqUKbp586bq1asnd3d3tW3b9pnFaTQaZTQan9n4AAAAAACkFIcTXIULF9bo0aOtyvLnz68xY8ZYlRmNRm3evDllogOeQ61bt9akSZO0bt06NW/eXNu2bdOyZcvsnmf3999/q3HjxmrRooU+/vhjGQwGZc2aVdu2bVPLli1VoUKFBOfp3bu3du/eLRcXF6t2Pj4+ql69upo3b65PPvnkGTwhAAAAAACpy+EE17Zt255lHMB/hqenp7Zu3aq+fftq6NChKlq0qLZs2aJcuXLZtPXy8tKMGTNUs2ZNq/Js2bJp586dkh6ev2XP9OnTE4xh165dmjBhQvIfAgAAAACAdMThBBeAlFOoUCGtWbPmie0yZ85sk9xy1KBBg/TNN9/I09PTps7JyUkdOnRI1rgAAAAAAKQ3JLiA59SpU6e0YMEC1a5dO0XHDRvVUF5eXik6JgAAAAAAT4MEF5AOubq6ysXFsT9PV1dXubq62pSXLl1aHTp0UJYsWRLsd/To0aeKEwAAAACA9MBg/vdrEQEgASaTSd7e3oqKimIFFwAAAADgmUvK91BWcAFIkoARW+Rk9LDcR4a+mobRAAAAAAAgOaV1AAAAAAAAAMDTIMEFOCh37twOt/X399fdu3e1YMECjR49Osl9xo0b98T2Fy5cULVq1STpiX1+//13Va1a1bHgAQAAAADIYEhw4bnSoEED/fjjj0nuN3/+fPn7+1uuEiVKKHfu3IqKirK0uXPn/7F353E1pv//wF+n7YRWRCE7hTZR0ZQtkWVs0RjGMhj7vmebZMk+IVvINhmDsRshHxLGUsYSMhqikV11RJ2Wc35/+Lm/jnNOnZNKeD0fj/vxcK71fZ+5+XzP+3td1/1aqd/gwYOFPjdu3BDKMzMzIZPJkJ2djaysLKV+3bt3x4kTJxTK3u+TnZ0tlE+cOBH29vaws7MTrsTERIWxP+yj6v7u3r2Lly9fav6lEBEREREREX0meAYXfVGysrJUJpTyM2jQIAwaNEj4fPnyZXTp0gXGxsZ59gsNDdV6LgDIyMiASCTSqO3Ro0exb98+1KpVS6E8MTFR4/67du3CrFmz0LNnT+zZs0ftmxWJiIiIiIiIPkdMcBF9QC6XY/z48Zg8eTJ0dNQvcuzXrx9iY2MBALm5uXj48CESExNRtmzZfOd4/vw5zM3NNY5HV1dXs+A/6BccHIylS5fizz//hIODA+RyOdzd3bFy5Uo0b948z/5SqRRSqVT4LJFItI6BiIiIiIiIqDhwiyJ9sbKystCqVSts375dq36zZ8/Gy5cvMXz4cAwePFjYFpiRkaHQbsuWLYiLi0NcXBw2bdoEZ2dnjZJbAHDv3j0YGhpqFZc29u7dCwcHB0RHR+PixYtwcHAAAIwaNQqhoaGYMmUK3NzclLZJvi8oKAimpqbCZW1tXWTxEhEREREREX0MruCiL9aIESPg5uaGXr16adxn5cqV2LFjB8RiMX799VeFLYjqtvVlZ2dj2rRpWLVqFfr374+YmBgkJyerneP+/ft49uwZoqKiYGlpiebNm0Mmk+XZR50bN27A1tYWqampGDp0qFCuq6uLLVu2wNnZWalPkyZNcP78eVy9ehUmJiZqx/b398f48eOFzxKJhEkuIiIiIiIiKpGY4KIv0urVq/HixQuNz8hKT0/H5MmTERsbizNnzkAul6Ndu3aIiYnBggULULp0aZX9ZDIZhgwZgjt37uDx48fYvHkzAKB69epq59q+fTu8vLywZs0aDBkyBNeuXcu3jzoNGjRATEwM1q5di8ePHwvlnTp1yrevo6NjnvVisRhisVjrmIiIiIiIiIiKG7co0hcnOjoaM2fOxJYtWzQ6yP3169do2LAhSpcujaioKJQvXx4WFhY4e/YsSpcujStXrqjsJ5FI4OfnB2dnZ5w4cQJDhgzB8ePH85wrIyMDK1aswNKlS1G/fn1s2LAh3/hEIhHkcrnw+c2bN7h69SoyMzNVth8zZozCGyHzu9avX59vDEREREREREQlGVdw0RcnLCwM7u7uWLt2LSZNmpRv+zJlyuDcuXOwsLBQKBeLxViwYIHKPklJSejQoQOmT5+O7777DgBw8OBBdOrUCf/73//UzjVkyBD06NEDjo6OWLp0Kdzd3eHu7o4GDRqo7ePi4gIPDw+UKVMGenp60NPTQ926dTFu3DiV7ZcvX65UlpiYCB8fH8THx6udh4iIiIiIiOhzxQQXfXGCg4Ph5eUFV1dXeHt7w8nJKd8+75JbrVq1Qnh4OKysrJTarFixQvhzlSpVcOzYMVhaWgpl9erVw82bN6Gvr69yjv379+PRo0fCqq1KlSph3bp1iIqKyjPBFRYWprI8MTEx3/siIiIiIiIi+howwUVfHFNTU5iZmWHjxo3o06cPLl68iFKlSmnU9+nTp5BKpSrrBgwYIPxZJBIpJLfeUZfcAt6ei9W+fXuFNm3bttUorpIkbnbbPA+nJyIiIiIiIipuPIOLvigGBgYwMDAAAHh6esLX1xf+/v4a99fkzC5NGBoaQkdH8a+XSCTKMwEmFouV+uRFX19fuFciIiIiIiKirxlXcNEX5dixYwqfAwICtOpvY2OD1q1bw9DQUGX9rFmz4Ofnl+8478660tfXzzOp9b7bt29r1ady5co4d+6cRn10dXWhp8e/7kRERERERPRlEsnffz0b0Rdo7ty52L17t8o6BwcHbN26tZgj+jxJJBKYmprCeuxO6IhLC+WJCzp8wqiIiIiIiIjoS/Xud2haWlq+R+UwwUVEGmGCi4iIiIiIiIqTNgkunsFFVMLY2NggNTVVoaxXr16oXbs2ateujSVLlgAA3rx5g3r16gEAdu7cicmTJ2s8R3h4OMaNG1doMRMRERERERF9SkxwERVAmzZtcPr06Y8aw9raWimRBQBSqRQ5OTkKZdu3b0dCQgISEhIwceJEAIBMJkNGRgYAICsrC1lZWUpj3blzB87Ozkrl2dnZyM7O/qj4iYiIiIiIiEoKnjpNVADqEkraePbsmdrD7N+5evUqevfurVRuZmaGiIgIjeYQi8UFjpGIiIiIiIjoc8AEF9En8Pr1a5QuXTrfBJejoyPi4uLw8uVLXLp0CaVLl4abmxsMDAyQnp6e7zxPnjxBtWrVChSjVCqFVCoVPkskkgKNQ0RERERERFTUuEWR6CNlZWWhVatW2L59u8Z9YmJiYGlpqVHbv/76C02bNsXhw4exefNmuLm54enTpwCA5ORk2NnZwd/fX2XfqKiofJNo6gQFBcHU1FS4rK2tCzQOERERERERUVFjgovoI40YMQJubm7o1auXxn0iIyMRHx+Pe/fu5ds2ODgYa9euxYoVK7Bx40b88MMP2LJlCwCgUqVKiIuLQ1BQkMq+EREROHHiBFJSUpTqwsPDYWtri9GjR6vs6+/vj7S0NOFKSkrS+P6IiIiIiIiIihMTXEQfYfXq1Xjx4gXmz5+vcZ+srCyEh4dj8uTJmD17dr7ta9SogTNnzgh9L126hJo1a+bbb8+ePahYsSImTJiAmTNnKtX37t0b8fHxWLFihcr+YrEYJiYmChcRERERERFRScQEF1EBRUdHY+bMmdiyZQtEIpHG/UJDQ+Ho6Ij58+cjISEBhw8fzrP9zJkz8fjxYzg7O+Obb75Bo0aN4OvrC7FYjH79+qnsI5VKMX36dMydOxejRo1CbGwsDhw4oNX9EREREREREX0ueMg8UQGFhYXB3d0da9euxaRJkzTqk5CQgMWLFyMqKgo6OjrYtm0b2rRpAysrKzg7O6vsU6ZMGaxatUqpXF9fX1gB1qZNG7i5uQl1w4YNQ9u2beHp6QkA2LlzJ5o3bw5jY2O0bNlS21slIiIiIiIiKtGY4CIqoODgYHh5ecHV1RXe3t5wcnLKs31KSgo6dOiA1atXo3r16gDebj/cunUr5s+fj927d+fZPyMjA0uWLMGRI0eQk5ODnJwcAG/P4erZsyd++OEHAG/P3UpJSUFoaKjQ19raGocOHUJiYmKB7/eduNltuV2RiIiIiIiIShQmuIgKyNTUFGZmZti4cSP69OmDixcvolSpUmrbm5mZYceOHWjYsKFCedOmTfNNbgGAn58fmjZtihMnTijM8+DBAwwbNgypqakYOXIkfHx84OPjo9S/fv36qF+/vhZ3SERERERERPR54BlcRAVgYGAAAwMDAICnpyd8fX3h7++fZx+RSKSU3NKGSCSCgYEBdHQU/9rq6elBX18furq6BR6biIiIiIiI6HPGFVxEBXDs2DGFzwEBAYU2tlgsVpms2rFjBxYuXIjmzZtDJpMhJycHIpEIFSpUwA8//IDevXtrPIe+vj709fULFJ/dz0ehIy6NxAUdCtSfiIiIiIiIqLCJ5HK5/FMHQfQlmDt3rtqthg4ODti6dWsxR1S4JBIJTE1NYT12JxNcREREREREVOTe/Q5NS0vL9yzoL3KLYsWKFZXK5HI51q9fjxYtWqBmzZqoVasWbG1t0bt3b5w9e7ZA89jY2CA1NfWj+kyYMAG//vqrQpsPy2xtbTWex9bWFm/evMHGjRsxb968fNs/fPgQ7u7uAKCyT0REBGxtbYVr5MiRed4P8PZMqIYNG8LJyQlOTk747bffhLrw8HCMHz9eo3v53MyYMQNXrlxReX1scqtr165aPacPHz5U+1ZGIiIiIiIioi/NJ0twtWnTBqdPn9a634YNGxQSLnXq1EHFihWRlpYmtHn9+rVSv+HDh+Po0aPYsGED7t69i3///Rc3b97EiBEjMHbsWGzbtk2hfXBwMCpVqgQ7OzvhMjc3R3R0tNBGKpUKb7IDgJ07dyrEZmNjA2NjYyQlJantk5ubq/BZVVlmZqZSGwDo3r07Tpw4oVCWmZkJmUyG7OxsZGdnC+UTJ06Evb29wv0kJiYiOzsbWVlZAKDUBwB8fHwQHx8vXCEhIQr1H95PSEgI9uzZg379+qF///7o378/njx5guDgYFy+fFlhPm2cO3cO7du3R8OGDWFvb4+uXbsq1K9fvx729vZwdHREu3bt8PDhQ63nUOfChQto27YtGjZsCDs7O/Tt2xfPnz8X6n/99VeULVtWSOg5OTnBzc0Nubm5Gs9x48YNNGvWDA0aNIC9vT3Cw8MV6lX9t/kw+di9e3eF9gX5nomIiIiIiIg+R5/sDK6srKwC/QAfNGgQBg0aJHy+fPkyunTpAmNj4zz7/fnnn4iMjETt2rWFMh0dHbi7u2P8+PH4448/0KdPH6EuISEB8+fPR//+/YWyfv36oXfv3sKyuOTkZIU5/Pz84OfnJ3w+c+YMpk6diipVqmh9n/Pnz8eGDRsAAI8fP1bZJiMjAyKRSKPxjh49in379qFWrVoK5YmJiSrbP378GPXq1YO6HayLFi3C4MGDlcrVJeMAqC3PT0REBCZOnIjt27fDwcEBABSSPUePHkVoaCjOnDkDU1NT7Ny5E926dcOFCxcKNN/77t27hx49euDgwYNwdHREbm4u5s6di759++LPP/8U7qt9+/ZKK/E0lZ2djY4dO2LlypXo2LEjHj16hGbNmqF69er45ptv1PZ7l3wkIiIiIiIi+tp91ofMy+VyjB8/HpMnT1Z6s9yH+vTpg8GDB2POnDlo1KgRSpUqhZSUFJw6dQrz5s3TaDufSCTCsmXLhJUy1atXV9s2MTERY8aMwd69ezF16lTs378fAFSuLJoxYwaWLFkifH78+DFGjhyJjh07AgA6deqkco7nz5/D3Nw837iBt9+VNm/Zs7S0xPPnzyGXy5GTk4O7d++iQoUKMDMzAwC1Y02cOBGhoaHYtGmTkISys7PDkiVLUL58edy8eVPjGABAJpNh2LBh2LNnj5DcAqBwQPq6desQGBgIU1NTAG8TjcHBwbhy5QqcnJy0mu9D586dQ6NGjeDo6Ajg7X2PGDECS5cu/ahx33fo0CE4ODgI/72trKwwe/ZsBAcHq0xw3b17Fx06dFCbfKxTpw5Wrlz50XFJpVJIpVLhs0Qi+egxiYiIiIiIiIpCiUhwZWVlwcfHB4MGDUKvXr007jd79my8fPkSw4cPx+DBg3Hu3DkAb1c2fWju3LmYM2cOevbsCXNzc2RkZMDMzAz//PMP/vzzT3h4eGg0Z3JysrBq5sMtY+9cuHABI0aMQFZWFnbt2oUFCxZg4cKFAFQnxebOnauwUmzs2LG4dOkSMjMzAajecgm8XV1kaGioUdwFoauri8jISAwdOhTVq1dHUlISWrVqhdWrV6N///6IjY1VWsUWGxuLdevWITo6GqVLlwYArFmzBhMnTsTmzZu1juHChQuwsLBAw4YN1bY5ceKE0hbT5s2b4/jx4x+d4HJxccGoUaNw9epVODo6Qi6XIyAgAM2bN/+ocd8XGxsLT09PhTIvLy9MmTJFZfuaNWvi1q1bAN4+6zdu3IC5ubnC6jx1K/O0ERQUhNmzZ3/0OERERERERERFrUQkuEaMGAE3NzetklsrV67Ejh07IBaL8euvvyI0NFSoMzIyUtmnRo0a8PHxEbb+AW9XF2m6hVAul+O3334Tzg778HD1tLQ0BAUF4cSJE9izZw9MTEzw008/wcXFBevXr1eZpLG2tsb06dMVVnA9ffoU27ZtQ9u2bQEAO3bsUOp3//59PHv2DFFRUbC0tETz5s0hk8mUEk6auHHjhnCQ/dChQ4Xy3NxcDBgwAJGRkahbty5kMhl69eqFzZs3Y8uWLQCUE3bVqlVDZmYmduzYAXt7e6SmpuLw4cNo166d0CY8PByRkZFo06YNVqxYkWdsV69eRYMGDRASEoLNmzcjJycHLVq0QEBAAMzMzJCeng49PT2UKVNGoZ+1tTWuX7+u9Xfxobp162Lp0qVo2bIlfvzxR1y8eBE5OTk4ePDgR4/9TkpKitL3WK5cOYVzvlQ5fPgwAgMD0aZNGzx9+hT37t3Djh07ULZsWQDAnTt3YGtrC3Nzc/z1119ax+Xv76/wQgCJRAJra2utxyEiIiIiIiIqap88wbV69Wq8ePFCIUGVl/T0dEyePBmxsbE4c+YM5HI52rVrh5iYGCxYsEBYNfTO77//Lqx2uXbtGhITExEQECDUP336FMHBwcLWOy8vL3h6eqJatWqYMmUKFixYILR99uwZIiIi4OLiAkAxuZOVlQV3d3cMGDAAZ8+ehYGBAQBg165dOH78uNqk24QJEzBhwoQ877lJkyZKK7W2b98OLy8vrFmzBkOGDMG1a9eUYtJUgwYNEBMTg7Vr1yqc9/Xy5UuUKVMGdevWBfD2zLKWLVvi1q1bePz4MdLT05XO1Spfvjyio6Oxa9cu+Pn5oV27dpgxYwaaNGkitOndu7fSYfXqvHjxAkeOHIGtra2wQs/f3x++vr44ceIEUlNTVa5iMzQ0xJs3b7T+LlTx8fHB3r17sWzZMhgYGGDNmjUoV66cUC8SiXD69Gl4eHjgxYsXqF27NqZNm4amTZtqNH758uXx4sULhbJHjx7B0tJSbR+pVIphw4bh8uXLKF++PABg8+bNmDZtGtauXQvg7VbFuLg4bW9XIBaLIRaLC9yfiIiIiIiIqLh80gRXdHQ0QkJCkJiYqNFh6a9fv0bDhg3RuXNnREVFCYmNs2fP4ueff8aVK1fg7u6u0Mfe3h4VK1YEALRo0UJpzA/L3iWIJk2ahEmTJml8LwYGBrh69Sr09JS/Um9vb+HPY8eOzfdA/A99uIIrIyMDK1asQEREBBYsWIANGzbgp59+ynMMkUikcGbTmzdvcOfOnTwTGBYWFihbtixWrFiBAQMG4N69e1i1ahVCQ0MRFBSEGzduKCRmTp48iaioKIU5X7x4gXXr1mHOnDlIT09X2IqpCR0dHdSoUUNhu96CBQtQqVIl3L17F8bGxsJWzvdlZGSgVKlSWs2lyn///YfGjRujR48eePDgAa5fv46JEyfif//7n3CofPfu3dG1a1eYmJhALpfjzz//RKdOnXDu3DnUqVMn3zk8PT2xcOFChXs8dOgQWrdurbZPcnIyypUrJyS3gLfbKd9fnUhERERERET0tfikCa6wsDC4u7tj7dq1GiWTypQpg3PnzsHCwkKhXCwWK6y0el/9+vVRv359AG+3dO3cuRPXr1/Hq1evYG5uDldXVwwaNEjtWU0SiQTGxsYaJeDeJbecnZ3VviHS2toao0ePVir39vbGo0ePVPZJS0tDQkKCkIwaMmQIevToAUdHRyxduhTu7u5wd3dHgwYN1Mbm4uICDw8PlClTBnp6etDT00PdunUxbty4PO9p3759+Pnnn+Hp6QkLCwssXrwYTZo0EVZkvb9irGbNmsL3JBKJ4OPjg1KlSsHIyAgWFhYwMTHBxYsXlbYT5qVChQrCCrJ39PX1UbVqVTx79gw1atRARkYG0tPTFVbJJSUlFejtlR9au3YtfHx8hEPbra2t0aRJE9SpUwf//PMP6tatq3A/IpEIHTp0QOfOnXHkyBGNElxeXl6YMWMG1qxZg6FDh+Lq1auYN28ejh07prbPu62g+/fvR+fOnSGVSrFo0SL4+Ph89D0TERERERERfW4+aYIrODgYXl5ecHV1hbe3t0YHgr9LbrVq1Qrh4eGwsrJSaqPqXKfx48fjypUrmDFjBho2bAhjY2M8efIEp06dgp+fH4KCguDr66vUr0uXLliyZAmcnZ2V6g4ePKiwVe2dy5cvq4xdLpejatWqSElJUep3/Phx1TeMt0mV9PR0iMVi7N+/H48ePRJW6lSqVAnr1q1DVFRUngmusLAwleX5HUZuYWGBefPmwczMTGWSr3Tp0kJir1q1aqhWrRoA4LfffsP27dvx4MEDZGdnQ19fH5aWlujatWu+q83e5+LiglWrVimUZWdn48GDB6hVqxZEIhHc3Nxw+vRptG/fXmgTFRWl0Zsx8yORSIQE6Ttly5ZFpUqVkJKSorZfbm6uytV8qohEIhw5cgSjR4/G6tWrYWlpid9//11p3vfp6Ojg0KFDGDduHKZPnw7g7bM6depUjeYkIiIiIiIi+pJ80gSXqakpzMzMsHHjRvTp0wcXL17UeFvZ06dPIZVKVdYNGDBAqWz79u24dOmSwiHZlStXRu/evZGbm4udO3eqTHDJZDLIZDKV89jb22sU6zsikQi6uroKWwXfadeuHf7991/h7K731a9fXzgjrFOnTmjfvj309fWF+neH0RcVX19ftUm+mzdvKpWFhIRg3759WL9+PWrUqCGUJycnIyAgACNGjBDOicqPg4MDypQpg5UrV2LUqFGQyWSYOnUq2rVrJ2zPGz16NGbNmgUPDw+YmJhg586deP36tcotqdrq27cvvv/+e7Rp00ZIwG7atAk6Ojpo1KgRAODhw4eoWLGikND6448/EBERgfnz52s8j7m5udKbIPNTq1YtHDhwQKs+hSFudluYmJgU+7xERERERERE6nyyBJeBgYGQzPH09ISvry/8/f0RHBysUX9Ntgy+r23btvD390dgYCBq1qwJ4O0qmwsXLmDFihUKbw/8mHkK6tatWzh//nyeB4u/i+f95NaHxGIxdHR0NJ5XX19fZVLtfXK5XG2STxWZTCZsTXxfmTJlYGRkhPT0dI3HAoCdO3diyJAhCA4OhkgkQqtWrbBmzRqhvmvXrkhKSkLTpk2ho6MDS0tL7N+/X6vvQZ3GjRtjw4YNGDduHFJSUiCXy9GgQQMcOnRISGhFRERg8eLFwhZSGxsb/O9//1O5upCIiIiIiIiICt8nS3B9eL7Q+2821ISNjQ1at26t8g16ADBr1iz4+fkJn0NDQ7Fy5Ur07NlT4awrBwcHBAYGKmxve5+trS169Oih9tyoUaNGYciQIRrH7eDgoPJg93r16qFZs2Zqk02LFi1SG+P7bt++DeBt4iqvRNg7lStXFt5OqK6Pra0tunXrpvSGynfGjBmDYcOGCZ9Hjx4NY2NjfP/993j58iVycnKgp6cHIyMjdOvWDSNHjsw3rvdZWVnlu1Jp9OjRKs82KwzNmzfHyZMn1dYPHDgQAwcOLJK539H0v+c7enp6+SYuiYiIiIiIiL4UIrmq/XKfyNy5c7F7926VdQ4ODti6dWsxR0RFzdXVVe2B/DNmzED37t0/avywsDCVZ7IBQPny5WFkZKT2HLIBAwZ8dNIsIiJC7blYIpEI58+fz/NNloVl+PDhQiLzQz4+Pmpf0vA+iUQCU1NTWI/dCR1xaSQu6FDYYRIREREREREJ3v0OTUtLy/eonBKV4CKikosJLiIiIiIiIipO2iS4Pv6QIiL65CpUqKBV+/DwcIwbN66IoiEiIiIiIiIqXkxwUbFo06YNTp8+rXW/5ORk9OvXD05OTnB0dIS3tzcuX74s1P/3338Qi8VwcnJSuK5cuVKI0b/d6igWi5W2M8bFxcHT0xP29vaws7ND06ZN8eeffyq0GTRoEKpWraoQn7qXGqizZ88eODo6okGDBnB3d1f4DgDgzZs3Sn0CAwNha2srXIsWLRLqsrOzkZ2drVUMRERERERERCXVJztknr4uWVlZas/aUkcmk8Hb2xv+/v7YsmULAODo0aPo2LEjEhISULp0aeTk5MDCwqLQE1rvmzlzJmJiYmBubo6cnByFuqpVq2LHjh2oXLkyAOD8+fPo0KEDjh8/DmdnZwBATk4OZs2ahUGDBhVo/ri4OIwZMwanT59GjRo1EBUVhc6dO+PGjRt5LtGcNWsWZs2aVaA5iYiIiIiIiD4nXMFFJdajR4+QnJyMH374QShr27YtypYti/j4+GKJQSaTwcrKCocOHVL5xk4TExMhuQUATZo0wffff4+IiIhCi2H9+vUYP348atSoAeDtWx3btWuHbdu2qWy/Y8cOhZVbH15z5szRaF6pVAqJRKJwEREREREREZVETHBRscvKykKrVq2wffv2PNtZWVnBzMwMYWFhQtmOHTvw8uVL2NjYFHWYAAAdHR0MHz4curq6GvdJSUlRSHp9rNjYWHh6eiqUeXl54fz58yrb9+zZE/Hx8YiPj8fZs2exefNmREVFCWUzZ87UaN6goCCYmpoKl7W19UffCxEREREREVFR4BZFKnYjRoyAm5sbevXqlWc7HR0d7N69G506dcKxY8dgaGiI48eP48CBAyhTpkwxRau5Fy9eYOvWrbh79y6+//77Qhs3JSUFZmZmCmXly5fHs2fP8uw3d+5cnDx5Es2aNcPff/+NypUrIyQkBCKRCMDbg+YjIyPRtm1bLF++XKm/v78/xo8fL3yWSCRMchEREREREVGJxAQXFavVq1fjxYsXCA0N1ai9nZ0d+vfvj/nz5wMAxo4dC1tbW6FeJBLh5cuXaNWqFZ48eQJzc3MMHjwYffv2LZL4VTlz5gx+/PFHJCYmombNmjh8+DAMDAwUYly5ciXWrVuHjIwMuLm5ISAgQONkUfny5fHixQvUrl1bKEtOToaVlZXaPn/99ReOHTuGqKgoIaHVv39//P777+jZsycAoHfv3ggJCVE7hlgshlgs1ihGIiIiIiIiok+JCS4qNtHR0QgJCUFiYqKQdMlLbm4uWrdujVKlSiE2Nhb6+vqYOnUqXF1dceHCBRgbG6Nq1aq4d+8eKlasCAC4ceMGevToAT09vXxXiBUWDw8P3LlzB7m5uTh48CDatm2Lv/76CxUqVAAALFmyBMbGxjAwMEBmZiaWLVuGtm3b4tq1a9DTy/+voKenJyIjI+Hm5iaUHTp0CJ06dVLbJz4+Ho0aNVL4nl1cXBAXF/cRd0pERERERERUMvEMLio2YWFhcHd3x9q1azVqf/LkSTx+/BiHDx+Gs7Mz7O3tcejQIVhaWuK3334D8HZ11LvkFgA0aNAA06ZNw+7du4vkHvKiq6uLLl26wMfHBzt27BDKy5UrJ6zoMjQ0xLRp05CVlYWbN29qNO7gwYOxceNGnD9/HnK5HFu3bsWVK1fQo0cPtX08PT2xf/9+3LlzBwDw5MkTbNiwAW3atPmIOyQiIiIiIiIqmbiCi4pNcHAwvLy84OrqCm9vbzg5OeXZXiKRoE6dOtDX1xfKRCIRGjRogJSUFLX9cnNzNVoZVVTS0tIgk8nybKNNjFWrVsXevXsxZcoU/Pfff3BwcMCJEycUtkF+qHbt2li/fj369esHiUQCAwMDTJo0Cc2aNdPqXoiIiIiIiIg+B0xwUbExNTWFmZkZNm7ciD59+uDixYsoVaqU2vZeXl6YNm0afvvtN+HQ9piYGBw6dAjHjx8H8PYAdl1dXZiYmAAArl+/jsDAwDzPlipMiYmJqFq1KnR0dCCXy/Hrr7/ixIkTWLJkidDm/v37qFatGgBAKpVizpw5sLS0RP369TWex9HREREREVrF5uXlBS8vL636aCJudlvh+yYiIiIiIiIqCZjgomJhYGAgrDjy9PSEr68v/P39ERwcrLaPqakpIiIiMHXqVCxYsAA6OjooW7Ystm3bJhy4fu/ePfTr1w/A2y2CZmZmCAkJQbt27YrkHt5fTQYAixcvxrFjx1CqVCno6OjA3t4e586dg6WlpdAmKCgIp06dgqGhIWQyGby9vXH48OFCj4+IiIiIiIjoayWSy+XyTx0EEX0cCwsLPHv2TOP24eHhiImJwS+//KJxH4lEAlNTU6SlpXEFFxERERERERU5bX6HMsFFn8zcuXPVHgbv4OCArVu3ftT4z58/h7e3N9Q94n5+fti5c6fKuvLlyyMyMvKj5tdEUX8HhendPyzWY3dCR1waiQs6fOqQiIiIiIiI6AvGBBd91eRyOUQi0acO44vDBBcREREREREVJ20SXDrFFBNRobKxsUFqaqrKukqVKuH169dK5ba2tmr7qBIeHo5x48YVMELVzp49i65duyqU3bp1C7a2tmqvypUrIzk5Wat5vL29cfXq1cIMnYiIiIiIiKjEYoKLikybNm1w+vRprfvt27dPKcljZmaGQ4cOCW2kUilycnKU+mZlZSEjIwNlypRRqsvMzFTZ586dO3B2dlYqz87ORnZ2ttbxA8Dx48fRrVs3jcasV68e4uPj1V716tVTm+CaPn06VqxYUaixExEREREREX1u+BZFKjJZWVnIysrSul+XLl3QpUsXhTIfHx/o6eX/uMbGxqJ06dJabVN89uwZxGKx1nHm5fnz58JbIz9WamoqTE1NVdY9e/YMVatWLZR5iIiIiIiIiD5XXMFFJd7jx49x/fp1tGzZMt+2GzduxPPnz/Hbb78BAFq2bCmsAnv48KHKPk+ePEG1atUKNeaXL1/CysqqQH1nz56t8DkpKQlVqlRR2bYoYn9HKpVCIpEoXEREREREREQlERNcVCyysrLQqlUrbN++Xeu+o0ePxtixY9GhQ4c8k1WHDx9GZGQkTp8+jalTp+Ly5cs4efKksNWvcuXKKsePioqCoaGh1nHlJSYmpsAJrpUrVwp/fvPmDTw9PVGqVCmldjKZDGfOnCn02N8JCgqCqampcFlbWxfJPEREREREREQfiwkuKhYjRoyAm5sbevXqpXGf3NxcjBw5EikpKRg/fjwiIyPVJqvCw8MxevRoHDp0CE2aNMGuXbvg5+eHpUuX5jtPREQETpw4gZSUFKW68PBw2NraYvTo0RrHDQAnTpzAn3/+qVUfVUqXLo3du3errLtw4QJycnKwceNGlfXdu3eHra0tTp48WaC5/f39kZaWJlxJSUkFGoeIiIiIiIioqPEMLipyq1evxosXLxAaGqpxnzt37mD48OEoVaoU9u/fD11dXbVtU1NTsW/fPkRFRQlb+dzc3HD+/Hns3btXaNe4cWOl1U579uxBxYoV0bVrV8ycORMhISEK9b1791Yqy8/x48eFOE6dOoUWLVqobJeeng5XV1fIZDKF8vLly8PW1lapvUgkQnR0NMqXLw/g7QqrZcuWYc2aNbh48SJcXV0V2u/evRuNGzfWKvb3icXiQj+bjIiIiIiIiKgoMMFFRSo6OhohISFITEzU+ND3efPmYc2aNQgICMDAgQPz7WdmZoZdu3YplZcvXx4//fST8PnDlVBSqRTTp09HaGgo3N3d4eHhgQMHDqBTp04axamKXC7HtGnTMGPGDNSoUQPff/89/vrrL5iYmCi1NTIyws2bNws0T1RUFO7cuYO+ffuiYcOG6N27N/766y+YmZkVOHYiIiIiIiKizxUTXFSkwsLC4O7ujrVr12LSpEka9fHz88OECRMKdLaUt7c3Hj16pLIuLS0NCQkJwqqkYcOGoW3btvD09AQA7Ny5E82bN4exsbFGB9qrsmTJElSqVAmdO3cGAAwZMgR+fn7Yt29fnvfz8uVLLFmyBIcPH0ZqaipkMhlMTEzQoUMHTJkyBeXKlRPaPnz4EAMGDMD27duhr68PZ2dnjBs3Dj4+Pjh27JjKZBoRERERERHRl4wJLipSwcHB8PLygqurK7y9veHk5JRvnzp16gAA2rRpg8WLF8PR0VGpzcaNG2Fubq5Ufvz4cbXjWltbIz09HWKxGBEREUhJSVHYNmltbY1Dhw4hMTEx/xtTYe/evdi2bRuioqKEstGjR+O///7DH3/8gd69e6vsl5KSAjc3NwwcOBDHjx9HhQoVAACPHj1CWFgYXFxcEBMTg7JlywIApkyZgvnz58PNzU0YY/DgwdDT00N2dnaBYiciIiIiIiL6nDHBRUXK1NQUZmZm2LhxI/r06YOLFy+qfCOgKllZWWoTNl5eXirL27Vrh7t370JfX1+prn79+sIWPh8fH/j4+KhsU79+fY3i+5CLiwsiIyOVEm+LFi3Ks99ff/2FqlWrYurUqQrlVlZWmD59Ok6cOIELFy6gXbt2AIBff/1V5TgDBgwoUNzaipvdlqvEiIiIiIiIqERhgouKjIGBAQwMDAAAnp6e8PX1hb+/P4KDgzXqr+mZXe+7desWYmJihIPYi9O7g+W15erqirt372LFihX44YcfhJVaL168wJYtW3D37l24uLgUZqhEREREREREXxQmuKjIHDt2TOFzQECAVv1tbW3Ro0cPlClTRmX9qFGjMGTIEIWyevXqwd3dXUisfWjRokVo3769RvPr6+urXAn2MVSNWb58eVy4cAELFy6Ep6cn0tPTAbw9hN7HxwcXL17UOmFXFLETERERERERlVQiuVwu/9RB0Ndj7ty5Sm8zfMfBwQFbt24t5og05+rqiqysLJV1M2bMQPfu3Ys5Is1EREQobX98RyQS4fz588LB+3mRSCQwNTWF9did0BGXRuKCDoUdKhEREREREZHg3e/QtLS0fI/KYYKLiDTCBBcREREREREVJ20SXDrFFBPRF6lz586Ijo5WWffLL78gKCio0OY6cOAAOnbsiFq1aqFatWqoU6cOBgwYgGvXrqlsHxERAVtbW+EaOXKkQr2NjQ1SU1MLLT4iIiIiIiKiT4VncNFXr02bNpgxYwaaNWumdd/c3Fzk5uZqVde4cWNIJBK154R16NABCxcuVCgLCAjAyZMn8csvv6Bhw4YQiUSQSqX4888/4evri1WrVqFNmzYKfXx8fBAfH682dqlUipycnPxukYiIiIiIiKjEY4KLvnpZWVlqz9bSxI8//qjyIPwXL15gxIgRSuXx8fFISUnR+BD4rKwsBAUF4dmzZwpLMsViMbp27QoAWLBggZDgevbsGZo1awZ1u48DAgLQs2dPjeYmIiIiIiIi+hwwwUX0kTZt2oQWLVoolS9ZsgSZmZkq+4hEIo3H19HRgb6+PqRSqcr6zMxMGBoaCp8tLCxw69YtAMCTJ09w69YtlCtXDnZ2dlrNK5VKFeaUSCQa9yUiIiIiIiIqTkxwEb0nKysLPj4+GDRoEHr16pVv+1q1aqFfv34wNjZWqnv+/DkWL1780THp6elh2bJl8Pb2xpw5c+Dh4QFjY2M8evQI+/fvR3BwMHbs2KHUb/r06Th48CBcXFzw+PFjPH36FAcPHsSUKVMQGxuL5OTkPOcNCgrC7NmzPzp+IiIiIiIioqLGtyjSV69FixaYMWMGWrdujZ9++gnly5cv0OHwiYmJ6NixI+Li4vJsZ2xsjGfPnimsutLEpUuXsGHDBsTExCA1NRWWlpZo1aoVBg8eDGtra4W2CQkJ8PX1RWxsLPT03uaxV6xYgfv372Pp0qUAgOrVqyMmJgbly5dXOZ+qFVzW1tZ8iyIREREREREVC23eosgVXET/3+rVq/HixQuEhoYW6Tw+Pj5wdHSESCRCTk4OkpOTUbVqVaG+adOm2LRpk1I/FxcXuLi4aDSHubk5MjIy8OzZM1hZWUEmk+HOnTuoUaMGxo0bh+vXr+PJkyd5jiEWiyEWi7W7OSIiIiIiIqJPgAkuIgDR0dEICQlBYmKiRudUjR49GseOHVNZZ2trq1Q2YcIE/PTTTwCAXbt2CeWJiYl5vu1w3LhxOH78uCa3AAAYO3YsBg0ahHLlymHt2rXw8/PDmzdvIJPJ0K5dO4wZMwa6uroA3q7gIiIiIiIiIvoSMMFFBCAsLAzu7u5Yu3YtJk2alG/7FStWFENUwC+//CL8+dChQ/j1118VztsyNDRUe5B9q1at0KpVK7VjT548WeXZYURERERERESfGya4iAAEBwfDy8sLrq6u8Pb2hpOTU759Xr16BTc3N8hkMpX1qampmD17NoYMGVLI0WouPT0d8+fPx/HjxyGXyyGTySCXy+Hi4oKxY8dyCyIRERERERF9EZjgIgJgamoKMzMzbNy4EX369MHFixdRqlSpPPsYGxvj5s2bauvXrl2rsPXwY7Y1FlSPHj3g4eGBs2fPwsDAAAAgk8kQGRmJNm3aIC4uDmZmZlqNGTe7bb6H+xEREREREREVJya46KtnYGAgJH88PT3h6+sLf39/BAcHf/TY77+ktLi2NX7IwMBA4VwxkUgEAwMD6OnpaXTeGBEREREREVFJxwQXffU+XFUVEBBQKOOKRKJCTSDp6uoKB8S/k98Ww127dmH+/Plwd3cXtigCQKNGjXDkyBGYmpoWWnxEREREREREn4pI/v4SEyICAMydOxe7d+9WWefg4ICtW7fmO8arV6+QlZWFcuXKFXZ4n4REIoGpqSmsx+6Ejrg0Ehd0+NQhERERERER0Rfs3e/QtLS0fI/KYYKLiDTCBBcREREREREVJ20SXDrFFBN94WxtbfHmzRts3LgR8+bNy7f9w4cP4e7uDgAa93ln+fLlKtvv2LEDNjY2qFSpEjw8PBATE6NQb2Njg9TUVI3nSUlJQe3atdXWOzo64tGjRxqPBwDe3t64evWqUvmQIUNga2srXCdOnBDqzp07h65du2o0/rx587Bu3ToAgLu7Ox4+fKhVfERERERERESfIya4vkJt2rTB6dOnC9y/e/fuCgkYAMjMzIRMJkN2djays7OF8okTJ8Le3h52dnbClZiYiOzsbGRlZQGAUp/hw4ejXr16Cn3s7e2FZNKH7QHg0KFDWLhwIY4fP47k5GQEBwfju+++w7///iu0kUqlyMnJ0fg+c3Nz82yfm5uL3NxchbL4+Hh07NgRVatWRfXq1TFo0CA8e/ZMqFcVOwCsW7cO8fHxwuXl5SXUZWVlKfSJioqCvb29QkLsXcLv/fE/7EdERERERET0peIh81+hrKwsIblUEBkZGRofnn706FHs27cPtWrVUihPTExU2yc6OhpHjhxB9erVNY5p8eLFWLhwIapWrQoAaNy4MYYPH45ly5Zh1apVGo/zoeTkZNjZ2amsS0hIUPj8+vVrdOjQAQsWLECPHj2Qk5ODJUuWoFu3boiOjhba9ezZE4aGhggJCcGdO3ewdOlSlePr6enh4sWLSuUxMTHo2rUrAgMDC3xfRERERERERF8SJrhIa8+fP4e5ublGbeVyudKb/zTpo61Hjx4pbSe0sbH5qJVqAFCpUiXExcWprPsw8XXhwgXY2NigR48eAN4mqKZOnYrNmzfj1q1bqFevHoC3WykbN24MAGjRogV++uknAMCNGzfw9OlT1KtXD5aWlmpjksvl0NEp+sWXUqkUUqlU+CyRSIp8TiIiIiIiIqKCYILrK5eVlQUfHx8MGjQIvXr10qjPvXv3YGhoWKRx3bx5E6mpqZDJZMjKysKbN2/QokULtYmdOnXq4O+//0bNmjWFsosXLwpJpYIwNjZGdna22hVcz549g6mpqfBZX19faWWcXC5HRkYG9PX1hbKnT5/iv//+Q4UKFWBgYICnT5+iU6dOMDc3R+XKlXHp0iW0adMGo0ePRrt27fD69Ws0aNBAYdx//vkHhw4dQm5uLjIzM1G7dm00atSowPeqSlBQEGbPnl2oYxIREREREREVBSa4vnIjRoyAm5ubxsmt+/fv49mzZ4iKioKlpSWaN28OmUyG5ORkree+ceMGbG1tkZqaiqFDhyrUbd26FaVLl4ZIJIKBgQFMTU3h4eEBAwMDAEBISAh27NiB7777DrNnz8bs2bPx3XffwcLCAg0bNsSBAwfw66+/4tKlS1rH9Y5YLNbqkHZXV1c8f/4cISEh6N+/PzIyMhAQEAAnJyeF1WWjR4+GoaEhVq5ciZYtW2LFihXw9fXFpEmTALw926tJkybo27cv4uLicOrUKSxZskRhruTkZMTExEAkEsHQ0FDYmgkAc+fORUhICO7fv1/gewcAf39/jB8/XvgskUhgbW39UWMSERERERERFQUmuL5iq1evxosXLxAaGqpxn+3bt8PLywtr1qzBkCFDcO3aNQDQ6rysdxo0aICYmBisXbsWjx8/VqhbsGBBnmOOHDkSAQEBwufGjRvj999/x6xZs3D//n04OTkhKioK5cqVE9rUr18fYrFY6zg1JRaLcezYMfz8889o2rQpDA0N0b59eyxevFih3ftbFAGgcuXKuHLlCnJzc6Grq4vnz58jPT0dWVlZaN26NVJSUmBlZaUwRosWLRTu/30zZszAyJEjFeYo6P0U5fdFREREREREVFiY4PpKRUdHIyQkBImJiRofGJ+RkYEVK1YgIiICCxYswIYNG4Tzo9QRiUQKZ2q9efMGd+7cKZLESePGjbFq1SpUr15d5T2tXr0aRkZG+Y6Tnp4Od3d3yGQyhfLU1FS8efMGlSpVUigXiUT43//+BwsLC1SoUAFr1qxRO3azZs1Qvnx5hbIhQ4Zg9uzZcHFxEVZkBQcHo1GjRoiMjFS5gouIiIiIiIiI/g8TXF+psLAwuLu7Y+3atcLWuPwMGTIEPXr0gKOjI5YuXQp3d3e4u7srnQ/1PhcXF3h4eKBMmTLQ09ODnp4e6tati3Hjxqnto6OjI5xllZGRgZSUFCQnJ+Off/5Bx44d84yxZcuWiImJUUoiAW9XPamre5+RkZGwMu19mzdvxvnz57F27do8+wPA7t27sWrVKjx58kRI8DVv3hwTJkxQWpmmo6MjbLNUpU6dOujXr59C+8zMTGRkZCAzMxMvX77EP//8U6DD+YmIiIiIiIi+BExwfaWCg4Ph5eUFV1dXeHt7w8nJKc/2+/fvx6NHj7BhwwYAb98uuG7dOkRFReWZ4AoLC1NZnpiYqLZPly5d0LFjR8jlchgZGaFcuXKoVKkS6tati5ycnHzvrWnTpirf3KjNeVoAcObMGYSHh+e5IkuV9evXIywsDGFhYcIh95mZmdixYwdatmyJixcvKq0CA4DY2FgsXrwY//zzDwBAV1cX+vr66Ny5M0aPHi208/DwwMCBA7F3714YGxujQoUKqF69Ory8vLSKk4iIiIiIiOhLwQTXV8rU1BRmZmbYuHEj+vTpg4sXL6JUqVJq23fq1Ant27dXeBtg27ZtiyS2wMBABAYGFrj/X3/9pXKVlrbnhKWnp+PZs2fC53eJpPwcOHAAEydOVHiDo6GhIfr374+IiAicOXMGfn5+Cn3u37+PTp06Yd++fXBxcVGIYdasWRgyZAi2bt0K4O1h9tevX1c5t7rywhQ3uy1MTEyKfB4iIiIiIiIiTel86gCo+BkYGAhvI/T09ISvry/8/f3z7CMSiRSSWx8Si8XQ0dH8cdLX1xdiKGzqtuppu4Xvw3O82rdvj6lTp+bb79tvv8XKlSvx77//CmXZ2dn4/fffcf78eXh6eqqc692Krffp6Oho9d3q6+vn+d+JiIiIiIiI6EvEFVxfoWPHjil8Vvc2Pm3cvn0bgOYJlsqVK+PcuXNa9Xknr/bVqlUT3mD4PrlcDqlUqtEh8+9Ur14dZ86cgZ2dndo2fn5+mDVrlkLZ4MGDYWpqioEDB+L58+eQyWQQiUTw8PDA8ePHld6ICABVq1bF3r17ERQUhDt37gB4m9wyMDBA586d8fPPP2sU8/Tp04U/GxgYQE+Pf8WJiIiIiIjoyyeS82RqAjB37lzs3r1bZZ2Dg4OwPe5L4erqKhxk/6EZM2age/fuxRxR0YmIiFC78kwkEuH8+fMavdVSIpHA1NQU1mN3QkdcGokLOhR2qERERERERESCd79D09LS8j0qh8s7ikDFihXx5MkThTK5XI4NGzYgPDwcDx48ELb8NWrUCMOHD8c333yj9Tw2Nja4cOECzMzMCtxnwoQJaNiwIWbMmIEZM2YolP3www8AAFtbW6Smpmo0j62tLS5fvozffvsNjx8/VlhRpMrDhw/Ro0cPnDt3Dhs3blTqExERgbFjxwqfW7dujZCQELX3AwAPHjxA586dhS2JU6ZMwffffw8ACA8PR2xsLC5evJjvvZQkzs7OOHjwICpXrqxR+7Nnz2LJkiXYu3cvfHx84OPjU8QREhEREREREX06X+0ZXG3atMHp06e17rdhwwbY2toKV506dVCxYkWkpaUJbV6/fq3Ub/jw4Th69Cg2bNiAu3fv4t9//8XNmzcxYsQIjB07Ftu2bVNoHxwcjEqVKsHOzk64zM3NER0dLbSRSqUKbxXcuXOnQmw2NjYwNjZGUlKS2j65ublKbyb8sCwzM1Pl2wu7d++OEydOKJRlZmZCJpMhOzsb2dnZQvnEiRNhb2+vcD+JiYnIzs4WVlJ92AcAfHx8EB8fL1zvJ7dU3U9ISAj27NmDfv36oX///ujfvz+ePHmC4OBgXL58WWE+bZw7dw7t27dHw4YNYW9vj65duyrUr1+/Hvb29nB0dES7du20fmNjdHQ0XF1d0aBBAzRq1EhpG2lWVpbSd7Nx40aF/95jxowR6lR9l0RERERERERfqq92BVdWVlaBEh2DBg3CoEGDhM+XL19Gly5dYGxsnGe/P//8E5GRkahdu7ZQpqOjA3d3d4wfPx5//PEH+vTpI9QlJCRg/vz56N+/v1DWr18/9O7dW1iWl5ycrDCHn5+fwtv5zpw5g6lTp6JKlSpa3+f8+fOxYcMGAMDjx49VtsnIyFA6iF2do0ePYt++fahVq5ZCeWJiosr2jx8/Rr169dQeDL9o0SIMHjxYqVxdMg6A2vL8REREYOLEidi+fTscHBwAQCF5dPToUYSGhuLMmTMwNTXFzp070a1bN1y4cEGj8Z8+fYrvvvsOBw4cQOPGjXHz5k0hAVuzZk21/QYOHIiBAwcW6J6IiIiIiIiIviRfbYKrMMjlcowfPx6TJ0/O9y13ffr0weDBgzFnzhw0atQIpUqVQkpKCk6dOoV58+Zh3rx5+c4nEomwbNky4Xyo6tWrq22bmJiIMWPGYO/evZg6dSr2798PACpXFs2YMQNLliwRPj9+/BgjR45Ex44dAQCdOnVSOcfz589hbm6eb9zA2+9KV1dXo7YAYGlpiefPn0MulyMnJwd3795FhQoVhK2I6saaOHEiQkNDsWnTJiEJZWdnhyVLlqB8+fK4efOmxjEAgEwmw7Bhw7Bnzx4huQVA4ZD7devWITAwEKampgDeJhqDg4Nx5coVODk55TtHeHg4evTogcaNGwMA6tevj+HDh2PNmjVYvHixUvvo6Gj89NNPasdr0aIFevbsqektqiWVSiGVSoXPEonko8ckIiIiIiIiKgpMcOHtai4fHx8MGjQIvXr10rjf7Nmz8fLlSwwfPhyDBw8W3gqYkZGh1Hbu3LmYM2cOevbsCXNzc2RkZMDMzAz//PMP/vzzT3h4eGg0Z3JyMuLj4wFA7Ra0CxcuYMSIEcjKysKuXbuwYMECLFy4EIDqpNjcuXMVVoqNHTsWly5dQmZmJgDVWy4B4N69e0pvKyxMurq6iIyMxNChQ1G9enUkJSWhVatWWL16Nfr374/Y2FilVWyxsbFYt24doqOjUbp0aQDAmjVrMHHiRGzevFnrGC5cuAALCws0bNhQbZsTJ04obTFt3rw5jh8/rlGCKzY2Fl26dFEo8/LywsSJE1W29/T0FJ4BiUSCW7duoVKlSrC2thbanDp1Kt958xMUFITZs2d/9DhERERERERERY0JLgAjRoyAm5ubVsmtlStXYseOHRCLxfj1118RGhoq1BkZGansU6NGDfj4+Ahb/4C3q4s03UIol8vx22+/CWeHpaamKtSnpaUhKCgIJ06cwJ49e2BiYoKffvoJLi4uWL9+vcokjbW1NaZPn66wguvp06fYtm0b2rZtCwDYsWOHUr/79+/j2bNniIqKgqWlJZo3bw6ZTKaUcNLEjRs3hIPshw4dKpTn5uZiwIABiIyMRN26dSGTydCrVy9s3rwZW7ZsAaCcsKtWrRoyMzOxY8cO2NvbIzU1FYcPH0a7du2ENuHh4YiMjESbNm2wYsWKPGO7evUqGjRogJCQEGzevBk5OTlo0aIFAgICYGZmhvT0dOjp6aFMmTIK/aytrXH9+nWN7j8lJUXpAP/y5cvj2bNnefYLCwtDWFgYWrdujYSEBEilUvz666/CGxFPnToFW1tb2NnZqX1DZl78/f0xfvx44bNEIlFIohERERERERGVFF99gmv16tV48eKFQoIqL+np6Zg8eTJiY2Nx5swZyOVytGvXDjExMViwYIGwauid33//Hbdu3QIAXLt2DYmJiQgICBDqnz59iuDgYCHB4eXlBU9PT1SrVg1TpkzBggULhLbPnj1DREQEXFxcACgmd7KysuDu7o4BAwbg7NmzMDAwAADs2rULx48fV5t0mzBhAiZMmJDnPTdp0kRppdb27dvh5eWFNWvWYMiQIbh27ZpSTJpq0KABYmJisHbtWoXzvl6+fIkyZcqgbt26AN6eWdayZUvcunULjx8/Rnp6utK5WuXLl0d0dDR27doFPz8/tGvXDjNmzECTJk2ENr1791Y6rF6dFy9e4MiRI7C1tRVW6Pn7+8PX1xcnTpxAamqqylVshoaGePPmjUZzlC9fHi9evFAoS05OhpWVldo+SUlJWLRoEa5cuSLMHxAQgF9++QVTp04F8Har4qFDhzSKQRWxWCwky4iIiIiIiIhKsq86wRUdHY2QkBAkJiZqdFj669ev0bBhQ3Tu3BlRUVFCYuHs2bP4+eefceXKFbi7uyv0sbe3R8WKFQG8TTh86MOydwmiSZMmYdKkSRrfi4GBAa5evQo9PeX/pN7e3sKfx44dm++B+B/6cAVXRkYGVqxYgYiICCxYsAAbNmzI80wo4O35Ye8fGP/mzRvcuXMnzwSKhYUFypYtixUrVmDAgAG4d+8eVq1ahdDQUAQFBeHGjRsKiaGTJ08iKipKYc4XL15g3bp1mDNnDtLT0xW2YmpCR0cHNWrUwJQpU4SyBQsWoFKlSrh79y6MjY2FrZzvy8jIQKlSpTSaw9PTE5GRkfjuu++EskOHDqF169Zq+yQkJKBevXoKyTUXFxf89ttvGs1JRERERERE9CX5qhNcYWFhcHd3x9q1azVKJpUpUwbnzp2DhYWFQrlYLFZYafW++vXro379+gCAw4cPY+fOnbh+/TpevXoFc3NzuLq6YtCgQWrPapJIJDA2NtYoAfcuueXs7Kz2DZHW1tYYPXq0Urm3tzcePXqksk9aWhoSEhKEZNSQIUPQo0cPODo6YunSpXB3d4e7uzsaNGigNjYXFxd4eHigTJky0NPTg56eHurWrYtx48bleU/79u3Dzz//DE9PT1hYWGDx4sVo0qSJsCLr/RVjNWvWFL4nkUgEHx8flCpVCkZGRrCwsICJiQkuXryotJ0wLxUqVBBWkL2jr6+PqlWr4tmzZ6hRowYyMjKQnp6usEouKSlJ462n3333HRYvXoz9+/ejc+fOOH78OMLDw3H58mW1fRo2bIi///4b58+fR5MmTSCRSLB8+XL07dtX43sjIiIiIiIi+lJ81Qmu4OBgeHl5wdXVFd7e3hodCP4uudWqVSuEh4er3Eam6lyn8ePH48qVK5gxYwYaNmwIY2NjPHnyBKdOnYKfnx+CgoLg6+ur1K9Lly5YsmQJnJ2dleoOHjyIcuXKKZWrS4zI5XJUrVoVKSkpSv2OHz+u+obxNimWnp4OsViM/fv349GjR8I5YpUqVcK6desQFRWVZ4IrLCxMZXliYqLaPsDb73vevHkwMzNTmeQrXbq0kNirVq0aqlWrBgD47bffsH37djx48ADZ2dnQ19eHpaUlunbtmu9qs/e5uLhg1apVCmXZ2dl48OABatWqBZFIBDc3N5w+fRrt27cX2kRFRWn0ZkwAMDY2xrFjxzBu3DhMnz4dNWvWxNGjR1GhQgW1fczMzLBnzx5MmjQJT548gY6ODgYMGIAffvhB43sjIiIiIiIi+lJ81QkuU1NTmJmZYePGjejTpw8uXryo8bayp0+fQiqVqqwbMGCAUtn27dtx6dIlhUO6K1eujN69eyM3Nxc7d+5UmeCSyWSQyWQq57G3t9co1ndEIhF0dXUVtgq+065dO/z777/C2V3vq1+/vnBGWKdOndC+fXvo6+sL9e8Ooy8qvr6+apN8N2/eVCoLCQnBvn37sH79etSoUUMoT05ORkBAAEaMGIG1a9dqNLeDgwPKlCmDlStXYtSoUZDJZJg6dSratWuH8uXLAwBGjx6NWbNmwcPDAyYmJti5cydev36tckuqOtWqVcOePXs0bg+8Xal34sQJrfoUhrjZbWFiYlLs8xIRERERERGp89UmuAwMDIRkjqenJ3x9feHv74/g4GCN+muyZfB9bdu2hb+/PwIDA1GzZk0Ab98SeOHCBaxYsULh7YEfM09B3bp1C+fPn4elpWWe7UQikUJy60NisRg6Ojoaz6uvr68yqfY+uVyuNsmnikwmE7Ymvq9MmTIwMjJCenq6xmMBwM6dOzFkyBAEBwdDJBKhVatWWLNmjVDftWtXJCUloWnTptDR0YGlpSX279+v1fdARERERERERAX31Sa4jh07pvD5/TcbasLGxgatW7dW+QY9AJg1axb8/PyEz6GhoVi5ciV69uypcNaVg4MDAgMDFba3vc/W1hY9evRQe27UqFGjMGTIEI3jdnBwUHmwe7169dCsWTO1yaZFixapjfF9t2/fBvA2cZVXIuydypUrC28nVNfH1tYW3bp1U3pD5TtjxozBsGHDhM+jR4+GsbExvv/+e7x8+RI5OTnQ09ODkZERunXrhpEjR+Yb1/usrKxw4MCBPNuMHj1a5dlmhUVfX1/lCwTyaq/J909ERERERET0JRDJVe1X+0rNnTsXu3fvVlnn4OCArVu3FnNEVNRcXV3VHsg/Y8YMdO/e/aPGDwsLU3kmGwCUL18ekZGRHzW+prp06aL2vLMBAwZolJyTSCQwNTWF9didePBLj0KOkIiIiIiIiEjRu9+haWlp+R6VwwQXURHz9vbGkiVL4OjoqFC+cOFCrF+/Xvg8c+ZM9OvXr1DnKMw+THARERERERFRcdImwcVDgog+wqVLl2Bra6twlStXTuGMruzsbGRnZyv1nTJlChISEoRLXXJrz549qFChAuzs7ISrfPnyCA8PVzvH2bNnleIyMjLCX3/9lW9cRERERERERJ+br/YMLqLC4OLigvj4eIWyoUOH5vlygL1792LmzJlK5U+ePIGRkRFu3bqlcLbb3bt3MXz4cIVz4n7++WdMnDgRQUFBAIB79+4pjPXNN98oxJWQkIAuXbqgYcOGWt0fERERERER0eeAK7iIClFmZiYOHz6Mzp07q23TtWtXxMXFCVdUVBR69eoFc3NzzJ8/X+2LC94nEokwYcIEYQwXFxe1bV++fIkffvgBW7duRWhoqLCi6+LFiwW6RyIiIiIiIqKShiu4iArRrFmz0KlTJ0ydOhUXLlwAADx48ECp3X///Ydjx47hwIEDiIqKwuvXrzF9+nTY2toiKytL7dss3/fs2TNhldabN29Utrl9+zb69++P3Nxc/Prrr1i0aJFwoHyLFi3yHF8qlUIqlQqfJRJJvjERERERERERfQpMcBEVkgULFiAiIgJ//fUXypQpI5S/n0hKSEhAx44dUa5cOXzzzTeYPHky9u7di/j4eJw6dQrLli1DXFwcwsPDUb9+fbVzyeVyHDlyBP/++y8A5S2KmZmZWLlyJTZv3ozffvsNNjY2GD9+POzs7BASEoLWrVvnez9BQUGYPXu2lt8CERERERERUfFjgovoIyUnJ2PixIlITExEZGSkQnLrQ7Vr18aNGzegq6urUF6vXj3Uq1cPw4YNU+pjbW2NESNGYMeOHULZixcvEBoaiq5duwJQXo3l7e2NZs2a4fz58zA2NgYArFq1ChcvXoSOjmY7k/39/TF+/Hjhs0QigbW1tUZ9iYiIiIiIiIoTE1xEH2Hz5s3w9/fHmDFjsGXLFujr66ttu3DhQmzatEmp/J9//kHdunWVynv27ImAgAB89913+O6777SK6+TJk9DTU/7r7erqKvy5X79+eSasxGIxxGKxVvMSERERERERfQpMcBF9hJYtWyI+Ph6mpqb5tp0yZQqmTJmiVG5oaKj0JkZVXr9+DbFYrDJx9aF3bTp27IjExESVbczNzdGlS5d8xyIiIiIiIiIq6ZjgIvoI1apVAwAMHjwYHTp0UPn2xPnz58PW1lb43KRJE6Smpgqfzc3NhXqpVIpBgwZh+vTpSuOMGjUKHTt2RLdu3ZTq1q5dixo1aiiVHzp0SG3sHh4euHv3Lho1aqT+BomIiIiIiIg+A0xwERWCrKwsZGdnq6xzd3dX+Hz+/Hm14xw5cgShoaEq62QyGWQymcq69xNomtLT04NcLte6HxEREREREVFJo9lp00SUJ5FIVGjjqEs6FdYcHytudttPHQIRERERERGRAq7gIioENjY2GDduHAICAlTWd+vWDYGBgfmOo6Ojo/Yth4U1xzt169aFiYmJxu2JiIiIiIiISiqRnHuUiEqM3NxcZGRkwMjI6FOHokQikcDU1BRpaWlMjBEREREREVGR0+Z3KLcoEpUgurq6JTK5RURERERERFSSMcFFRERERERERESfNSa4CsDZ2RkPHz7UqO3+/fvx008/Fer8Dx8+hLOzs0LZ48ePUaFCBdja2qq8ypcvj6NHjxZqHPPmzcPy5cs1auvt7Y3bt28jKysL9erVK9Q4bGxskJqaWuR9iIiIiIiIiKhkYoLrA1u2bIGdnZ3CVbduXZw5c0Zok5WVhezsbADA/PnzYWtrq9QnJiYGAJCdnS201VROTg4CAgJQt25dVK9eHR4eHoiOjhbqs7OzkZWVpdDn4cOHqFOnDuLj41VeAwYMQEJCglZxnDt3Di1atIC1tTVq166NadOmITMzUyGO9+/t1KlTaNiwocL38C4B9q6tTCZDRkaGxjHI5XIsXrwYNjY2sLa2hqurKw4fPqzQRiqVIicnR6Fs586dcHR0RPXq1WFnZ4f169fn2yc/z549Q9OmTdG/f3+t+r1+/RojR46Eo6MjHB0d4erqisjISI37P378GK1bt0aDBg1gb28PZ2dnbN26VasY3tevXz/o6elBJpMVeAwiIiIiIiKikoRvUfxAv3790K9fP4WygQMH4sGDByrb//XXX1i7di1atGhRaDEEBAQgMTERly9fhpGREa5duwZfX1/s27cPDRo0UNlHLpdDJBKpHVNHRwfavE8gMTERPXv2xPbt2+Hh4YE3b95gypQpGD58OMLCwlT2iY2NRceOHTFnzhyN58nPsmXLcOLECZw5cwYWFha4ffs2unfvjtKlS6Nly5Yq+xw/fhzz58/HgQMHUK1aNTx58gS9e/eGnp4efvzxxwLF8c8//6B79+6wtbXVOjEml8vRoUMHrFixAjo6OoiNjUW7du1w9epVWFlZ5du/bNmyCA0NRc2aNYVYvL29YWJigi5dumgVy9mzZ/H8+XPk5uZCJpOpfWMjERERERER0eeEv241EBcXl2diqbBt2LABS5YsEQ4bd3BwwLBhw9QmlorCnj170L17d3h4eAAASpcujeDgYOzevRuvX79W2Ucul0NXV7dQ49iyZQsWLVoECwsLAG+3Fk6fPh3r1q1T22fjxo2YOnUqqlWrBgCoWLEiFi1ahJCQkALH8fjxY2zZsgXt27fXuq+RkRHatWsnJJMaNWoEd3d3/PXXXxr1NzAwEJJbAFC3bl2MGjUKf/75p1Zx5ObmYty4cVi2bJlG7aVSKSQSicJFREREREREVBJxBVc+/vvvPzx69AgODg5q2yQkJMDMzAwymQzZ2dl48+YNXF1dUaZMmQLP++FqLLlcjtzc3Dzbf7ht8X2ZmZlardbR19dX2kqYlZVV7Kt+VMXx5s0b6Onl/ehq+/3lp1mzZgCAq1evFniM9718+RKGhoYF7p+SkoLKlStr1WfNmjXw8PCAjY2NRu2DgoIwe/bsgoRHREREREREVKyY4MrH0qVLMWTIkDy3/+3duxfnzp2DSCSCvr4+jIyMYGdnJyS49u7di/Pnz8PDwwMbNmzId86ffvoJU6ZMwdq1a2FoaIjr169j1apV2LNnj9o+1atXh0gkgq2trcp6sViM3r175zv3Oz179oSrqyt27dqFb7/9Fk+fPsW4ceMwaNAglCpVSuNx3tepUyfo6+tr1Wfs2LEYPXo0Nm3ahNq1a+Ps2bOYM2cOfv/9d7V9Bg4ciKlTp8LT0xOVKlXC8+fPMWnSJAwZMqRAcRe2Gzdu4N69e2jVqpXWfV+9eoU9e/bg4MGDOHnypMb9nj9/jpUrV+LixYsa9/H398f48eOFzxKJBNbW1lrFS0RERERERFQcmODKw6VLl3Dw4EFcu3Ytz3aTJk3K8wyurl27YvPmzRrP+/PPP2P+/PlwdnaGTCaDubk51q9fDycnJ7V9ypUrhwsXLmg8R34sLCwQGRmJgIAA/PzzzzA3N4efnx9GjBhR4DEPHDiA2rVrq03CqdKnTx+UKlUKI0aMQHJyMmxtbfHbb7/B1dVVbR9vb2+kpaWhY8eOwmqvYcOGYdiwYQWOvbDIZDIMGTIEgYGBWq3gunv3Ltq3b4979+6hbNmy2L9/P8qVK6dx/6lTp2L8+PEwNTXVuI9YLIZYLNa4PREREREREdGnwgSXGv/++y969eqF8PBwlC5duljn1tPTw6xZszBr1iyN2jdt2hQpKSkaj//777/D0dEx33a1atXCtm3b1Nb7+voqrMgSiUQKZ5LJZDLcv3//o7c0du/eHd27d1dbHxwcDDMzM636fCrTp0+HlZWV1ofd16xZE/Hx8ZDL5Th9+jR69eqF/fv3qz0b7n0XL17E5cuXERoaWtCwiYiIiIiIiEo0JrhUiIyMxJAhQ7BmzRq4ubnl2VZHR0c4+0oqlSI1NRXJycm4c+cO3N3dCy2mV69e4f79+4iPj0fdunUV6lQdVr5582acP38ea9eu/ah5z5w5g9GjR6s830tfXx/BwcHCiix7e3v88MMPCA8PFxJflSpVKpStgf7+/jh06JDKQ/0tLS3x7bff5tlfKpUiKSkJd+7cUTiwvTht374dR44cwdmzZws8hkgkQvPmzTFkyBBs3LhRowPjR48ejV9++YVvTCQiIiIiIqIvFhNcH5g+fTr+/PNP7N+/H3Z2dvm2b9++PUaNGgWZTIbSpUujbNmyqFSpEmrXrp1vckyVoUOH4u+//0ZWVhZycnKEpISJiQmqVasGW1tb1K5dW+txC+rChQto27YtgoKClOqmTp2KmJgYNG/eHADQpk0bPH36VOU4q1ev/qg4Dh06hN27dysdkC6TyVClShWkpqaiXLlyCA0NRVhYGLKzs5GVlSV8f4aGhrC2tkadOnVQtWrVj4qlIKKjozF9+nScPn36o14+8E5aWhpkMplGbZ8/f44xY8YolTdu3Bg//vijyjoiIiIiIiKizwkTXB8YPnw4AgMDoaurq1H7oUOHYujQoWrrL126pNX8y5YtQ3Z2NvT19dVujUxMTNRqzI8hl8vVnsNkaGiockVVccaho6MDAwMDIY7+/fvju+++g66uLkqXLl0iVi39888/+P7777Fv374CHdKelJQES0tLYVXc0aNHERoaihMnTmjUPyEhQalMJBIhJiYm37dREhEREREREX0O+Ov2A5UrV/6k8xf3eV+aUJfEksvleb5d8n36+vpav0GxIHEYGBjAwMDgo+bJS0HGX7VqFdLT0zFo0CCF8m7duml0ztq2bduwadMmGBoaQk9PD9WqVcPx48dhb2+vVRzvE4vFGv+3IyIiIiIiIirpmOAqAH19fY1XvhRGYudDenp6+Y6pp6dXKKtzqlatiuHDh+OPP/5Qqnv69CnWrVun0TjHjx8HAGRlZWn19sB3atSoAS8vL6UEoFwuR25uLoyNjbUaTywWa7xK7329evVCr169tOqzfPlyLF++XOu53pk2bRqmTZtW4P6qZGZmFup4RERERERERJ+SSF5ce8wIANClSxe1WwwHDBiA0aNHF3kM165dQ9++fVXWiUQi7N27F9WrVy/yOObOnYvdu3errHNwcMDWrVuLPAYAcHV1VXmIPgDMmDEj37cxfu79NSWRSGBqaoq0tDSYmJgUyphERERERERE6mjzO5QJLiLSCBNcREREREREVJy0+R366U/gJvqMjBkzRusXBxSHrVu34ptvvoGjoyMaNGiA6dOnK71l8ezZs3Bzc4OTkxPc3NwQHR39iaIlIiIiIiIiKlxcwUVUjB4+fIgTJ06o3SJaUMeOHUOjRo1Qrlw5vH79Gh07doSvry9GjhwJ4O15aS4uLjhy5Ajq16+P27dvo23btjh//jwsLS01moMruIiIiIiIiKg4cQUXUQl1584dhIaGFvq4bdq0Qbly5QAAZcqUwZgxY3Ds2DGhfseOHejZsyfq168PALCxsUGvXr2wY8eOQo+FiIiIiIiIqLgxwUWkhTZt2uD06dO4fPkymjRpAicnJzRu3BgRERH59h0zZgwGDRqEK1euwMnJSThEPycnB9OnT0etWrVga2sLFxcX4a2TALB9+3bMmjULffr0QaNGjVCzZk2MGTMGOTk5aud6+fKlwtsqIyMj0bx5c4U2zZs3V5jnQ1KpFBKJROEiIiIiIiIiKomY4CLSQlZWFrKysjBu3DisWrUKV65cQUxMDNq2bZtv3+XLl2PDhg1wcnLClStXhG2K06ZNQ1xcHK5du4b4+HisX78eAwYMwI0bN4Q5g4OD4evri9jYWKHt8uXL1c61bt06hW2QycnJsLa2VmhjbW2Nu3fvqh0jKCgIpqamwvVhfyIiIiIiIqKSggkuogKQyWQKK6hEIlGBxnn9+jVCQ0MRGhqKMmXKAACcnJwwfvx4LF68WGjn5uaGLl26AABKly6NefPmYfPmzSrHDA0NhaGhITp27CiUpaamKqzoAgBDQ0O8efNGbWz+/v5IS0sTrqSkpALdIxEREREREVFRY4KLqACWL1+OwYMHY/Dgwbh//36Bx0lISEDlypVRsWJFhXIPDw9cu3ZN+Ozk5KRQb29vj3v37imN9/fff2Pu3LnC9sd3xGIxMjMzFcoyMjJQqlQptbGJxWKYmJgoXEREREREREQlERNcRAXg7OyMy5cv45tvvkGTJk1w5syZAo2jq6urslwulyvUZWdnK9S/efNGKTn1+PFj9OjRA1u2bEG1atUU6qpUqYIHDx4olCUlJaFKlSoFipuIiIiIiIioJGGCi6iAdHV10a9fPyxevBgLFizQuM/76tSpg8ePH+Px48cK5WfPnkXDhg2Fz1euXFGoj42NFd6ICLxNeH377beYOnUqWrZsqTSvu7s7oqKiFMqioqLg7u6uUdxEREREREREJRkTXEQFkJKSIvz5+vXrqFy5skb9ypUrh//++w+5ubkA3m4DHDp0KH766Sekp6cDAC5fvoxly5Zh/PjxQr/z589j9+7dAN6epzVr1iyMHDkSwNvzwL7//nu0aNECgwYNUjnvwIEDER4ejps3bwIAbt++jW3btmHgwIFa3jkRERERERFRyaP3qQMg+pwYGBjAwMAArVq1wuvXr6GnpwdbW1usX79eo/716tWDi4sL7Ozs0KRJE2zatAmBgYGYO3cuGjZsCF1dXZiZmeHXX3+Fra2t0G/48OH4448/MGvWLLx69QoTJkxAjx49AABXr17FgQMHYG9vj+PHjwt9xGIxzp07B11dXVSpUgXbtm1Dnz59kJOTA11dXWzatElpKyMRERERERHR50gkl8vlnzoIIlJv8+bNSExMREBAwCeNQyKRwNTUFGlpaTxwnoiIiIiIiIqcNr9DuYKLqJDMnTtX2Eb4IQcHB6U3G2pKV1cX+vr6HxMaERERERER0ReNK7iIShBvb2+EhITg/v372LVrl0ZbH8+ePYslS5Zg7969CuV//vknAgMD8fr1a8jlcrRu3Rrz589H6dKlAQAuLi7YvXu3xtsUuYKLiIiIiIiIipM2v0N5yDxRMZszZw5sbW2Fy9nZGdeuXQMAZGdnIzs7G1lZWcjOzhb6/PLLL7CxsUGlSpXQqFEj7Nu3T6h71+d9CQkJGDduHHbs2IHr16/j6tWrsLKywtChQ/PsR0RERERERPQ5YoKLqJjNnDkT8fHxwmVlZYWkpCS17desWYPDhw8jOjoaycnJ+O233zB9+nScOHFCaHP69GnY2dnBz88PABAXFwdPT09Ur14dwNttjv3798dff/1VpPdGRERERERE9CkwwUX0icXHx2PMmDGws7PDpUuXlOp///13zJs3DxUqVAAA1K1bF9OnT1fYvtisWTPExcVh586dAIBvvvkG586dw9GjR5GZmYl///0Xo0aNQq9evYrnpoiIiIiIiIiKEQ+ZJ/qETp48iTp16iAiIgIA0KJFC6U2IpEIMplMoSwnJwc6Ov+Xn05LS0NMTAyMjY1hY2MDCwsLHD58GIsWLUJgYCDMzc3Ro0cP9O3bFzx2j4iIiIiIiL40THARfSJyuRz+/v6YN29enu369u2LKVOmYOfOnbC0tMT169cRGBiIsLAwoc2dO3cwd+5c1KxZE3369EFoaCjkcjn09fVRuXJlmJub4++//8bFixeRnZ2NcePG5RufVCqFVCoVPkskkoLfLBEREREREVERYoKL6BOZPHky6tWrBy8vrzzb/fjjj5DJZGjXrh1SU1NhZWWFlStXolmzZkKbxo0bKxw8v2bNGuHPHTt2xPDhw9GiRQsMGzYM586dw7lz5/Dvv//mOW9QUBBmz55dsJsjIiIiIiIiKkZMcBEVM7lcjnnz5uHSpUs4cuQIli5dik2bNgEA7t27p7LPwIEDMXDgQJV1tra2Cm9H/JBIJIJIJAKgmPgaNmxYnq9Z9ff3x/jx44XPEokE1tbW6m+MiIiIiIiI6BNhgouoGMnlcnTt2hW6uro4dOgQSpUqhQkTJmDChAkAVJ/BBQB79uzBtGnTVNbp6OioTHC9evUKgYGBsLKywr59+3Dw4EGF+ho1aqB8+fJqYxWLxRCLxRreGREREREREdGnwwQXUTESiUQYNWpUvtsSP9StWzd069ZNZV1kZCSWL1+O0aNHK5SXKlUK3bt3V3uo/HfffYfvv/+eq7KIiIiIiIjos8cEF1Ex0za5lR89PT2VSSwdHR3Mnj0bSUlJKutzcnJgZmZWqLEQERERERERfQpMcBGVIHp6etDX1y+UsVJSUnDt2jX8999/hTIeERERERERUUnFBBdRCRIZGQkASExM1DjRZWRkBCsrK6VyExMT6Orqws7OTm3fgIAAdO/evWDBEhEREREREZUQIrm6A3qIiN4jkUhgamqKtLS0PN++SERERERERFQYtPkdqlNMMRERERERERERERUJJriIShBvb2/cvn0bx44dw08//aRRn7Nnz6Jr165K5REREbCzs0OFChXQtGlTXLhwQaHe2dkZDx8+LJS4iYiIiIiIiD4lJriIitmcOXNga2srXM7Ozrh27RoAIDs7G9nZ2cjKykJ2drbQ55dffoGNjQ0qVaqERo0aYd++fULduz7v+/vvvzFu3Djs27cPT58+RXBwMHr27Im7d+8KbT6cg4iIiIiIiOhzxQQXUTGbOXMm4uPjhcvKygpJSUlq269ZswaHDx9GdHQ0kpOT8dtvv2H69Ok4ceKE0Ob06dOws7ODn58fAGDFihWYPHkyateuDQBwc3PDyJEj8csvvxTtzRERERERERF9AkxwEX1i8fHxGDNmDOzs7HDp0iWl+t9//x3z5s1DhQoVAAB169bF9OnTsX79eqFNs2bNEBcXh507dwIAHj58iGrVqimMY2NjgwcPHhThnRARERERERF9GkxwEX1CJ0+eRJ06dZCQkIC4uDi4uLgotRGJRJDJZAplOTk50NH5v7++aWlpiImJwe3btwEAtWvXxvXr1xX6xMTEwMbGpgjugoiIiIiIiOjTYoKL6BORy+Xw9/fHpEmT8mzXt29fTJkyBY8fPwYAXL9+HYGBgRg6dKjQ5s6dO5g7dy7WrVsHAJg8eTKCg4MRFRWF169fY9++fdiyZQsmTpyocXxSqRQSiUThIiIiIiIiIiqJ9D51AERfq8mTJ6NevXrw8vLKs92PP/4ImUyGdu3aITU1FVZWVli5ciWaNWsmtGncuLHCwfPVq1fH/v37MXnyZNy5cweOjo743//+J2xzBN6ey2VkZKR23qCgIMyePbvgN0hERERERERUTERyuVz+qYMg+prI5XLMmzcPkZGROHLkCFavXo1NmzYBAO7du4cLFy4gMTERu3fvxubNm/Md7/Hjx4iJiUHHjh0LNU6pVAqpVCp8lkgksLa2RlpaGkxMTAp1LiIiIiIiIqIPSSQSmJqaavQ7lCu4iIqRXC5H165doauri0OHDqFUqVKYMGECJkyYAABo0aKFyn579uzBtGnTVNbp6OgobFd8X79+/RAbG6uy7uXLl7h16xZMTU1V1ovFYojF4nzuiIiIiIiIiOjTY4KLqBiJRCKMGjUq322JH+rWrRu6deumsi4yMhLLly/H6NGjleq2bNmidsx69eohNTVVbYKLiIiIiIiI6HPBBBdRMdM2uZUfPT09qNtpPHToUBw7dgylS5dWqrOyslI4k4uIiIiIiIjoc8UEF1EJoqenB319/UIbLz4+Hjt37kTjxo0LbUwiIiIiIiKikoYJLqISJDIyEgCQmJiocaLLyMgIVlZWKutsbW3h5+encgUXAAQEBKB79+4FC5aIiIiIiIiohOBbFIlII9q8vYKIiIiIiIjoY2nzO1SnmGIiIiIiIiIiIiIqEkxwERERERERERHRZ40JLqLPSMuWLVG7dm3hun37doHGmTdvHlasWFHI0RERERERERF9Gkxw0VehTZs2OH36dIH7z5kzB3p6epDJZArlJ0+ehJubGxwcHFC/fn14eXnhr7/+UmjTunVr1K5dG05OTsIVGBhYoDhOnjyJhIQE4bKxsVHZrlWrVrC1tVW4HBwcEBERAQDIzs5GVlZWgWIgIiIiIiIiKmn4FkX6KmRlZRUooZOTk4PBgwdDLpcjNzcXMpkMOjr/lxe2s7PDn3/+iXLlygEADhw4gPbt2+PGjRuoVKmSMMbatWvRunXrAsffrVs33Lx5U6lcR0cHNjY22Lt3r0L5//73P6W2q1evxsmTJ+Hj41PgOIiIiIiIiIhKIia4iPIglUrh7u6OQYMGYfPmzUr1FhYWCp87deqEb775BqdOnUKvXr0KLY49e/YolaWkpMDX1xcVK1bUaAyxWAxdXV2N55RKpZBKpcJniUSicV8iIiIiIiKi4sQtivTVycrKQqtWrbB9+/Z825YpUwaDBg3SavyUlBRUrly5oOFpZN++fXByckLjxo2xevVqjfpIJJJ8X6v6vqCgIJiamgqXtbV1QcMlIiIiIiIiKlJMcNFXZ8SIEXBzcyvUFVYAkJycDH9/f1hYWKB58+aFOjbwNjG3c+dONG3aFL6+vqhZsya6desGkUikts+0adNw7NgxAMDDhw+FbZOa8Pf3R1pamnAlJSV99D0QERERERERFQUmuOirsnr1arx48QLz588vtDF37tyJGjVqwNraGidPnsT69esV6kUiEaZNmwZnZ2c4Ojpi7NixePnypcbj7927Fz/88APq1KmDgwcPYsmSJUhJScGQIUOwatUqODg4oEePHirP6EpOTha2FsbFxSkcSr9o0SLY2tpi4cKFKucVi8UwMTFRuIiIiIiIiIhKIp7BRV+N6OhohISEIDExMc9VT9ry8/ODn58fpFIptm3bhlatWuHSpUswNDQE8DYBVrZsWejq6kIikWDatGno2bOnsLIqP7m5uRg4cCDCwsJgYGCAxMRE7N27F/369UPPnj0hl8tx8+ZNVK1aNc9xNm7cCEtLS+Hz5MmTMXHixILfOBEREREREVEJwRVc9NUICwuDu7s71q5dWyTji8ViDBo0CFWrVlVIXllYWAiHu5uYmCA4OBjR0dFIS0vTaNzu3bujZcuWMDAwAAAkJiZi165dQr1IJEKDBg1gZGSU5ziVKlXC8+fPERsbq+2tEREREREREZVoXMFFX43g4GB4eXnB1dUV3t7ecHJyKpJ50tLSIJPJ1NbLZDLo6Oho9UZDbQwZMkRIYt27dw+RkZGYPXs2dHV1UaVKFdjb20MsFhfJ3ERERERERESfAhNc9NUwNTWFmZkZNm7ciD59+uDixYsoVarUR41579491KhRAwCQk5ODZcuW4enTp2jbtq3Q5v79+6hWrRqAt28yHDduHL799tt8V1wBgK+vL27cuKGyztbWVqls2bJlWLZsGTIzMwG8XVVWpkwZpS2ZAQEBGt0fERERERER0eeACS76KhgYGAhb/Dw9PeHr6wt/f38EBwdrPEapUqWUEkXjxo3DjRs3ULp0aYhEInh4eODMmTMKibORI0fi33//Febv1q0bJk2apNGcf/zxh8bxva9MmTIF6kdERERERET0ORLJ5XL5pw6CiIrXvHnzYGRkhDFjxmjcRyKRwNTUFGlpaXyjIhERERERERU5bX6HMsFFX625c+di9+7dKuscHBywdevWIo9h+PDhOHfunMo6Hx8fLFiwoMhj0BQTXERERERERFScmOAiokLHBBcREREREREVJ21+h+oUU0xEX53w8HCMGzdO634VKlTQep7x48errVeXw7axsUFqaqpWcxERERERERGVRExw0VevTZs2OH36tNb9AgMDYWtrq3BVrVoVo0aNAgBkZ2cjOztboU9ISAjs7OwUrgYNGuDBgwdCmzdv3ij0GThwIA4ePKhUtnfvXmGerKwslTG+evUKVatWVVknlUqRk5Oj3U0TERERERERlUBMcNFXLysrS22CKC+zZs1CfHy8wrV7927cuXNHbZ+RI0ciLi5O4apevTru3buntk9ubi5yc3PzLVPlwYMHqFixouY3RURERERERPQZ0vvUARB97uRyOWQyGXR1dZGSkgILCwut+otEIrXbCN+ZMWMGgoODhc/x8fHo2LFjvmPv27cP//zzD9LS0mBqaqpVXFKpFFKpVPgskUi06k9ERERERERUXLiCi+g9WVlZaNWqFbZv365xny1btmDChAkAgNjYWDg7Owt14eHhsLW1xejRo9X2z8nJgUgkynOOGTNm4NChQ8LVunVrhfp384wZM0Yoe/HiBTZu3IiePXti0qRJGt/PO0FBQTA1NRUua2trrccgIiIiIiIiKg5McBG9Z8SIEXBzc0OvXr007pOTkyOcZTVmzBgMGzZMqOvduzfi4+OxYsUKAMDKlSthZWWlcG5XSkoK6tatq3Z8kUgEPT09GBkZCde78g/nWb58OQDg9evX8PPzw7hx47B69WokJSVh6tSpGm1rfMff3x9paWnClZSUpHFfIiIiIiIiouLELYpE/9/q1avx4sULhIaGFqh/dnY25HI5nj9/jqdPnyps73vn9u3bCAoKQv/+/TUe183NDYGBgQgICBDKDA0NUa9ePZXt79y5gy5dumDgwIHCgfd79+7F0KFD8d1332H37t0azSsWiyEWizWOk4iIiIiIiOhTYYKLCEB0dDRCQkKQmJiY73ZBAFiyZAlCQkIAvF1JpaOjg8jISJQuXRply5ZFhQoVYGdnVyixDR06FEOHDlVbb2lpiVq1agmfy5Ytiw0bNqBp06ZCmaGhITZv3oy0tDShbNasWTAxMSmUGImIiIiIiIg+JSa4iACEhYXB3d0da9eu1ei8qokTJ2LixIl5ttm8eTOSk5MVykQiETIyMpCeno43b97gzZs3ePDgAeLj43Hnzh0EBgaqHe+///5DUFAQTp48CblcDrlcjipVqmDEiBEYN26c0K5cuXJo2rQppFIpnJ2d1W5LrFq1qlYryYiIiIiIiIhKKia4iAAEBwfDy8sLrq6u8Pb2hpOTk0b9oqOjkZCQgB9//FGp7v1VVe+0bNkSgYGBWLVqFQwNDWFubo7KlSujVq1aaNasGfT19VXOk56ejubNm2PKlClYunQpDA0NAQBXrlzB4MGD8fz5c/z0008KfcRiMW7cuKFyPLlcjqpVqyIlJQXlypXT6F6JiIiIiIiISiomuIgAmJqawszMDBs3bkSfPn1w8eJFlCpVKt9+//77L2JjY1UmuDw9PeHp6alQ1q1bN3Tr1k3r+OLi4mBiYoLBgwcrlDs5OSEwMBArVqxQSnDlRSQSQVdXF3K5XOtYiIiIiIiIiEoavkWRvnoGBgYwMDAA8DYp5evrC39/f436anJeV2Gws7PDq1evsG3bNmRnZwvl8fHxmDdvHrp3714scRARERERERGVRCI5l3AQFdiZM2fg6+sLCwsLtW327t2LOnXqaDymhYUFnj17plT+4MEDzJs3D2fPnoVMJgMAWFlZYeTIkejatavWsbdo0QIHDhzQ+KB5iUQCU1NTpKWl8XB6IiIiIiIiKnLa/A5lgotIhblz52L37t0q6xwcHLB169ZijujTY4KLiIiIiIiIihMTXEQlgLe3N0JCQnD//n3s2rUL69evz7fP2bNnsWTJEuzdu1fruZYsWQJHR8eChpsvJriIiIiIiIioOGnzO5RncBF9hDlz5sDW1la4nJ2dce3aNQBAdnY2srOzkZWVpXBu1i+//AIbGxtUqlQJjRo1wr59+4S6d30+9Ouvv8Le3l544+KcOXOQm5ubZz9fX1/UqFEDdnZ2Kq8+ffoU8rdBRERERERE9GnwLYpEH2HmzJmYOXOm8LlDhw5ISkqCg4ODyvZr1qzB4cOHER0djQoVKuCff/5B165dYWxsDC8vLwDA6dOnYWdnh/r162Pnzp04cOAAfvnlFxw6dAjVqlXDf//9h++++w7r1q2DmZkZAODevXtKc92+fRvHjh3T6vwvIiIiIiIios8RV3ARFaL4+HiMGTMGdnZ2uHTpklL977//jnnz5qFChQoAgLp162L69OkK2xebNWuGuLg47Ny5EwAQFhaGmTNnolq1agCAKlWqYOHChbC2tkZcXBzi4uLg4uKiMp7iessjERERERER0afEFVxEheTkyZOoU6cOIiIiALx9S+GHRCKR8AbEd3JycqCj83+55rS0NMTExMDY2Bg2NjYAoNRHJpPB2Ni4kO9AkVQqhVQqFT5LJJIinY+IiIiIiIiooLiCi6gQyOVy+Pv7Y9KkSXm269u3L6ZMmYLHjx8DAK5fv47AwEAMHTpUaHPnzh3MnTsX69atAwAMGjQIc+bMQVJSEgDg4cOHmDhxIp4+fYoWLVqgRYsWuHLlitJcIpEIOTk5Bb6noKAgmJqaCpe1tXWBxyIiIiIiIiIqSlzBRVQIJk+ejHr16gnnaKnz448/QiaToV27dkhNTYWVlRVWrlyJZs2aCW0aN26scPB8x44d8erVK3To0AFpaWkoXbo05s2bh27dugltVK0W8/LyQufOnYVtinfv3kXNmjWF+qpVq+LYsWNqY/X398f48eOFzxKJhEkuIiIiIiIiKpFEcrlc/qmDIPpcyeVyzJs3D5GRkThy5AhWr16NTZs2AXh78PuFCxeQmJiI3bt3Y/PmzfmO9/jxY8TExKBjx45axdGiRQssWbIEjRs3VtvG0NAQmZmZWo37Pm1ez0pERERERET0sbT5HcoVXEQFJJfL0bVrV+jq6uLQoUMoVaoUJkyYgAkTJgBQvaoKAPbs2YNp06aprNPR0VHYrkhERERERERE+WOCi6iARCIRRo0ale+2xA9169ZNYXvh+yIjI7F8+XKMHj1aqW7cuHFo3LgxevfurVS3cuVK1KlTR6s4iIiIiIiIiL4UTHARfQRtk1v50dPTg7pdw9nZ2cjOzlZZZ29vL/x54cKFwjbJ91WvXh22trZK5T179kRAQEDBAiYiIiIiIiIqAZjgIioienp60NfXL7Tx3h0Wn58pU6ZgypQphTYvERERERERUUnHBBdREYmMjAQAJCYmapzoMjIygpWVlcq6unXrYvr06ViyZInKej8/P8yaNatgwRIRERERERF9xvgWRSLSCN+iSERERERERMVJm9+hOsUUExERERERERERUZFggouokKhbDOnt7Y2rV68qlS9cuBC1a9cWri1bthR47jZt2ijN4ebmhjt37qhsf+vWLTRv3rzA8xERERERERGVJExwERWSunXr4uXLl0rl6t5+OGXKFCQkJAhXv3791I69Z88e1K9fH1ZWVvD09FRKZmVlZSnNkZubi9zcXJXj5VVHRERERERE9LnhIfNEhSA3NxdPnz7V6GyqvXv3YubMmUrlT548gZGREW7dugVDQ0OhPC4uDsOGDUNkZCTs7e1x8OBBdOjQATdu3ICpqWmec3Xq1AkGBgZK5VKpFBUrVtTgzoiIiIiIiIhKPia4iArB2bNnkZOTg4yMDBgbG+fZtmvXrujatavw+cWLF1i3bh02b96M2bNnKyS3AGD9+vWYOHEi7O3tAQDffvst9uzZg1q1asHS0hIAcO/ePZVzHThwALa2tkrlcXFxGDp0qFb3SERERERERFRSMcFFVAg2btyISpUqYcWKFZg+fXq+7f/77z8cO3YMBw4cQFRUFF6/fo3p06fD1tYWWVlZCquuYmNj0bt3b4X+bdu2hUwmE87tatGihdIcIpFI5dZI4O2WRpFIlGeMUqkUUqlU+CyRSPK9LyIiIiIiIqJPgQkuoo906tQpnD17FufPn0eLFi3QvHlzeHh4qGybkJCAjh07oly5cvjmm28wefJk7N27F/Hx8Th16hSWLVuGuLg4hIeHo379+gCAlJQUmJubK4xjYWGBp0+f5hlXq1at0KtXL5WH34tEInTv3j3P/kFBQZg9e3aebYiIiIiIiIhKAia4iD7C5cuXMXDgQISHh6N8+fL4448/0KlTJ8yePRs9e/ZUal+7dm3cuHEDurq6CuX16tVDvXr1MGzYMKU+5cqVUzq8/uHDh6hcuXKesS1cuBALFy4swF295e/vj/HjxwufJRIJrK2tCzweERERERERUVFhgouogJKSktC1a1ds2bIFTZo0AQDY2Njg6NGjmDlzJrp166aw1XDhwoXYtGmT0jj//PMP6tatq1Tes2dPBAQEwMPDA0ePHoWbm5tQd+DAAcTGxsLOzg6A4hlc6uZR5908HxKLxRCLxRqPQ0RERERERPSpiOSq9i8RkUZevXqV76HyLVq0wJIlS9C4cWOV9YaGhsjMzFTb//79+/D09ERYWBg8PDywadMmLFu2DNevXxcOpM9vDk3myY9EIoGpqSnS0tI0elskERERERER0cfQ5ncoV3ARfYR3ya3BgwejQ4cO6Ny5s1Kb+fPnK7zJsEmTJkhNTRU+m5ubC/VSqRSDBg1SOKi+WrVq2LdvHyZPnoz79+/DyckJx48fV3rbIhEREREREdHXigkuokKQlZWl9o2F7u7uCp/Pnz+vdpwjR44gNDRUqdzZ2RmRkZEfFyQRERERERHRF0rnUwdA9CUQiUSFNg53DRMRERERERFphyu4iAqBjY0Nxo0bp/KwdgDw8/PDrFmz8h1HR0cHOjra55319fWhr6+fZxseGE9ERERERERfKh4yT1SC5ObmIiMjA0ZGRp86FCU8ZJ6IiIiIiIiKkza/Q7lFkagE0dXVLZHJLSIiIiIiIqKSjAkuIiIiIiIiIiL6rDHBRUREREREREREnzUmuIiIiIiIiIiI6LPGBBcREREREREREX3WmOAiIiIiIiIiIqLPGhNcRERERERERET0WWOCi4iIiIiIiIiIPmtMcBERERERERER0WeNCS4iIiIiIiIiIvqsMcFFRERERERERESfNSa4iIiIiIiIiIjos8YEFxERERERERERfdaY4CIiIiIiIiIios8aE1xERERERERERPRZY4KLiIiIiIiIiIg+a0xwERERERERERHRZ40JLiIiIiIiIiIi+qwxwUVERERERERERJ81JriIiIiIiIiIiOizxgQXERERERERERF91pjgIiIiIiIiIiKizxoTXERERERERERE9FljgouIiIiIiIiIiD5rep86ACL6PMjlcgCARCL5xJEQERERERHR1+Dd7893v0fzwgQXEWnkxYsXAABra+tPHAkRERERERF9TV69egVTU9M82zDBRUQaKVu2LADgwYMH+f7DQvQpSCQSWFtbIykpCSYmJp86HCKV+JxSScdnlD4HfE6ppOMzWnjkcjlevXqFSpUq5duWCS4i0oiOztsj+0xNTfmPNJVoJiYmfEapxONzSiUdn1H6HPA5pZKOz2jh0HSBBQ+ZJyIiIiIiIiKizxoTXERERERERERE9FljgouINCIWi/Hzzz9DLBZ/6lCIVOIzSp8DPqdU0vEZpc8Bn1Mq6fiMfhoiuSbvWiQiIiIiIiIiIiqhuIKLiIiIiIiIiIg+a0xwERERERERERHRZ40JLiIiIiIiIiIi+qwxwUVEgvXr18Pe3h6Ojo5o164dHj58qLbtq1ev8MMPP8DOzg4NGjRAYGAgeKQfFQdNn1O5XA5/f384OzvD0dERTk5O2LFjRzFHS18jbf4tfd/PP/8MkUiExMTEog2QCNo/pzdv3kSPHj3g5OQEBwcHuLq6FlOk9LXS5hm9cOEC2rZti4YNG8LOzg59+/bF8+fPizFa+pqFhYVBLBbn+7/f/P1U9JjgIiIAwNGjRxEaGoozZ87g6tWr+PHHH9GtWze17QcPHoz69esjLi4Of//9Ny5fvow1a9YUY8T0NdLmORWJRHBxccH58+dx9epV7Nu3DxMmTMDVq1eLOWr6mmj7b+k7d+/exZEjR1ClShXk5OQUQ6T0NdP2Ob1y5Qo6d+6MkSNH4sqVK7h27RrOnj1bjBHT10abZ/TevXvo0aMHFi1ahL///htXr15FrVq10Ldv32KOmr5GM2fOxK5du2Bubp7v/37z91PRY4KLiAAA69atQ2BgIExNTQEAfn5+0NXVxZUrV5Tavnz5EmfPnsWUKVMAAAYGBli0aBFCQ0OLM2T6CmnznAJAt27dYGBgAACoXr06evTogRMnThRXuPQV0vYZfWfMmDFYsGABdHV1iyFK+tpp+5yOGTMGixYtQvPmzYUyfX394giVvlLaPKPnzp1Do0aN4OjoCADQ1dXFiBEjcObMmeIMmb5CMpkMVlZWOHToEAwNDfNsy99PxYMJLiICAJw4cQLNmjVTKGvevDmOHz+u1PbUqVNo0qSJwg+xunXr4unTp3j69GmRx0pfL22eU1VevnyZ7/8BQvQxCvKMHjp0CPr6+mjVqlVRh0cEQLvnNDk5GXfu3EGnTp2KKzwirZ5RFxcXREVFCSu05XI5AgICFBKyREVBR0cHw4cP1+j/OcXfT8WDCS4iQnp6OvT09FCmTBmFcmtra9y9e1epfXJyMqytrZXKq1Spgnv37hVZnPR10/Y5/dCzZ88QEREBX1/fogqRvnIFeUalUimmTp2KJUuWFEeIRFo/p9euXYOtrS12796NJk2awNHREQMHDkRycnJxhUxfGW2f0bp162Lp0qVo2bIlJkyYgGbNmiE2NhabNm0qrpCJ8sXfT8WDCS4iQmpqqspVLYaGhnjz5s1HtycqDB/73I0ZMwbDhg1DxYoViyI8ogI9o4sXL8a3336LmjVrFnV4RAC0f05fvHiBmzdv4uzZs/jf//6Hy5cvw97eHl5eXsjOzi6OkOkrU5B/S318fODh4YFly5bh4sWL+Omnn1CuXLmiDpVIY/z9VDyY4CIiiMViZGZmKpVnZGSgVKlSH92eqDB8zHMXGhqKxMREzJgxo6jCI9L6GX3w4AE2b96M6dOnF0d4RAC0f051dHSgp6eHX375BaVLl4auri7Gjh0LAwMDREdHF0fI9JXR9hn977//0LBhQ1SrVg0PHjzA3r17sWTJEvTp06c4wiXSCH8/FQ8muIgI5cuXR0ZGBtLT0xXKk5KSUKVKFaX2VapUwYMHD5TK1bUnKgzaPqfvREVFYcGCBdizZw8PRaYipe0zOnnyZMycORNGRkbFFSKR1s9phQoVUKtWLaUzZmrXro1nz54Vaaz0ddL2GV27di18fHywcuVKWFtbo3379jhz5gyOHDmCf/75p7jCJsoTfz8VDya4iAgikQhubm44ffq0QnlUVBTc3d2V2jdt2hRnz55Fbm6uUHb79m0YGBjwH2gqMto+pwAQHx+PPn36YM+ePbC0tCyOMOkrpu0z+vjxYyxbtgxOTk7ClZycjE6dOglvWSIqbNo+pw0bNkRCQoLSdsSEhATUrl27SGOlr5O2z6hEIkH9+vUVysqWLYtKlSohJSWlSGMl0hR/PxUPJriICAAwevRozJo1CxKJBACwc+dOvH79Gi1atFBqW716dbi4uGDhwoUAgOzsbEyZMgWjRo0qzpDpK6TNc/rs2TN8++23WLVqFZycnIo3UPpqafOMnjp1BEzmFgAADUhJREFUClevXsWVK1eEq1KlSjhw4IDw7ytRUdDmOS1btixatWqFadOmQS6XAwCWL18OMzMzNGrUqDjDpq+INs9o3759sX79ely5ckUo27RpE3R0dPiMUonB30/FQ+9TB0BEJUPXrl2RlJSEpk2bQkdHB5aWlti/fz90dHSQnZ0NPz8/rFmzRlgFs2nTJgwbNgwNGjSATCZD586dMWHChE98F/Sl0+Y5/fXXX/Hw4UPMnDkTM2fOFMZo8v/au/+YqOs/DuBPPDk40DlCh+AMJJj8OOQ8oCATpQJdOofRMOmHFGTlz9jMcv4okKWErJgFlZQFmeUf0khJc64kszkwzkiWQYGlUwjxB8LRHdzr+4fjMz7gjzvSL157PjY23597/3h93seO8/V5f96f2Fi89957w3gW9F/m6GfpQK6urhg5kl/P6PZy9Pe0qKgIS5cuRWBgIDQaDYxGI3bt2jXMZ0H/ZY78jkZHR6OkpARZWVm4cOECRATh4eHYs2cPP0/p/0ar1aq2wuD/n4aHi/RdiiEiIiIiIiIiInJCvEWRiIiIiIiIiIicGhNcRERERERERETk1JjgIiIiIiIiIiIip8YEFxEREREREREROTUmuIiIiIiIiIiIyKkxwUVERERERERERP/aRx99BDc3NzQ3N9vdxmq1Ijs7G5GRkQgLC0NCQgJOnjzp8NgjHW5BRERERERERETUz/r161FTUwMvLy/09PTY3W7dunU4deoUjh49Cnd3d1RVVSElJQW1tbVwdXW1ux+u4CIiIiIihyxevBh33303DAYDYmJi8OCDD+Lo0aPDHdZtsXLlSlRXV9+2/hcvXozS0tLb1j8REdH/g81mg6+vL/bs2QN3d3e724kIiouLUVRUpLSLj4/HAw88gP379zsUAxNcREREROQQi8WCDRs2wGQyobq6Gu+++y5SU1PR2to63KGpbN68GSJid/0zZ84MSjYVFhYiJibmVoemsFgssFgst63/obpy5Qq2bt063GEQEZGTGDFiBJYsWQKNRnPN1z///HOEh4cjLCwM06ZNQ21tLQCgpaUFWq0Wd911l6p+eHi4wxeYmOAiIiIion8lNDQUCQkJqKysHO5QVNasWYPe3l676zc0NOCDDz64jRE5j7a2NuTl5Q13GERE9B9QU1ODLVu24LvvvkN9fT3y8/ORmpoKq9WKMWPG4MqVK7h48aKqTWNjI1paWhwahwkuIiIiIvrXJkyYgNOnTwO4/lVaAEhKSkJFRQVmzpyJqVOnwmKx4KmnnsLbb7+N2NhY6PV6JCYm4ty5cygsLER4eDgiIiKQm5urGs/T01OVvOrt7YWnpycAoKysDAaDAQAQHR2NFStWAACampowa9YshIWFYcqUKZg9e7YS88qVK5GZmQmTyQSDwaCs5EpKSkJVVZUyzjfffIPo6GhMnjwZQUFBWLdunSqOSZMmobS0FHq9Hnq9HnFxcairq7NrDj/88EMsW7YMKSkp0Ov1iIqKwuHDh3Hw4EFERUUhMjISaWlp6OzsVNpERUWhvLwcsbGxiIiIgNFoxJEjR1T93izmkJAQHDx4EHFxcXj44YeRl5eHRx55BK2trTAYDNi8eTMA4NixY4iPj1fObcGCBbh06ZIy/xMnTkRhYSFCQ0OV9/Gvv/5SxVJZWQmDwYCQkBDo9Xp8+eWXAIDz588jNTUVwcHBmDx5MtasWQObzWbXvBER0Z3trbfeQnZ2NsaNGwcAuP/++xEYGIgjR45Ap9Nh4cKFeOmll9DV1QURwb59+7B7927H/w4IEREREZEDFi1aJNu2bVMdS0tLk7KyMqmurpaoqChpbW0VEZEffvhBgoKCxGKxiIjIjBkzJCEhQdrb21X9BQYGypkzZ0RE5LPPPpPo6GhJT0+Xnp4esVqtMnv2bKmqqlLaABCr1aqUrVarDPxqO7DOH3/8ISdOnFDKubm58sQTTyjlb7/9VqZNm6bqY8aMGXLgwAERETl+/Lj4+flJTU2NiIh0dnbK/PnzZc2aNUp9f39/SUxMlMuXL4uIyM6dO8VgMNg1l9u3bxetViuHDh0SEZHff/9dwsPDJSEhQc6fPy8iIhs3bpScnBylfXBwsDz00EPS0dEhIiJfffWVjB8/Xsxms0MxL1iwQGkjItLU1CQTJkxQxfrzzz9Lc3OziIjYbDbJzMyUtWvXKq9rNBpJS0uTf/75R0RENm3aJMnJycrru3fvFqPRqPTR39y5c6WkpERERCwWiyQnJ8v7779/3XkjIqI7l7+/vzQ0NChlg8EgISEhEhkZqfwEBARIeXm5iIh0d3fLa6+9JkajUfR6vbzwwguyefNmeeWVVxwalyu4iIiIiGjIbDYbdu3ahdraWjz22GM3vErbZ9asWfDy8lL188wzz8DPzw8AkJycjJ9++gkFBQXQaDQYOXIk5s2bN2hlkqMmTZqEsLAwpdw3jr22bNmCVatWISoqCgDg4eGB4uJiFBcXo6urS6n36quvYvTo0QCAxx9/HL/++is6OjrsGmP69OmIj48HAAQGBsLT0xMrVqxQ9iZJSUlRzYPFYkF2djZGjRoFAJg7dy4iIiKwd+9eh2KeP3/+TTcFjoiIgL+/PwDAxcVl0Pz19vZi48aN0Gq1AID09HTV6rfVq1dj27ZtSh99GhoacO7cOWRkZAAAXF1dsXr1auzcudOuOSMiojub2WzGJ598ApPJpPw0NTUhOTkZAODm5obXX38dx44dQ11dHYqLi/Hnn39Cr9c7NM7I2xA7EREREf3H5eTkYOvWrXBxcYHRaMSBAwfg7u6O+vp6rFq1CmvXrlXqXrp0CRcuXFDKoaGhg/obP3688m+dTgcfHx/VhrM6nc7hvTgG6u7uRmFhIb7++mu0tLRARNDd3W13+7q6Oixfvlx1zMfHB35+fmhsbMSUKVMAABMnTlTV8fb2Rnt7u5L0upH+8wBcPe/+STmdTgez2ayq03c7Zp+IiAg0NTU5FPO13pOBLly4oOyh0t7eDovFMuhc+5fHjh2L9vZ2AMDff/+Ns2fPwmg0Duq3vr4ejY2NqvPo7e3FmDFjbhoTERHd+YKDg1FdXY17773XrvodHR2oqKjAxo0bHRqHCS4iIiIictiGDRuQmZk56LjZbEZpaekNv8R6eHjctP++VUD26r8a6Xqee+45mM1mFBUVITQ0FPX19ZgzZ47dY1zvyVAionrNxcXlmnWG6mZzYbVaVeWuri7odDoA9sdsz3syb948TJkyBWVlZQgMDMTevXuRn5+vqnOtc+9js9kgIoPqmM1mxMXF3XEPKSAiolsjIyMDy5YtQ0JCgnLRprm5GQEBAQCuXtTo+5t05swZpKenIysra9CTFW+GCS4iIiIiumUcvUo7VGPGjEFbW5uy4qn/RvZ9RoxQ78ZRXl6OU6dOwdvbGwBw4sQJ1evXSwb1MRqN+P777xETE6Mca2lpQWtrK4KCgoZ0HreCyWTCzJkzlfKxY8eQmpoKYOgxD5yLtrY21NXV4dChQ8q8Dpy/Gxk3bhz8/Pxw+PBhTJ8+XfVacHAwTCYTrFYrXF1d7e6TiIjuTFqtVvV5npycDLPZjLS0NPT09MDV1RWJiYl48803AQBvvPEGKioqYLVaodPpsHTpUjz55JMOj8s9uIiIiIjolsnIyMCmTZtQX1+vHGtubr7l49x33334+OOPAQCdnZ3Iz88ftArJ29sbp06dUsq+vr44fvw4AODs2bMoKioaVP/06dOqJwz2t3LlShQUFKCmpgbA1ZVSzz//PJYsWQI3N7dbdWoOy8nJweXLlwFcfYKk2WxWEl5DjdnLywsXL17ElStXAACjR4+Gi4sLGhsbAQAnT55EWVmZw3G++OKLqvcEuJqE8/HxUT05ceBtrURE5Dx+++23QfstLly4ECaTCb/88gtqa2uV5BYArF+/HtXV1TCZTPjxxx+HlNwCuIKLiIiIiBw08Mpsfze7Suvm5jYosXKtY56enqqyq6ur6la9d955B88++yy++OILaDQa5ObmKsmrPi+//DKSkpLg6+uLffv2YceOHVi2bBksFgs8PDyQl5eHRYsWKfVDQ0MRExMDvV6P2NhYbN++HVqtVhk3PDwcO3bswNKlS3Hx4kXYbDY8/fTTqv3G3NzcBt1S6Obmdt356t//teZh4FwPnAcAWL58OaZPn47Ozk74+vqisrJSuQ1wqDGPGjUKmZmZmDp1Ku655x7s27cPn376KVJTUyEiGDt2LAoKClT7o3h4eKhuP3RxcVElHdPS0qDRaDBnzhxoNBrYbDZkZ2fj0Ucfxf79+5GVlYWwsDDodDq4u7ujpKRk0MMIiIiIrsdF/s2GAERERERENGwCAgJuywo5IiIiZ8NbFImIiIiInJS7u/twh0BERHRH4AouIiIiIiIiIiJyalzBRURERERERERETo0JLiIiIiIiIiIicmpMcBERERERERERkVNjgouIiIiIiIiIiJwaE1xEREREREREROTUmOAiIiIiIiIiIiKnxgQXERERERERERE5NSa4iIiIiIiIiIjIqf0PV1QU2W4spacAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"DataFrame columns:\", feature_importances.columns)\n",
    "\n",
    "# 중요도 순으로 정렬\n",
    "importance_df = feature_importances.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Permutation Feature Importances:\")\n",
    "print(importance_df)\n",
    "\n",
    "# DataFrame을 CSV 파일로 저장\n",
    "importance_df.to_csv('feature_importances.csv', index=False)\n",
    "print(\"Feature importances saved to 'feature_importances.csv'\")\n",
    "\n",
    "# 결과 시각화\n",
    "print(\"Visualizing feature importances...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importances['feature'], feature_importances['importance'])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance DataFrame:\n",
      "                   feature    importance\n",
      "0                     계약년월  1.045655e+09\n",
      "1                     전용면적  9.756646e+08\n",
      "2                       급지  6.113884e+08\n",
      "3                     건축년도  1.695819e+08\n",
      "4                      좌표Y  1.136631e+08\n",
      "5                      좌표X  6.300580e+07\n",
      "6                  address  5.447845e+07\n",
      "7                        구  2.346532e+07\n",
      "8                 대장아파트_거리  2.164844e+07\n",
      "9                        층  1.891177e+07\n",
      "10                   k_연면적  1.570995e+07\n",
      "11                  k_전체동수  1.222237e+07\n",
      "12                       동  1.209701e+07\n",
      "13                    주차대수  1.160945e+07\n",
      "14      k_단지분류_아파트_주상복합등등_  1.124049e+07\n",
      "15                  k_난방방식  1.095110e+07\n",
      "16                k_주거전용면적  1.078178e+07\n",
      "17          distance_score  8.790410e+06\n",
      "18                     도로명  7.312892e+06\n",
      "19                    아파트명  6.965256e+06\n",
      "20                     이자율  5.912753e+06\n",
      "21                 k_전체세대수  5.691434e+06\n",
      "22                   k_시행사  5.517553e+06\n",
      "23  k_전용면적별세대현황_60__85_이하_  3.954117e+06\n",
      "24            k_85__135_이하  3.815404e+06\n",
      "25              k_건설사_시공사_  3.456251e+06\n",
      "26                  k_복도유형  3.350095e+06\n",
      "27            k_세대타입_분양형태_  3.336379e+06\n",
      "28      k_전용면적별세대현황_60_이하_  3.019038e+06\n",
      "29                    건축면적  1.852536e+06\n",
      "30     기타_의무_임대_임의_1_2_3_4  1.585483e+06\n",
      "31                is_top20  1.393460e+06\n",
      "32                     계약월  1.083496e+06\n",
      "33                     계약일  9.880150e+05\n",
      "34                k_135_초과  4.552280e+05\n",
      "35                  k_관리방식  2.713488e+05\n",
      "36                    계약연도  1.213199e+05\n",
      "37                    신축여부  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 불러오기\n",
    "importance_df = pd.read_csv('feature_importances.csv')\n",
    "\n",
    "# 중요도 순으로 정렬\n",
    "feature_importance_df = importance_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Feature Importance DataFrame:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Select & LightGBM Feature Optimization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 특성 중요도에 따라 정렬된 특성 리스트 (이전 코드에서 얻은 결과)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sorted_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_importance_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# RMSE 값을 저장할 리스트\u001b[39;00m\n\u001b[1;32m      8\u001b[0m rmse_scores \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_importance_df' is not defined"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "# 특성 중요도에 따라 정렬된 특성 리스트 (이전 코드에서 얻은 결과)\n",
    "sorted_features = feature_importance_df['feature'].tolist()\n",
    "\n",
    "# RMSE 값을 저장할 리스트\n",
    "rmse_scores = []\n",
    "\n",
    "# 특성 개수 범위 설정\n",
    "feature_counts = range(9, 31, 3)\n",
    "\n",
    "for feature_count in feature_counts:\n",
    "    print(f\"Training with top {feature_count} features\")\n",
    "    # 상위 n개의 특성 선택\n",
    "    selected_features = sorted_features[:feature_count]\n",
    "    X_selected = X[selected_features]\n",
    "    \n",
    "    # 데이터 분할\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # wandb 초기화\n",
    "    wandb.init(\n",
    "        project=\"re_price_prediction\",\n",
    "        group=\"features_final\",\n",
    "        name=f\"top_{feature_count}_features\",\n",
    "        reinit=True,\n",
    "        config={\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'n_estimators': 10000,\n",
    "            'learning_rate': 0.1,\n",
    "            # 'num_leaves': 107,\n",
    "            # 'max_depth': 10,\n",
    "            # 'min_child_samples': 8,\n",
    "            # 'subsample': 0.5668227534307944,\n",
    "            # 'colsample_bytree': 0.6033862727269486,\n",
    "            # 'reg_alpha': 2.5906244698002496e-06,\n",
    "            # 'reg_lambda': 2.979573198070953e-05,\n",
    "            'force_col_wise': True,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 파라미터 정의\n",
    "    params = wandb.config\n",
    "    \n",
    "    # LGBMRegressor 모델 생성\n",
    "    gbm = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    # Callback function to log evaluation metrics to wandb\n",
    "    def log_to_wandb(env):\n",
    "        for data_name, eval_name, eval_result, _ in env.evaluation_result_list:\n",
    "            metric_name = f\"{data_name}_{eval_name}\"\n",
    "            wandb.log({metric_name: eval_result})\n",
    "    \n",
    "    # 모델 학습\n",
    "    gbm.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "            eval_metric='rmse',\n",
    "            categorical_feature=\"auto\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                       lgb.log_evaluation(period=10, show_stdv=True),\n",
    "                       log_to_wandb])\n",
    "    \n",
    "    # 검증 세트에 대한 RMSE 계산\n",
    "    y_pred = gbm.predict(X_val)\n",
    "    rmse = np.sqrt(np.mean((y_val - y_pred)**2))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    print(f\"RMSE with {feature_count} features: {rmse}\")\n",
    "    \n",
    "    # wandb run 종료\n",
    "    wandb.finish()\n",
    "\n",
    "# 최적의 특성 개수 찾기\n",
    "best_feature_count = feature_counts[np.argmin(rmse_scores)]\n",
    "best_rmse = min(rmse_scores)\n",
    "\n",
    "print(f\"Best number of features: {best_feature_count}\")\n",
    "print(f\"Best RMSE: {best_rmse}\")\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(feature_counts, rmse_scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Number of Features')\n",
    "plt.annotate(f'Best: {best_feature_count} features\\nRMSE: {best_rmse:.4f}', \n",
    "             xy=(best_feature_count, best_rmse), \n",
    "             xytext=(5, 5), \n",
    "             textcoords='offset points')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 최적의 특성 리스트 출력\n",
    "best_features = sorted_features[:best_feature_count]\n",
    "print(\"Best features:\")\n",
    "for i, feature in enumerate(best_features, 1):\n",
    "    print(f\"{i}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best features 자동/수동 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance DataFrame:\n",
      "                   feature    importance\n",
      "0                     계약년월  1.045655e+09\n",
      "1                     전용면적  9.756646e+08\n",
      "2                       급지  6.113884e+08\n",
      "3                     건축년도  1.695819e+08\n",
      "4                      좌표Y  1.136631e+08\n",
      "5                      좌표X  6.300580e+07\n",
      "6                  address  5.447845e+07\n",
      "7                        구  2.346532e+07\n",
      "8                 대장아파트_거리  2.164844e+07\n",
      "9                        층  1.891177e+07\n",
      "10                   k_연면적  1.570995e+07\n",
      "11                  k_전체동수  1.222237e+07\n",
      "12                       동  1.209701e+07\n",
      "13                    주차대수  1.160945e+07\n",
      "14      k_단지분류_아파트_주상복합등등_  1.124049e+07\n",
      "15                  k_난방방식  1.095110e+07\n",
      "16                k_주거전용면적  1.078178e+07\n",
      "17          distance_score  8.790410e+06\n",
      "18                     도로명  7.312892e+06\n",
      "19                    아파트명  6.965256e+06\n",
      "20                     이자율  5.912753e+06\n",
      "21                 k_전체세대수  5.691434e+06\n",
      "22                   k_시행사  5.517553e+06\n",
      "23  k_전용면적별세대현황_60__85_이하_  3.954117e+06\n",
      "24            k_85__135_이하  3.815404e+06\n",
      "25              k_건설사_시공사_  3.456251e+06\n",
      "26                  k_복도유형  3.350095e+06\n",
      "27            k_세대타입_분양형태_  3.336379e+06\n",
      "28      k_전용면적별세대현황_60_이하_  3.019038e+06\n",
      "29                    건축면적  1.852536e+06\n",
      "30     기타_의무_임대_임의_1_2_3_4  1.585483e+06\n",
      "31                is_top20  1.393460e+06\n",
      "32                     계약월  1.083496e+06\n",
      "33                     계약일  9.880150e+05\n",
      "34                k_135_초과  4.552280e+05\n",
      "35                  k_관리방식  2.713488e+05\n",
      "36                    계약연도  1.213199e+05\n",
      "37                    신축여부  0.000000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['계약년월',\n",
       " '전용면적',\n",
       " '급지',\n",
       " '건축년도',\n",
       " '좌표Y',\n",
       " '좌표X',\n",
       " 'address',\n",
       " '구',\n",
       " '대장아파트_거리',\n",
       " '층',\n",
       " 'k_연면적',\n",
       " 'k_전체동수',\n",
       " '동',\n",
       " '주차대수',\n",
       " 'k_단지분류_아파트_주상복합등등_',\n",
       " 'k_난방방식',\n",
       " 'k_주거전용면적',\n",
       " 'distance_score']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CSV 파일 불러오기\n",
    "importance_df = pd.read_csv('feature_importances.csv')\n",
    "\n",
    "# 중요도 순으로 정렬\n",
    "feature_importance_df = importance_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Feature Importance DataFrame:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "sorted_features = feature_importance_df['feature'].tolist()\n",
    "\n",
    "#display(best_feature_count)\n",
    "best_feature_count = 18\n",
    "best_features = sorted_features[:best_feature_count]\n",
    "display(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 특성 리스트 생성\n",
    "X_final = X[best_features]\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(X_final, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"re_price_prediction\",\n",
    "    group=\"lightBGM\",\n",
    "    name=\"final_model\",\n",
    "    reinit=True,\n",
    "    config=params\n",
    ")\n",
    "\n",
    "final_model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "final_model.fit(X_train_final, y_train_final,\n",
    "                eval_set=[(X_train_final, y_train_final), (X_val_final, y_val_final)],\n",
    "                eval_metric='rmse',\n",
    "                categorical_feature=\"auto\",\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                           lgb.log_evaluation(period=10, show_stdv=True),\n",
    "                           log_to_wandb])\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# 모델 저장\n",
    "import joblib\n",
    "joblib.dump(final_model, 'final_model.joblib')\n",
    "print(\"Final model saved as 'final_model.joblib'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test data...\n",
      "Execution time: 11.49 seconds\n",
      "Predictions:\n",
      "[176361.47002299 285797.6445489  315441.53519327 ...  81097.01363005\n",
      "  69827.85753275  71823.36769997]\n",
      "Predictions saved to 'output.csv'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 실제 테스트 데이터에 대한 예측\n",
    "print(\"Predicting on test data...\")\n",
    "start_time = time.time()\n",
    "X_test = test_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "# 테스트 데이터에서 최적의 특성만 선택\n",
    "X_test_selected = X_test[best_features]\n",
    "\n",
    "# 최적의 반복 횟수로 X_test에 대한 예측 수행\n",
    "real_test_pred = final_model.predict(X_test_selected, num_iteration=final_model.best_iteration_)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(real_test_pred)\n",
    "\n",
    "# 예측 결과를 DataFrame으로 변환하고 CSV 파일로 저장\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)\n",
    "print(\"Predictions saved to 'output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['계약년월',\n",
       " '전용면적',\n",
       " '급지',\n",
       " '건축년도',\n",
       " '좌표Y',\n",
       " '좌표X',\n",
       " 'address',\n",
       " '구',\n",
       " '대장아파트_거리',\n",
       " '층',\n",
       " 'k_연면적',\n",
       " 'k_전체동수',\n",
       " '동',\n",
       " '주차대수',\n",
       " 'k_단지분류_아파트_주상복합등등_',\n",
       " 'k_난방방식',\n",
       " 'k_주거전용면적',\n",
       " 'distance_score',\n",
       " '도로명',\n",
       " '아파트명',\n",
       " '이자율']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CSV 파일 불러오기\n",
    "importance_df = pd.read_csv('feature_importances.csv')\n",
    "\n",
    "# 중요도 순으로 정렬\n",
    "feature_importance_df = importance_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "sorted_features = feature_importance_df['feature'].tolist()\n",
    "\n",
    "#display(best_feature_count)\n",
    "best_feature_count = 21\n",
    "best_features = sorted_features[:best_feature_count]\n",
    "display(best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM + Sweep : 하이퍼파라미터 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "X_final = X[best_features]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_final, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "# 새로운 W&B 백엔드 사용\n",
    "wandb.require(\"core\")\n",
    "\n",
    "def objective():\n",
    "    with wandb.init(project=\"re_price_prediction\", group=\"lightGBM sweep 3\", job_type=\"optimize\"):\n",
    "        config = wandb.config\n",
    "        \n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'n_estimators': config.n_estimators,\n",
    "            'num_leaves': config.num_leaves,\n",
    "            'max_depth': config.max_depth,\n",
    "            'learning_rate': config.learning_rate,\n",
    "            'min_child_samples': config.min_child_samples,\n",
    "            'subsample': config.subsample,\n",
    "            'colsample_bytree': config.colsample_bytree,\n",
    "            'reg_alpha': config.reg_alpha,\n",
    "            'reg_lambda': config.reg_lambda,\n",
    "            'force_col_wise': True,\n",
    "        }\n",
    "        \n",
    "        gbm = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        gbm.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "            eval_metric='rmse',\n",
    "            categorical_feature=\"auto\",\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(period=10),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        val_preds = gbm.predict(X_val)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        \n",
    "        wandb.log({\"val_rmse\": val_rmse})\n",
    "        \n",
    "        return val_rmse\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_rmse',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'n_estimators': {'distribution': 'int_uniform', 'min': 19000, 'max': 20000},\n",
    "        'num_leaves': {'distribution': 'int_uniform', 'min': 96, 'max': 128},\n",
    "        'max_depth': {'distribution': 'int_uniform', 'min': 9, 'max': 10},\n",
    "        'learning_rate': {'distribution': 'uniform', 'min': 0.01, 'max': 0.02},\n",
    "        'min_child_samples': {'distribution': 'int_uniform', 'min': 5, 'max': 100},\n",
    "        'subsample': {'distribution': 'uniform', 'min': 0.5, 'max': 1.0},\n",
    "        'colsample_bytree': {'distribution': 'uniform', 'min': 0.5, 'max': 1.0},\n",
    "        'reg_alpha': {'distribution': 'uniform', 'min': 0, 'max': 1},\n",
    "        'reg_lambda': {'distribution': 'uniform', 'min': 0, 'max': 1},\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"re_price_prediction\")\n",
    "\n",
    "# Sweep 실행\n",
    "wandb.agent(sweep_id, function=objective, count=37)\n",
    "\n",
    "# Sweep이 완료될 때까지 대기\n",
    "print(\"Waiting for sweep to complete...\")\n",
    "time.sleep(60)  # 60초 대기, 필요에 따라 조정\n",
    "\n",
    "# 최적의 하이퍼파라미터로 최종 모델 학습\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"ml9_regression/re_price_prediction/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "best_config = best_run.config\n",
    "\n",
    "with wandb.init(project=\"re_price_prediction\", config=best_config, group=\"lightGBM sweep\", job_type=\"train\"):\n",
    "    config = wandb.config\n",
    "    \n",
    "    final_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': config.n_estimators,\n",
    "        'num_leaves': config.num_leaves,\n",
    "        'max_depth': config.max_depth,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'min_child_samples': config.min_child_samples,\n",
    "        'subsample': config.subsample,\n",
    "        'colsample_bytree': config.colsample_bytree,\n",
    "        'reg_alpha': config.reg_alpha,\n",
    "        'reg_lambda': config.reg_lambda,\n",
    "        'force_col_wise': True,\n",
    "    }\n",
    "    \n",
    "    final_model = lgb.LGBMRegressor(**final_params)\n",
    "    \n",
    "    final_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        categorical_feature=\"auto\",\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=10),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    final_val_preds = final_model.predict(X_val)\n",
    "    final_val_rmse = np.sqrt(mean_squared_error(y_val, final_val_preds))\n",
    "    wandb.log({\"final_val_rmse\": final_val_rmse})\n",
    "    \n",
    "    print(f\"Final Validation RMSE: {final_val_rmse}\")\n",
    "\n",
    "    # 테스트 세트에 대한 예측 및 RMSE 계산 \n",
    "    #     test_preds = final_model.predict(X_test)\n",
    "    #     test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "    #     wandb.log({\"test_rmse\": test_rmse})\n",
    "    #     print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple lightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data type minizatoin\n",
    "- 데이터 컬럼 확인할것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 최적화\n",
    "# for df in [train_data_encoded, test_data_encoded]:\n",
    "#     df['아파트명'] = df['아파트명'].astype('int16')\n",
    "#     df['전용면적'] = df['전용면적'].astype('float32')\n",
    "#     df['층'] = df['층'].astype('int8')\n",
    "#     df['건축년도'] = df['건축년도'].astype('int16')\n",
    "#     df['도로명'] = df['도로명'].astype('int32')\n",
    "#     df['k-복도유형'] = df['k-복도유형'].astype('int8')\n",
    "#     df['k-난방방식'] = df['k-난방방식'].astype('int8')\n",
    "#     df['k-전체세대수'] = df['k-전체세대수'].astype('float32')\n",
    "#     df['주차대수'] = df['주차대수'].astype('float32')\n",
    "#     df['기타/의무/임대/임의=1/2/3/4'] = df['기타/의무/임대/임의=1/2/3/4'].astype('int8')\n",
    "#     df['좌표X'] = df['좌표X'].astype('float32')\n",
    "#     df['좌표Y'] = df['좌표Y'].astype('float32')\n",
    "#     df['target'] = df['target'].astype('float32')\n",
    "#     df['계약연도'] = df['계약연도'].astype('int16')\n",
    "#     df['계약월'] = df['계약월'].astype('int8')\n",
    "#     df['구'] = df['구'].astype('int8')\n",
    "#     df['동'] = df['동'].astype('int16')\n",
    "#     df['distance_score'] = df['distance_score'].astype('int8')\n",
    "#     df['급지'] = df['급지'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb #wandb login..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "X_final = X[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['계약년월', '전용면적', '급지', '건축년도', '좌표Y', '구', '좌표X', '층', 'k_연면적',\n",
       "       'distance_score', 'k_난방방식', 'k_단지분류_아파트_주상복합등등_', '동', '주차대수', '도로명'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "run_name = f\"lightGBM_final_{datetime.now().strftime('%m%d_%H%M')}\"\n",
    "\n",
    "# Start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    project=\"re_price_prediction\",\n",
    "    group=\"lightGBM final\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': 100000,\n",
    "        'learning_rate': 0.01,  # 최적화된 learning_rate 사용\n",
    "        'num_leaves': 108,        # 최적화된 num_leaves 사용\n",
    "        'max_depth': 10,          # 최적화된 max_depth 사용\n",
    "        #'min_child_samples': 8,\n",
    "        #'subsample': 0.5668227534307944,\n",
    "        #'colsample_bytree': 0.6033862727269486,\n",
    "        #'reg_alpha': 2.5906244698002496e-06,\n",
    "        #'reg_lambda': 2.979573198070953e-05,\n",
    "        #'force_col_wise': True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 파라미터 정의\n",
    "params = wandb.config\n",
    "\n",
    "# Define the LightGBM regressor\n",
    "gbm = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    objective='regression',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=100000,\n",
    "    metric='rmse',\n",
    ")\n",
    "\n",
    "# Callback function to log evaluation metrics to wandb\n",
    "def log_to_wandb(env):\n",
    "    for data_name, eval_name, eval_result, stdv in env.evaluation_result_list:\n",
    "        metric_name = f\"{data_name}_{eval_name}\"\n",
    "        wandb.log({metric_name: eval_result})\n",
    "\n",
    "# Fit the model\n",
    "gbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[[X_train, y_train], [X_val, y_val]],\n",
    "    eval_metric='rmse',\n",
    "    categorical_feature=\"auto\",\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=10, show_stdv=True),\n",
    "        log_to_wandb\n",
    "    ]\n",
    ")\n",
    "\n",
    "# [optional] finish the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[191522.14239343 275602.71543252 327149.71025315 ...  80560.99953826\n",
      "  68044.8291902   68190.79535061]\n",
      "CPU times: user 9.34 s, sys: 172 ms, total: 9.51 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = test_data_encoded.drop(['target'], axis=1)\n",
    "# 최적의 반복 횟수로 X_test에 대한 예측 수행\n",
    "real_test_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "print(real_test_pred)\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM + final parmameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['계약년월',\n",
       " '전용면적',\n",
       " '급지',\n",
       " '건축년도',\n",
       " '좌표Y',\n",
       " '좌표X',\n",
       " 'address',\n",
       " '구',\n",
       " '대장아파트_거리',\n",
       " '층',\n",
       " 'k_연면적',\n",
       " 'k_전체동수',\n",
       " '동',\n",
       " '주차대수',\n",
       " 'k_단지분류_아파트_주상복합등등_',\n",
       " 'k_난방방식',\n",
       " 'k_주거전용면적',\n",
       " 'distance_score',\n",
       " '도로명',\n",
       " '아파트명',\n",
       " '이자율']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CSV 파일 불러오기\n",
    "importance_df = pd.read_csv('feature_importances.csv')\n",
    "\n",
    "# 중요도 순으로 정렬\n",
    "feature_importance_df = importance_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "sorted_features = feature_importance_df['feature'].tolist()\n",
    "\n",
    "#display(best_feature_count)\n",
    "best_feature_count = 21\n",
    "best_features = sorted_features[:best_feature_count]\n",
    "display(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "X_final = X[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['계약년월', '전용면적', '급지', '건축년도', '좌표Y', '좌표X', 'address', '구', '대장아파트_거리',\n",
       "       '층', 'k_연면적', 'k_전체동수', '동', '주차대수', 'k_단지분류_아파트_주상복합등등_', 'k_난방방식',\n",
       "       'k_주거전용면적', 'distance_score', '도로명', '아파트명', '이자율'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaanmego\u001b[0m (\u001b[33mml9_regression\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/upstage-ml-regression-ml9/wandb/run-20240718_214633-bpwhdjob</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml9_regression/re_price_prediction/runs/bpwhdjob' target=\"_blank\">lightGBM_final_0718_2146</a></strong> to <a href='https://wandb.ai/ml9_regression/re_price_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml9_regression/re_price_prediction' target=\"_blank\">https://wandb.ai/ml9_regression/re_price_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml9_regression/re_price_prediction/runs/bpwhdjob' target=\"_blank\">https://wandb.ai/ml9_regression/re_price_prediction/runs/bpwhdjob</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on full dataset...\n",
      "[LightGBM] [Info] Total Bins 3504\n",
      "[LightGBM] [Info] Number of data points in the train set: 1118822, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 57991.532149\n",
      "[50]\ttraining's rmse: 12272.3\n",
      "[100]\ttraining's rmse: 10138.9\n",
      "[150]\ttraining's rmse: 9228.9\n",
      "[200]\ttraining's rmse: 8641.78\n",
      "[250]\ttraining's rmse: 8194.92\n",
      "[300]\ttraining's rmse: 7874.87\n",
      "[350]\ttraining's rmse: 7621.31\n",
      "[400]\ttraining's rmse: 7379.78\n",
      "[450]\ttraining's rmse: 7187.15\n",
      "[500]\ttraining's rmse: 7018.09\n",
      "[550]\ttraining's rmse: 6862.44\n",
      "[600]\ttraining's rmse: 6735.12\n",
      "[650]\ttraining's rmse: 6608.43\n",
      "[700]\ttraining's rmse: 6501.39\n",
      "[750]\ttraining's rmse: 6408.79\n",
      "[800]\ttraining's rmse: 6325.92\n",
      "[850]\ttraining's rmse: 6242.29\n",
      "[900]\ttraining's rmse: 6169.58\n",
      "[950]\ttraining's rmse: 6101.21\n",
      "[1000]\ttraining's rmse: 6035.22\n",
      "[1050]\ttraining's rmse: 5972.69\n",
      "[1100]\ttraining's rmse: 5914.4\n",
      "[1150]\ttraining's rmse: 5857.47\n",
      "[1200]\ttraining's rmse: 5801.24\n",
      "[1250]\ttraining's rmse: 5745.98\n",
      "[1300]\ttraining's rmse: 5695.14\n",
      "[1350]\ttraining's rmse: 5644.75\n",
      "[1400]\ttraining's rmse: 5599.86\n",
      "[1450]\ttraining's rmse: 5558.26\n",
      "[1500]\ttraining's rmse: 5522.73\n",
      "[1550]\ttraining's rmse: 5485.28\n",
      "[1600]\ttraining's rmse: 5442.99\n",
      "[1650]\ttraining's rmse: 5406.07\n",
      "[1700]\ttraining's rmse: 5367.82\n",
      "[1750]\ttraining's rmse: 5334.21\n",
      "[1800]\ttraining's rmse: 5305.65\n",
      "[1850]\ttraining's rmse: 5276.14\n",
      "[1900]\ttraining's rmse: 5241.58\n",
      "[1950]\ttraining's rmse: 5213.26\n",
      "[2000]\ttraining's rmse: 5183.97\n",
      "[2050]\ttraining's rmse: 5156.61\n",
      "[2100]\ttraining's rmse: 5129.55\n",
      "[2150]\ttraining's rmse: 5102.54\n",
      "[2200]\ttraining's rmse: 5075.24\n",
      "[2250]\ttraining's rmse: 5051.35\n",
      "[2300]\ttraining's rmse: 5028.27\n",
      "[2350]\ttraining's rmse: 5005.9\n",
      "[2400]\ttraining's rmse: 4979.74\n",
      "[2450]\ttraining's rmse: 4957.84\n",
      "[2500]\ttraining's rmse: 4937.27\n",
      "[2550]\ttraining's rmse: 4916.21\n",
      "[2600]\ttraining's rmse: 4893.6\n",
      "[2650]\ttraining's rmse: 4873.58\n",
      "[2700]\ttraining's rmse: 4852.77\n",
      "[2750]\ttraining's rmse: 4835.61\n",
      "[2800]\ttraining's rmse: 4816.44\n",
      "[2850]\ttraining's rmse: 4792.15\n",
      "[2900]\ttraining's rmse: 4773.69\n",
      "[2950]\ttraining's rmse: 4755.54\n",
      "[3000]\ttraining's rmse: 4739.57\n",
      "[3050]\ttraining's rmse: 4721.9\n",
      "[3100]\ttraining's rmse: 4704.84\n",
      "[3150]\ttraining's rmse: 4691.93\n",
      "[3200]\ttraining's rmse: 4675.41\n",
      "[3250]\ttraining's rmse: 4658.63\n",
      "[3300]\ttraining's rmse: 4643.56\n",
      "[3350]\ttraining's rmse: 4625.86\n",
      "[3400]\ttraining's rmse: 4610.46\n",
      "[3450]\ttraining's rmse: 4592.61\n",
      "[3500]\ttraining's rmse: 4576.35\n",
      "[3550]\ttraining's rmse: 4563.23\n",
      "[3600]\ttraining's rmse: 4548.47\n",
      "[3650]\ttraining's rmse: 4535.74\n",
      "[3700]\ttraining's rmse: 4521.35\n",
      "[3750]\ttraining's rmse: 4508\n",
      "[3800]\ttraining's rmse: 4495.21\n",
      "[3850]\ttraining's rmse: 4483.21\n",
      "[3900]\ttraining's rmse: 4470.09\n",
      "[3950]\ttraining's rmse: 4458.1\n",
      "[4000]\ttraining's rmse: 4446.48\n",
      "[4050]\ttraining's rmse: 4434.18\n",
      "[4100]\ttraining's rmse: 4423.34\n",
      "[4150]\ttraining's rmse: 4411.32\n",
      "[4200]\ttraining's rmse: 4398.52\n",
      "[4250]\ttraining's rmse: 4385.7\n",
      "[4300]\ttraining's rmse: 4373.29\n",
      "[4350]\ttraining's rmse: 4361.16\n",
      "[4400]\ttraining's rmse: 4349.91\n",
      "[4450]\ttraining's rmse: 4338.58\n",
      "[4500]\ttraining's rmse: 4328.74\n",
      "[4550]\ttraining's rmse: 4318.29\n",
      "[4600]\ttraining's rmse: 4308.14\n",
      "[4650]\ttraining's rmse: 4294.62\n",
      "[4700]\ttraining's rmse: 4283.84\n",
      "[4750]\ttraining's rmse: 4274.62\n",
      "[4800]\ttraining's rmse: 4262.8\n",
      "[4850]\ttraining's rmse: 4251.72\n",
      "[4900]\ttraining's rmse: 4242.94\n",
      "[4950]\ttraining's rmse: 4232.77\n",
      "[5000]\ttraining's rmse: 4223\n",
      "[5050]\ttraining's rmse: 4213.28\n",
      "[5100]\ttraining's rmse: 4203.05\n",
      "[5150]\ttraining's rmse: 4193.71\n",
      "[5200]\ttraining's rmse: 4183.99\n",
      "[5250]\ttraining's rmse: 4174.19\n",
      "[5300]\ttraining's rmse: 4164.98\n",
      "[5350]\ttraining's rmse: 4156.94\n",
      "[5400]\ttraining's rmse: 4147.74\n",
      "[5450]\ttraining's rmse: 4139.59\n",
      "[5500]\ttraining's rmse: 4132.04\n",
      "[5550]\ttraining's rmse: 4123.49\n",
      "[5600]\ttraining's rmse: 4115.16\n",
      "[5650]\ttraining's rmse: 4106.96\n",
      "[5700]\ttraining's rmse: 4098.75\n",
      "[5750]\ttraining's rmse: 4089.61\n",
      "[5800]\ttraining's rmse: 4080.87\n",
      "[5850]\ttraining's rmse: 4071.81\n",
      "[5900]\ttraining's rmse: 4062.88\n",
      "[5950]\ttraining's rmse: 4053.55\n",
      "[6000]\ttraining's rmse: 4046.01\n",
      "[6050]\ttraining's rmse: 4036.84\n",
      "[6100]\ttraining's rmse: 4028.15\n",
      "[6150]\ttraining's rmse: 4020.56\n",
      "[6200]\ttraining's rmse: 4012.72\n",
      "[6250]\ttraining's rmse: 4005.66\n",
      "[6300]\ttraining's rmse: 3997.11\n",
      "[6350]\ttraining's rmse: 3989.21\n",
      "[6400]\ttraining's rmse: 3982.28\n",
      "[6450]\ttraining's rmse: 3975.56\n",
      "[6500]\ttraining's rmse: 3968.31\n",
      "[6550]\ttraining's rmse: 3960.89\n",
      "[6600]\ttraining's rmse: 3953.98\n",
      "[6650]\ttraining's rmse: 3946.01\n",
      "[6700]\ttraining's rmse: 3937.95\n",
      "[6750]\ttraining's rmse: 3931.37\n",
      "[6800]\ttraining's rmse: 3924.39\n",
      "[6850]\ttraining's rmse: 3917.32\n",
      "[6900]\ttraining's rmse: 3910.92\n",
      "[6950]\ttraining's rmse: 3902.72\n",
      "[7000]\ttraining's rmse: 3894.58\n",
      "[7050]\ttraining's rmse: 3887.46\n",
      "[7100]\ttraining's rmse: 3878.66\n",
      "[7150]\ttraining's rmse: 3870.54\n",
      "[7200]\ttraining's rmse: 3863.18\n",
      "[7250]\ttraining's rmse: 3856.49\n",
      "[7300]\ttraining's rmse: 3850.46\n",
      "[7350]\ttraining's rmse: 3843.6\n",
      "[7400]\ttraining's rmse: 3837.54\n",
      "[7450]\ttraining's rmse: 3831.72\n",
      "[7500]\ttraining's rmse: 3825.05\n",
      "[7550]\ttraining's rmse: 3819.55\n",
      "[7600]\ttraining's rmse: 3814.19\n",
      "[7650]\ttraining's rmse: 3808.01\n",
      "[7700]\ttraining's rmse: 3801.45\n",
      "[7750]\ttraining's rmse: 3796.14\n",
      "[7800]\ttraining's rmse: 3789.98\n",
      "[7850]\ttraining's rmse: 3784.39\n",
      "[7900]\ttraining's rmse: 3778.51\n",
      "[7950]\ttraining's rmse: 3772.34\n",
      "[8000]\ttraining's rmse: 3766.45\n",
      "[8050]\ttraining's rmse: 3760.95\n",
      "[8100]\ttraining's rmse: 3754.81\n",
      "[8150]\ttraining's rmse: 3749.49\n",
      "[8200]\ttraining's rmse: 3744.5\n",
      "[8250]\ttraining's rmse: 3739.92\n",
      "[8300]\ttraining's rmse: 3735.58\n",
      "[8350]\ttraining's rmse: 3730.79\n",
      "[8400]\ttraining's rmse: 3724.78\n",
      "[8450]\ttraining's rmse: 3719.54\n",
      "[8500]\ttraining's rmse: 3713.77\n",
      "[8550]\ttraining's rmse: 3708.68\n",
      "[8600]\ttraining's rmse: 3703.26\n",
      "[8650]\ttraining's rmse: 3697.55\n",
      "[8700]\ttraining's rmse: 3693.27\n",
      "[8750]\ttraining's rmse: 3687.81\n",
      "[8800]\ttraining's rmse: 3682.76\n",
      "[8850]\ttraining's rmse: 3677.26\n",
      "[8900]\ttraining's rmse: 3672.26\n",
      "[8950]\ttraining's rmse: 3667.65\n",
      "[9000]\ttraining's rmse: 3662.82\n",
      "[9050]\ttraining's rmse: 3657.83\n",
      "[9100]\ttraining's rmse: 3652.53\n",
      "[9150]\ttraining's rmse: 3648.13\n",
      "[9200]\ttraining's rmse: 3643.49\n",
      "[9250]\ttraining's rmse: 3638.7\n",
      "[9300]\ttraining's rmse: 3634.19\n",
      "[9350]\ttraining's rmse: 3629.07\n",
      "[9400]\ttraining's rmse: 3624.28\n",
      "[9450]\ttraining's rmse: 3619.96\n",
      "[9500]\ttraining's rmse: 3614.74\n",
      "[9550]\ttraining's rmse: 3609.78\n",
      "[9600]\ttraining's rmse: 3605.13\n",
      "[9650]\ttraining's rmse: 3601.21\n",
      "[9700]\ttraining's rmse: 3596.46\n",
      "[9750]\ttraining's rmse: 3590.44\n",
      "[9800]\ttraining's rmse: 3586.46\n",
      "[9850]\ttraining's rmse: 3582.22\n",
      "[9900]\ttraining's rmse: 3577.36\n",
      "[9950]\ttraining's rmse: 3572.89\n",
      "[10000]\ttraining's rmse: 3568.73\n",
      "[10050]\ttraining's rmse: 3564.26\n",
      "[10100]\ttraining's rmse: 3560.36\n",
      "[10150]\ttraining's rmse: 3556.23\n",
      "[10200]\ttraining's rmse: 3551.75\n",
      "[10250]\ttraining's rmse: 3547.6\n",
      "[10300]\ttraining's rmse: 3543.53\n",
      "[10350]\ttraining's rmse: 3538.91\n",
      "[10400]\ttraining's rmse: 3534.76\n",
      "[10450]\ttraining's rmse: 3530.61\n",
      "[10500]\ttraining's rmse: 3526.58\n",
      "[10550]\ttraining's rmse: 3522.17\n",
      "[10600]\ttraining's rmse: 3517.61\n",
      "[10650]\ttraining's rmse: 3513.94\n",
      "[10700]\ttraining's rmse: 3509.61\n",
      "[10750]\ttraining's rmse: 3505.16\n",
      "[10800]\ttraining's rmse: 3500.64\n",
      "[10850]\ttraining's rmse: 3496.7\n",
      "[10900]\ttraining's rmse: 3492.37\n",
      "[10950]\ttraining's rmse: 3488.83\n",
      "[11000]\ttraining's rmse: 3484.66\n",
      "[11050]\ttraining's rmse: 3481.24\n",
      "[11100]\ttraining's rmse: 3477.39\n",
      "[11150]\ttraining's rmse: 3474.1\n",
      "[11200]\ttraining's rmse: 3470.7\n",
      "[11250]\ttraining's rmse: 3466.69\n",
      "[11300]\ttraining's rmse: 3463.31\n",
      "[11350]\ttraining's rmse: 3459.67\n",
      "[11400]\ttraining's rmse: 3456.26\n",
      "[11450]\ttraining's rmse: 3452.77\n",
      "[11500]\ttraining's rmse: 3448.62\n",
      "[11550]\ttraining's rmse: 3444.81\n",
      "[11600]\ttraining's rmse: 3441.1\n",
      "[11650]\ttraining's rmse: 3437.04\n",
      "[11700]\ttraining's rmse: 3433.28\n",
      "[11750]\ttraining's rmse: 3429.46\n",
      "[11800]\ttraining's rmse: 3426.2\n",
      "[11850]\ttraining's rmse: 3423.02\n",
      "[11900]\ttraining's rmse: 3419.47\n",
      "[11950]\ttraining's rmse: 3416.15\n",
      "[12000]\ttraining's rmse: 3412.46\n",
      "[12050]\ttraining's rmse: 3408.79\n",
      "[12100]\ttraining's rmse: 3404.76\n",
      "[12150]\ttraining's rmse: 3401.32\n",
      "[12200]\ttraining's rmse: 3397.62\n",
      "[12250]\ttraining's rmse: 3394.17\n",
      "[12300]\ttraining's rmse: 3390.97\n",
      "[12350]\ttraining's rmse: 3387.41\n",
      "[12400]\ttraining's rmse: 3383.98\n",
      "[12450]\ttraining's rmse: 3380.53\n",
      "[12500]\ttraining's rmse: 3377.46\n",
      "[12550]\ttraining's rmse: 3374.37\n",
      "[12600]\ttraining's rmse: 3370.42\n",
      "[12650]\ttraining's rmse: 3366.95\n",
      "[12700]\ttraining's rmse: 3363.38\n",
      "[12750]\ttraining's rmse: 3359.54\n",
      "[12800]\ttraining's rmse: 3356.55\n",
      "[12850]\ttraining's rmse: 3353.58\n",
      "[12900]\ttraining's rmse: 3350.3\n",
      "[12950]\ttraining's rmse: 3346.6\n",
      "[13000]\ttraining's rmse: 3343.36\n",
      "[13050]\ttraining's rmse: 3339.62\n",
      "[13100]\ttraining's rmse: 3336.53\n",
      "[13150]\ttraining's rmse: 3333.1\n",
      "[13200]\ttraining's rmse: 3329.96\n",
      "[13250]\ttraining's rmse: 3326.53\n",
      "[13300]\ttraining's rmse: 3323.72\n",
      "[13350]\ttraining's rmse: 3320.79\n",
      "[13400]\ttraining's rmse: 3317.39\n",
      "[13450]\ttraining's rmse: 3314.57\n",
      "[13500]\ttraining's rmse: 3311.82\n",
      "[13550]\ttraining's rmse: 3308.18\n",
      "[13600]\ttraining's rmse: 3304.84\n",
      "[13650]\ttraining's rmse: 3301.5\n",
      "[13700]\ttraining's rmse: 3298.08\n",
      "[13750]\ttraining's rmse: 3294.56\n",
      "[13800]\ttraining's rmse: 3290.38\n",
      "[13850]\ttraining's rmse: 3286.26\n",
      "[13900]\ttraining's rmse: 3282.24\n",
      "[13950]\ttraining's rmse: 3278.75\n",
      "[14000]\ttraining's rmse: 3275.4\n",
      "[14050]\ttraining's rmse: 3272.96\n",
      "[14100]\ttraining's rmse: 3269.68\n",
      "[14150]\ttraining's rmse: 3265.71\n",
      "[14200]\ttraining's rmse: 3262.3\n",
      "[14250]\ttraining's rmse: 3259.26\n",
      "[14300]\ttraining's rmse: 3255.41\n",
      "[14350]\ttraining's rmse: 3251.99\n",
      "[14400]\ttraining's rmse: 3249.07\n",
      "[14450]\ttraining's rmse: 3245.94\n",
      "[14500]\ttraining's rmse: 3243.11\n",
      "[14550]\ttraining's rmse: 3239.35\n",
      "[14600]\ttraining's rmse: 3236.78\n",
      "[14650]\ttraining's rmse: 3233.95\n",
      "[14700]\ttraining's rmse: 3231.29\n",
      "[14750]\ttraining's rmse: 3228.02\n",
      "[14800]\ttraining's rmse: 3225.27\n",
      "[14850]\ttraining's rmse: 3221.96\n",
      "[14900]\ttraining's rmse: 3218.83\n",
      "[14950]\ttraining's rmse: 3215.9\n",
      "[15000]\ttraining's rmse: 3212.83\n",
      "[15050]\ttraining's rmse: 3210.4\n",
      "[15100]\ttraining's rmse: 3207.23\n",
      "[15150]\ttraining's rmse: 3204.5\n",
      "[15200]\ttraining's rmse: 3201.27\n",
      "[15250]\ttraining's rmse: 3198.28\n",
      "[15300]\ttraining's rmse: 3195.27\n",
      "[15350]\ttraining's rmse: 3192\n",
      "[15400]\ttraining's rmse: 3189.4\n",
      "[15450]\ttraining's rmse: 3186.99\n",
      "[15500]\ttraining's rmse: 3184.13\n",
      "[15550]\ttraining's rmse: 3181.51\n",
      "[15600]\ttraining's rmse: 3178.57\n",
      "[15650]\ttraining's rmse: 3176.25\n",
      "[15700]\ttraining's rmse: 3173.22\n",
      "[15750]\ttraining's rmse: 3170.33\n",
      "[15800]\ttraining's rmse: 3167.91\n",
      "[15850]\ttraining's rmse: 3165.55\n",
      "[15900]\ttraining's rmse: 3162.94\n",
      "[15950]\ttraining's rmse: 3159.97\n",
      "[16000]\ttraining's rmse: 3157.28\n",
      "[16050]\ttraining's rmse: 3154.68\n",
      "[16100]\ttraining's rmse: 3152.31\n",
      "[16150]\ttraining's rmse: 3149.96\n",
      "[16200]\ttraining's rmse: 3147.49\n",
      "[16250]\ttraining's rmse: 3145.01\n",
      "[16300]\ttraining's rmse: 3142.21\n",
      "[16350]\ttraining's rmse: 3139.62\n",
      "[16400]\ttraining's rmse: 3136.91\n",
      "[16450]\ttraining's rmse: 3134.16\n",
      "[16500]\ttraining's rmse: 3131.11\n",
      "[16550]\ttraining's rmse: 3129.01\n",
      "[16600]\ttraining's rmse: 3126.42\n",
      "[16650]\ttraining's rmse: 3123.46\n",
      "[16700]\ttraining's rmse: 3120.78\n",
      "[16750]\ttraining's rmse: 3117.97\n",
      "[16800]\ttraining's rmse: 3115.56\n",
      "[16850]\ttraining's rmse: 3112.99\n",
      "[16900]\ttraining's rmse: 3110.79\n",
      "[16950]\ttraining's rmse: 3108.13\n",
      "[17000]\ttraining's rmse: 3105.74\n",
      "[17050]\ttraining's rmse: 3103.12\n",
      "[17100]\ttraining's rmse: 3100.37\n",
      "[17150]\ttraining's rmse: 3097.84\n",
      "[17200]\ttraining's rmse: 3095.09\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# WandB 초기화\n",
    "run_name = f\"lightGBM_final_{datetime.now().strftime('%m%d_%H%M')}\"\n",
    "wandb.init(\n",
    "    project=\"re_price_prediction\",\n",
    "    group=\"lightGBM final\",\n",
    "    name=run_name,\n",
    "    reinit=True,\n",
    "    config={\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': 100000,\n",
    "        #'learning_rate': 0.2,\n",
    "        #'num_leaves': 98,\n",
    "        #'max_depth': 6,\n",
    "        #'subsample': 0.64,\n",
    "        #'colsample_bytree': 0.64,\n",
    "        #'min_data_in_leaf': 1144,\n",
    "        'force_col_wise': True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 파라미터 정의\n",
    "params = wandb.config\n",
    "\n",
    "# 전체 학습 데이터 준비\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)\n",
    "X_final = X[best_features]\n",
    "\n",
    "# LGBMRegressor 모델 생성\n",
    "gbm = lgb.LGBMRegressor(**params)\n",
    "\n",
    "# Callback function to log evaluation metrics to wandb\n",
    "def log_to_wandb(env):\n",
    "    for data_name, eval_name, eval_result, stdv in env.evaluation_result_list:\n",
    "        metric_name = f\"{data_name}_{eval_name}\"\n",
    "        wandb.log({metric_name: eval_result})\n",
    "\n",
    "# 전체 데이터로 모델 학습\n",
    "print(\"Training on full dataset...\")\n",
    "gbm.fit(X_final, y,\n",
    "        eval_set=[(X_final, y)],\n",
    "        eval_metric='rmse',\n",
    "        categorical_feature=\"auto\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                   lgb.log_evaluation(period=50, show_stdv=True),\n",
    "                   log_to_wandb])\n",
    "\n",
    "# 최종 RMSE 계산\n",
    "final_train_preds = gbm.predict(X_final)\n",
    "final_train_rmse = np.sqrt(mean_squared_error(y, final_train_preds))\n",
    "wandb.log({\"final_train_rmse\": final_train_rmse})\n",
    "print(f\"Final Training RMSE: {final_train_rmse}\")\n",
    "\n",
    "\n",
    "\n",
    "# WandB 실행 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 테스트 데이터에 대한 예측\n",
    "print(\"Predicting on test data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "X_test = test_data_encoded.drop(['target'], axis=1)\n",
    "X_test_selected = X_test[best_features]\n",
    "\n",
    "real_test_pred = gbm.predict(X_test_selected)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(real_test_pred)\n",
    "\n",
    "# 예측 결과를 DataFrame으로 변환하고 CSV 파일로 저장\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)\n",
    "print(\"Predictions saved to 'output.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time based Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "X = X[best_features]\n",
    "y = y[best_features]\n",
    "\n",
    "X = X.sort_values(by=['계약년', '계약월'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 부여 함수\n",
    "def assign_weight(year):\n",
    "    if year >= 2020:\n",
    "        return 2\n",
    "    elif year >= 2018:\n",
    "        return 1.5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "X['weight'] = X['계약년'].apply(assign_weight)\n",
    "\n",
    "# LightGBM 모델 설정\n",
    "def create_model():\n",
    "    return LGBMRegressor(\n",
    "        objective='regression',\n",
    "        boosting_type='gbdt',\n",
    "        n_estimators=100000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "# 폴드 1: 2007-2011 데이터로 2012 예측\n",
    "X_train_1 = X[(X['계약년'] >= 2007) & (X['계약년'] <= 2011)]\n",
    "y_train_1 = y[X_train_1.index]\n",
    "X_val_1 = X[X['계약년'] == 2012]\n",
    "y_val_1 = y[X_val_1.index]\n",
    "\n",
    "model_1 = create_model()\n",
    "model_1.fit(\n",
    "    X_train_1.drop(['계약년월', 'weight'], axis=1),\n",
    "    y_train_1,\n",
    "    eval_set=[(X_val_1.drop(['계약년월', 'weight'], axis=1), y_val_1)],\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100,\n",
    "    sample_weight=X_train_1['weight']\n",
    ")\n",
    "\n",
    "# 폴드 2: 2018-2022 데이터로 2023 예측\n",
    "X_train_2 = X[(X['계약년'] >= 2018) & (X['계약년'] <= 2022)]\n",
    "y_train_2 = y[X_train_2.index]\n",
    "X_val_2 = X[X['계약년'] == 2023]\n",
    "y_val_2 = y[X_val_2.index]\n",
    "\n",
    "model_2 = create_model()\n",
    "model_2.fit(\n",
    "    X_train_2.drop(['계약년월', 'weight'], axis=1),\n",
    "    y_train_2,\n",
    "    eval_set=[(X_val_2.drop(['계약년월', 'weight'], axis=1), y_val_2)],\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100,\n",
    "    sample_weight=X_train_2['weight']\n",
    ")\n",
    "\n",
    "# 2023년 예측\n",
    "future_data = X[X['계약년'] == 2023]\n",
    "\n",
    "# 앙상블 예측\n",
    "y_pred_1 = model_1.predict(future_data.drop(['계약년월', 'weight'], axis=1))\n",
    "y_pred_2 = model_2.predict(future_data.drop(['계약년월', 'weight'], axis=1))\n",
    "y_future_pred = (y_pred_1 + y_pred_2) / 2  # 간단한 평균 앙상블\n",
    "\n",
    "# 결과 저장\n",
    "results = pd.DataFrame({\n",
    "    '계약년월': future_data['계약년월'],\n",
    "    '예측가격': y_future_pred\n",
    "})\n",
    "results.to_csv('future_predictions_ensemble.csv', index=False)\n",
    "print(\"2023년 예측 완료. 'future_predictions_ensemble.csv'에 저장되었습니다.\")\n",
    "\n",
    "# 각 모델의 성능 평가\n",
    "rmse_1 = np.sqrt(mean_squared_error(y_val_1, model_1.predict(X_val_1.drop(['계약년월', 'weight'], axis=1))))\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_val_2, model_2.predict(X_val_2.drop(['계약년월', 'weight'], axis=1))))\n",
    "\n",
    "print(f\"모델 1 (2007-2011 -> 2012) RMSE: {rmse_1}\")\n",
    "print(f\"모델 2 (2018-2022 -> 2023) RMSE: {rmse_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블 모델 생성\n",
    "def ensemble_predict(model1, model2, X):\n",
    "    pred1 = model1.predict(X, num_iteration=model1.best_iteration_)\n",
    "    pred2 = model2.predict(X, num_iteration=model2.best_iteration_)\n",
    "    return (pred1 + pred2) / 2\n",
    "\n",
    "\n",
    "# 테스트 데이터에 시계열 특성 추가 (년, 월만)\n",
    "X_test['계약년'] = pd.to_datetime(X['계약년월']).dt.year\n",
    "X_test['계약월'] = pd.to_datetime(X['계약년월']).dt.month\n",
    "\n",
    "# 앙상블 예측 수행\n",
    "real_test_pred = ensemble_predict(model_1, model_2, X_test.drop(['계약년월'], axis=1))\n",
    "\n",
    "print(\"테스트 데이터에 대한 예측 결과:\")\n",
    "print(real_test_pred)\n",
    "\n",
    "# 예측 결과를 DataFrame으로 변환\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "preds_df.to_csv('output.csv', index=True)\n",
    "print(\"예측 결과가 'output.csv' 파일로 저장되었습니다.\")\n",
    "\n",
    "# 각 모델의 성능 평가 (2023년 데이터에 대해)\n",
    "X_val_2023 = X[X['계약년'] == 2023].drop(['계약년월', 'weight'], axis=1)\n",
    "y_val_2023 = y[X[X['계약년'] == 2023].index]\n",
    "\n",
    "rmse_1 = np.sqrt(mean_squared_error(y_val_2023, model_1.predict(X_val_2023)))\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_val_2023, model_2.predict(X_val_2023)))\n",
    "rmse_ensemble = np.sqrt(mean_squared_error(y_val_2023, ensemble_predict(model_1, model_2, X_val_2023)))\n",
    "\n",
    "print(f\"모델 1 (2007-2011 -> 2023) RMSE: {rmse_1}\")\n",
    "print(f\"모델 2 (2018-2022 -> 2023) RMSE: {rmse_2}\")\n",
    "print(f\"앙상블 모델 (2023) RMSE: {rmse_ensemble}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# catBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple catBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "# Prepare the training data\n",
    "y = house_data['target']\n",
    "X = house_data.drop(['target'], axis=1)\n",
    "\n",
    "# Handle outliers in '전용면적' and apply log transformation\n",
    "def handle_outliers_and_log_transform(df, column):\n",
    "    # # Calculate the upper limit for outliers (using IQR)\n",
    "    # Q1 = df[column].quantile(0.25)\n",
    "    # Q3 = df[column].quantile(0.75)\n",
    "    # IQR = Q3 - Q1\n",
    "    # upper_limit = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # # Cap the outliers\n",
    "    # df[column] = np.where(df[column] > upper_limit, upper_limit, df[column])\n",
    "    \n",
    "    # Apply log transformation\n",
    "    df[column] = np.log1p(df[column])\n",
    "    return df\n",
    "\n",
    "X = handle_outliers_and_log_transform(X, '전용면적')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열의 결측치 개수 확인\n",
    "missing_data = house_data.isnull().sum().sort_values(ascending=False)\n",
    "missing_data = missing_data[missing_data > 0]  # 결측치가 있는 열만 표시\n",
    "\n",
    "print(\"결측치 개수:\")\n",
    "print(missing_data)\n",
    "\n",
    "# 결측치를 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=missing_data.index, y=missing_data.values)\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.title('Missing Values per Column')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "X = X.fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "categorical_features = [i for i, col in enumerate(X_train.columns) if X_train[col].dtype == 'object']\n",
    "\n",
    "# CatBoost Pool 생성\n",
    "train_pool = Pool(X_train, y_train, cat_features=categorical_features)\n",
    "val_pool = Pool(X_val, y_val, cat_features=categorical_features)\n",
    "\n",
    "\n",
    "# CatBoost 모델 정의\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    depth=10,\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='RMSE',\n",
    "    random_seed=42,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    cat_features=categorical_features,  \n",
    "    use_best_model=True,\n",
    "    verbose=10,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catBoost + optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "categorical_features = [i for i, col in enumerate(X_train.columns) if X_train[col].dtype == 'object']\n",
    "\n",
    "# CatBoost Pool 생성\n",
    "train_pool = Pool(X_train, y_train, cat_features=categorical_features)\n",
    "val_pool = Pool(X_val, y_val, cat_features=categorical_features)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 3000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-8, 100.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'max_bin': trial.suggest_int('max_bin', 200, 400),\n",
    "        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "        'od_wait': trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        random_seed=42,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        use_best_model=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    return rmse\n",
    "\n",
    "# Optuna 학습 실행\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('RMSE: {}'.format(trial.value))\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "# 최적의 하이퍼파라미터로 최종 모델 학습\n",
    "best_params = study.best_params\n",
    "best_model = CatBoostRegressor(\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='RMSE',\n",
    "    random_seed=42,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "best_model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,\n",
    "    use_best_model=True,\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y_train = train_data_encoded['target']\n",
    "X_train = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "# Hold out split을 사용해 학습 데이터와 검증 데이터를 8:2 비율로 나누겠습니다.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# WandB 초기화\n",
    "wandb.init(\n",
    "    project=\"re_price_prediction\",\n",
    "    group=\"xgboost\",\n",
    "    reinit=True,\n",
    "    config={\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 12,\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 100000,\n",
    "    }\n",
    ")\n",
    "\n",
    "params = dict(wandb.config)\n",
    "\n",
    "\n",
    "# DMatrix 생성\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# 사용자 정의 WandB 콜백 함수 정의\n",
    "class WandbCallback(xgb.callback.TrainingCallback):\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        for data_name, eval_metrics in evals_log.items():\n",
    "            for metric_name, scores in eval_metrics.items():\n",
    "                wandb.log({f\"{data_name}_{metric_name}\": scores[-1], \"iteration\": epoch})\n",
    "        return False\n",
    "\n",
    "# 모델 학습\n",
    "watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "model = xgb.train(params, dtrain, evals=watchlist, num_boost_round=100000, early_stopping_rounds=50, callbacks=[WandbCallback()])\n",
    "\n",
    "# 예측 및 평가\n",
    "# val_preds = model.predict(dval)\n",
    "# final_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "# wandb.log({'final_val_rmse': final_rmse})\n",
    "\n",
    "# WandB 실행 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189626.67 252217.7  325632.72 ...  87333.41  72458.06  74300.24]\n",
      "CPU times: user 3.88 s, sys: 0 ns, total: 3.88 s\n",
      "Wall time: 419 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = test_data_encoded.drop(['target'], axis=1)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "real_test_pred = model.predict(dtest)\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)\n",
    "\n",
    "print(real_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost + RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# WandB 초기화\n",
    "wandb.init(\n",
    "    project=\"re_price_prediction\",\n",
    "    group=\"xgboost\",\n",
    "    reinit=True,\n",
    "    config={\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 12,\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 100000,\n",
    "    }\n",
    ")\n",
    "\n",
    "params = dict(wandb.config)\n",
    "\n",
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y_train = train_data_encoded['target']\n",
    "X_train = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "# Hold out split을 사용해 학습 데이터와 검증 데이터를 8:2 비율로 나누겠습니다.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# DMatrix 생성\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# 사용자 정의 WandB 콜백 함수 정의\n",
    "class WandbCallback(xgb.callback.TrainingCallback):\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        for data_name, eval_metrics in evals_log.items():\n",
    "            for metric_name, scores in eval_metrics.items():\n",
    "                wandb.log({f\"{data_name}_{metric_name}\": scores[-1], \"iteration\": epoch})\n",
    "        return False\n",
    "\n",
    "# 하이퍼파라미터 범위 정의\n",
    "param_dist = {\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# XGBoost 모델 생성\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# RandomizedSearchCV 설정\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=50, scoring=make_scorer(mean_squared_error, squared=False), cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "# RandomizedSearchCV 실행 및 WandB 로깅\n",
    "def wandb_random_search_logging(cv_results):\n",
    "    for i in range(len(cv_results['params'])):\n",
    "        wandb.log({\n",
    "            'iteration': i,\n",
    "            'mean_test_score': cv_results['mean_test_score'][i],\n",
    "            'std_test_score': cv_results['std_test_score'][i],\n",
    "            'params': cv_results['params'][i]\n",
    "        })\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "wandb_random_search_logging(random_search.cv_results_)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "best_params = random_search.best_params_\n",
    "wandb.log({'best_params': best_params})\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델 학습\n",
    "watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "best_params['eval_metric'] = 'rmse'  # eval_metric 추가\n",
    "model = xgb.train(best_params, dtrain, evals=watchlist, num_boost_round=100000, early_stopping_rounds=50, callbacks=[WandbCallback()])\n",
    "\n",
    "# 예측 및 평가\n",
    "val_preds = model.predict(dval)\n",
    "final_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "wandb.log({'final_val_rmse': final_rmse})\n",
    "\n",
    "# 테스트 데이터 예측 및 결과 저장\n",
    "X_test = test_data_encoded.drop(['target'], axis=1)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "real_test_pred = model.predict(dtest)\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)\n",
    "\n",
    "print(real_test_pred)\n",
    "\n",
    "# WandB 실행 종료\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGBM + K-Fold Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "X_final = X[best_features]\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(X_final, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "# WandB 초기화\n",
    "wandb.init(\n",
    "    project=\"re_price_prediction\",\n",
    "    group=\"k-fold holdout\",\n",
    "    reinit=True,\n",
    "    config={\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': 100000,\n",
    "        'learning_rate': 0.010204448913134991,\n",
    "        'num_leaves': 128,\n",
    "        'max_depth': 10,\n",
    "        'min_child_samples': 8,\n",
    "        'subsample': 0.5668227534307944,\n",
    "        'colsample_bytree': 0.6033862727269486,\n",
    "        'reg_alpha': 2.5906244698002496e-06,\n",
    "        'reg_lambda': 2.979573198070953e-05,\n",
    "        'force_row_wise': True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 파라미터 정의\n",
    "params = wandb.config\n",
    "\n",
    "# 데이터 준비\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)\n",
    "X_final = X[best_features]\n",
    "\n",
    "# Holdout set 생성\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# K-Fold 설정\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# K-Fold 교차 검증 실행\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_train_final):\n",
    "    X_train_fold, X_val_fold = X_train_final.iloc[train_index], X_train_final.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_final.iloc[train_index], y_train_final.iloc[val_index]\n",
    "    \n",
    "    # LGBMRegressor 모델 생성\n",
    "    gbm = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    # 모델 학습\n",
    "    gbm.fit(X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n",
    "            eval_metric='rmse',\n",
    "            categorical_feature=\"auto\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                       lgb.log_evaluation(period=10, show_stdv=True)])\n",
    "    \n",
    "    # 검증 결과 WandB에 로그\n",
    "    val_preds = gbm.predict(X_val_fold)\n",
    "    val_rmse = mean_squared_error(y_val_fold, val_preds, squared=False)\n",
    "    wandb.log({f'fold_{fold}_val_rmse': val_rmse})\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# 최종 홀드아웃 세트에 대한 평가\n",
    "final_val_preds = gbm.predict(X_val_final)\n",
    "final_val_rmse = mean_squared_error(y_val_final, final_val_preds, squared=False)\n",
    "wandb.log({'final_holdout_rmse': final_val_rmse})\n",
    "\n",
    "# [optional] finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 테스트 데이터에 대한 예측\n",
    "print(\"Predicting on test data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "X_test = test_data_encoded.drop(['target'], axis=1)\n",
    "X_test_selected = X_test[best_features]\n",
    "\n",
    "real_test_pred = gbm.predict(X_test_selected, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(real_test_pred)\n",
    "\n",
    "# 예측 결과를 DataFrame으로 변환하고 CSV 파일로 저장\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)\n",
    "print(\"Predictions saved to 'output.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGBM + TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y = train_data_encoded['target']\n",
    "X = train_data_encoded.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sort_values(by=['계약년', '계약월'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2155\n",
      "[LightGBM] [Info] Number of data points in the train set: 124318, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 36842.178550\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's rmse: 24983.6\tvalid_1's rmse: 34322.8\n",
      "[20]\ttraining's rmse: 23286.7\tvalid_1's rmse: 32109.5\n",
      "[30]\ttraining's rmse: 21881.1\tvalid_1's rmse: 30291.4\n",
      "[40]\ttraining's rmse: 20658.1\tvalid_1's rmse: 28732.9\n",
      "[50]\ttraining's rmse: 19320.4\tvalid_1's rmse: 27014.6\n",
      "[60]\ttraining's rmse: 18176.5\tvalid_1's rmse: 25529.2\n",
      "[70]\ttraining's rmse: 17011.6\tvalid_1's rmse: 24026.2\n",
      "[80]\ttraining's rmse: 16137.9\tvalid_1's rmse: 22875\n",
      "[90]\ttraining's rmse: 15253.1\tvalid_1's rmse: 21709.5\n",
      "[100]\ttraining's rmse: 14452.6\tvalid_1's rmse: 20652.8\n",
      "[110]\ttraining's rmse: 13754.1\tvalid_1's rmse: 19721.5\n",
      "[120]\ttraining's rmse: 13203.8\tvalid_1's rmse: 19013.4\n",
      "[130]\ttraining's rmse: 12556.2\tvalid_1's rmse: 18154.9\n",
      "[140]\ttraining's rmse: 12052.7\tvalid_1's rmse: 17483.9\n",
      "[150]\ttraining's rmse: 11558.5\tvalid_1's rmse: 16835.5\n",
      "[160]\ttraining's rmse: 11188.2\tvalid_1's rmse: 16360.6\n",
      "[170]\ttraining's rmse: 10830.8\tvalid_1's rmse: 15911.9\n",
      "[180]\ttraining's rmse: 10439.7\tvalid_1's rmse: 15394.7\n",
      "[190]\ttraining's rmse: 10138.7\tvalid_1's rmse: 15001.8\n",
      "[200]\ttraining's rmse: 9860.18\tvalid_1's rmse: 14658.3\n",
      "[210]\ttraining's rmse: 9612.02\tvalid_1's rmse: 14354.4\n",
      "[220]\ttraining's rmse: 9352.43\tvalid_1's rmse: 14014\n",
      "[230]\ttraining's rmse: 9111.88\tvalid_1's rmse: 13710.6\n",
      "[240]\ttraining's rmse: 8907.93\tvalid_1's rmse: 13461.8\n",
      "[250]\ttraining's rmse: 8711.17\tvalid_1's rmse: 13215.9\n",
      "[260]\ttraining's rmse: 8555.04\tvalid_1's rmse: 13047.5\n",
      "[270]\ttraining's rmse: 8398.49\tvalid_1's rmse: 12872.4\n",
      "[280]\ttraining's rmse: 8242.17\tvalid_1's rmse: 12687.2\n",
      "[290]\ttraining's rmse: 8107.04\tvalid_1's rmse: 12547.1\n",
      "[300]\ttraining's rmse: 7977.27\tvalid_1's rmse: 12398.1\n",
      "[310]\ttraining's rmse: 7861.9\tvalid_1's rmse: 12275.3\n",
      "[320]\ttraining's rmse: 7750.97\tvalid_1's rmse: 12133.1\n",
      "[330]\ttraining's rmse: 7663.84\tvalid_1's rmse: 12052.5\n",
      "[340]\ttraining's rmse: 7571.63\tvalid_1's rmse: 11960.1\n",
      "[350]\ttraining's rmse: 7492.74\tvalid_1's rmse: 11885.1\n",
      "[360]\ttraining's rmse: 7404.89\tvalid_1's rmse: 11791.1\n",
      "[370]\ttraining's rmse: 7324.5\tvalid_1's rmse: 11702.2\n",
      "[380]\ttraining's rmse: 7251.66\tvalid_1's rmse: 11631.5\n",
      "[390]\ttraining's rmse: 7181.54\tvalid_1's rmse: 11561.1\n",
      "[400]\ttraining's rmse: 7117.76\tvalid_1's rmse: 11512.4\n",
      "[410]\ttraining's rmse: 7053.32\tvalid_1's rmse: 11444.4\n",
      "[420]\ttraining's rmse: 6994.49\tvalid_1's rmse: 11383.9\n",
      "[430]\ttraining's rmse: 6937.59\tvalid_1's rmse: 11336.9\n",
      "[440]\ttraining's rmse: 6883.14\tvalid_1's rmse: 11293.4\n",
      "[450]\ttraining's rmse: 6831.59\tvalid_1's rmse: 11252.5\n",
      "[460]\ttraining's rmse: 6785.22\tvalid_1's rmse: 11215.9\n",
      "[470]\ttraining's rmse: 6735.79\tvalid_1's rmse: 11178.2\n",
      "[480]\ttraining's rmse: 6689.47\tvalid_1's rmse: 11144.9\n",
      "[490]\ttraining's rmse: 6642.87\tvalid_1's rmse: 11100.3\n",
      "[500]\ttraining's rmse: 6600.77\tvalid_1's rmse: 11075\n",
      "[510]\ttraining's rmse: 6560.46\tvalid_1's rmse: 11050.5\n",
      "[520]\ttraining's rmse: 6521.76\tvalid_1's rmse: 11020.4\n",
      "[530]\ttraining's rmse: 6480.63\tvalid_1's rmse: 10989.5\n",
      "[540]\ttraining's rmse: 6443.37\tvalid_1's rmse: 10968.3\n",
      "[550]\ttraining's rmse: 6408.17\tvalid_1's rmse: 10946\n",
      "[560]\ttraining's rmse: 6371.78\tvalid_1's rmse: 10922.3\n",
      "[570]\ttraining's rmse: 6336.97\tvalid_1's rmse: 10900\n",
      "[580]\ttraining's rmse: 6303.85\tvalid_1's rmse: 10881\n",
      "[590]\ttraining's rmse: 6274.38\tvalid_1's rmse: 10867.4\n",
      "[600]\ttraining's rmse: 6243.84\tvalid_1's rmse: 10856.4\n",
      "[610]\ttraining's rmse: 6212.76\tvalid_1's rmse: 10841.4\n",
      "[620]\ttraining's rmse: 6181.7\tvalid_1's rmse: 10818.7\n",
      "[630]\ttraining's rmse: 6152.55\tvalid_1's rmse: 10805.8\n",
      "[640]\ttraining's rmse: 6123.04\tvalid_1's rmse: 10789.5\n",
      "[650]\ttraining's rmse: 6095.76\tvalid_1's rmse: 10777.3\n",
      "[660]\ttraining's rmse: 6066.19\tvalid_1's rmse: 10757.2\n",
      "[670]\ttraining's rmse: 6037.8\tvalid_1's rmse: 10735.7\n",
      "[680]\ttraining's rmse: 6012.93\tvalid_1's rmse: 10726\n",
      "[690]\ttraining's rmse: 5985.56\tvalid_1's rmse: 10709.6\n",
      "[700]\ttraining's rmse: 5959.29\tvalid_1's rmse: 10691.6\n",
      "[710]\ttraining's rmse: 5932.72\tvalid_1's rmse: 10687.2\n",
      "[720]\ttraining's rmse: 5908.95\tvalid_1's rmse: 10678.6\n",
      "[730]\ttraining's rmse: 5886.71\tvalid_1's rmse: 10675.1\n",
      "[740]\ttraining's rmse: 5864.76\tvalid_1's rmse: 10667.6\n",
      "[750]\ttraining's rmse: 5841.89\tvalid_1's rmse: 10655.2\n",
      "[760]\ttraining's rmse: 5820.43\tvalid_1's rmse: 10651.9\n",
      "[770]\ttraining's rmse: 5795.88\tvalid_1's rmse: 10642.2\n",
      "[780]\ttraining's rmse: 5775.62\tvalid_1's rmse: 10632.7\n",
      "[790]\ttraining's rmse: 5755.59\tvalid_1's rmse: 10627.1\n",
      "[800]\ttraining's rmse: 5734.23\tvalid_1's rmse: 10619.1\n",
      "[810]\ttraining's rmse: 5713.93\tvalid_1's rmse: 10612.2\n",
      "[820]\ttraining's rmse: 5693.42\tvalid_1's rmse: 10605.7\n",
      "[830]\ttraining's rmse: 5673.51\tvalid_1's rmse: 10601.3\n",
      "[840]\ttraining's rmse: 5652.16\tvalid_1's rmse: 10591.7\n",
      "[850]\ttraining's rmse: 5633.63\tvalid_1's rmse: 10586.7\n",
      "[860]\ttraining's rmse: 5613.64\tvalid_1's rmse: 10581.9\n",
      "[870]\ttraining's rmse: 5593.97\tvalid_1's rmse: 10574.1\n",
      "[880]\ttraining's rmse: 5576.94\tvalid_1's rmse: 10570.5\n",
      "[890]\ttraining's rmse: 5560.37\tvalid_1's rmse: 10568.5\n",
      "[900]\ttraining's rmse: 5543.2\tvalid_1's rmse: 10562.5\n",
      "[910]\ttraining's rmse: 5525.6\tvalid_1's rmse: 10555.9\n",
      "[920]\ttraining's rmse: 5508.69\tvalid_1's rmse: 10550.1\n",
      "[930]\ttraining's rmse: 5491.88\tvalid_1's rmse: 10538.7\n",
      "[940]\ttraining's rmse: 5474.43\tvalid_1's rmse: 10531.9\n",
      "[950]\ttraining's rmse: 5458.07\tvalid_1's rmse: 10525.5\n",
      "[960]\ttraining's rmse: 5442.18\tvalid_1's rmse: 10522.3\n",
      "[970]\ttraining's rmse: 5425.32\tvalid_1's rmse: 10513.9\n",
      "[980]\ttraining's rmse: 5410.47\tvalid_1's rmse: 10507.4\n",
      "[990]\ttraining's rmse: 5397.25\tvalid_1's rmse: 10503.1\n",
      "[1000]\ttraining's rmse: 5382.94\tvalid_1's rmse: 10496.3\n",
      "[1010]\ttraining's rmse: 5366.2\tvalid_1's rmse: 10486.5\n",
      "[1020]\ttraining's rmse: 5351.33\tvalid_1's rmse: 10481.9\n",
      "[1030]\ttraining's rmse: 5336.85\tvalid_1's rmse: 10476.5\n",
      "[1040]\ttraining's rmse: 5322.74\tvalid_1's rmse: 10469.3\n",
      "[1050]\ttraining's rmse: 5308.92\tvalid_1's rmse: 10463.6\n",
      "[1060]\ttraining's rmse: 5294.99\tvalid_1's rmse: 10462.1\n",
      "[1070]\ttraining's rmse: 5280.82\tvalid_1's rmse: 10454.2\n",
      "[1080]\ttraining's rmse: 5269.26\tvalid_1's rmse: 10449.8\n",
      "[1090]\ttraining's rmse: 5257.47\tvalid_1's rmse: 10445.4\n",
      "[1100]\ttraining's rmse: 5245.35\tvalid_1's rmse: 10441.4\n",
      "[1110]\ttraining's rmse: 5232.95\tvalid_1's rmse: 10436.6\n",
      "[1120]\ttraining's rmse: 5220.97\tvalid_1's rmse: 10433.1\n",
      "[1130]\ttraining's rmse: 5209.01\tvalid_1's rmse: 10429.6\n",
      "[1140]\ttraining's rmse: 5196.87\tvalid_1's rmse: 10423.9\n",
      "[1150]\ttraining's rmse: 5186.01\tvalid_1's rmse: 10420.2\n",
      "[1160]\ttraining's rmse: 5174.14\tvalid_1's rmse: 10414.1\n",
      "[1170]\ttraining's rmse: 5162.13\tvalid_1's rmse: 10408.5\n",
      "[1180]\ttraining's rmse: 5149.98\tvalid_1's rmse: 10405.1\n",
      "[1190]\ttraining's rmse: 5139.77\tvalid_1's rmse: 10399.8\n",
      "[1200]\ttraining's rmse: 5128.57\tvalid_1's rmse: 10394.8\n",
      "[1210]\ttraining's rmse: 5119.5\tvalid_1's rmse: 10393.4\n",
      "[1220]\ttraining's rmse: 5109.53\tvalid_1's rmse: 10391.2\n",
      "[1230]\ttraining's rmse: 5099.89\tvalid_1's rmse: 10388.5\n",
      "[1240]\ttraining's rmse: 5089.33\tvalid_1's rmse: 10383.6\n",
      "[1250]\ttraining's rmse: 5079.02\tvalid_1's rmse: 10378.3\n",
      "[1260]\ttraining's rmse: 5068.8\tvalid_1's rmse: 10371.1\n",
      "[1270]\ttraining's rmse: 5060.77\tvalid_1's rmse: 10366.9\n",
      "[1280]\ttraining's rmse: 5051.82\tvalid_1's rmse: 10362.9\n",
      "[1290]\ttraining's rmse: 5042\tvalid_1's rmse: 10359.2\n",
      "[1300]\ttraining's rmse: 5034.9\tvalid_1's rmse: 10356.9\n",
      "[1310]\ttraining's rmse: 5026.16\tvalid_1's rmse: 10352.4\n",
      "[1320]\ttraining's rmse: 5016.94\tvalid_1's rmse: 10349.5\n",
      "[1330]\ttraining's rmse: 5006.43\tvalid_1's rmse: 10344.9\n",
      "[1340]\ttraining's rmse: 4996.08\tvalid_1's rmse: 10340.7\n",
      "[1350]\ttraining's rmse: 4986.83\tvalid_1's rmse: 10335.3\n",
      "[1360]\ttraining's rmse: 4977.79\tvalid_1's rmse: 10334.5\n",
      "[1370]\ttraining's rmse: 4968.15\tvalid_1's rmse: 10330.9\n",
      "[1380]\ttraining's rmse: 4957.99\tvalid_1's rmse: 10326.6\n",
      "[1390]\ttraining's rmse: 4949.99\tvalid_1's rmse: 10320.9\n",
      "[1400]\ttraining's rmse: 4939.61\tvalid_1's rmse: 10316.9\n",
      "[1410]\ttraining's rmse: 4930.63\tvalid_1's rmse: 10314.7\n",
      "[1420]\ttraining's rmse: 4923.02\tvalid_1's rmse: 10311.3\n",
      "[1430]\ttraining's rmse: 4914.12\tvalid_1's rmse: 10310\n",
      "[1440]\ttraining's rmse: 4904.62\tvalid_1's rmse: 10306.3\n",
      "[1450]\ttraining's rmse: 4896.68\tvalid_1's rmse: 10303.5\n",
      "[1460]\ttraining's rmse: 4889.05\tvalid_1's rmse: 10300\n",
      "[1470]\ttraining's rmse: 4881.32\tvalid_1's rmse: 10298.4\n",
      "[1480]\ttraining's rmse: 4871.61\tvalid_1's rmse: 10295.4\n",
      "[1490]\ttraining's rmse: 4863.65\tvalid_1's rmse: 10293.2\n",
      "[1500]\ttraining's rmse: 4854.8\tvalid_1's rmse: 10289.3\n",
      "[1510]\ttraining's rmse: 4846.61\tvalid_1's rmse: 10287.3\n",
      "[1520]\ttraining's rmse: 4839.31\tvalid_1's rmse: 10288.2\n",
      "[1530]\ttraining's rmse: 4830.2\tvalid_1's rmse: 10284.1\n",
      "[1540]\ttraining's rmse: 4821.67\tvalid_1's rmse: 10279.7\n",
      "[1550]\ttraining's rmse: 4814.02\tvalid_1's rmse: 10276.7\n",
      "[1560]\ttraining's rmse: 4806.59\tvalid_1's rmse: 10274.5\n",
      "[1570]\ttraining's rmse: 4798.2\tvalid_1's rmse: 10273.6\n",
      "[1580]\ttraining's rmse: 4790.93\tvalid_1's rmse: 10271.7\n",
      "[1590]\ttraining's rmse: 4782.46\tvalid_1's rmse: 10269.5\n",
      "[1600]\ttraining's rmse: 4776.2\tvalid_1's rmse: 10266.5\n",
      "[1610]\ttraining's rmse: 4767.96\tvalid_1's rmse: 10265.2\n",
      "[1620]\ttraining's rmse: 4760.94\tvalid_1's rmse: 10263.4\n",
      "[1630]\ttraining's rmse: 4753.51\tvalid_1's rmse: 10260.9\n",
      "[1640]\ttraining's rmse: 4745.86\tvalid_1's rmse: 10257.5\n",
      "[1650]\ttraining's rmse: 4737.78\tvalid_1's rmse: 10253.7\n",
      "[1660]\ttraining's rmse: 4729.55\tvalid_1's rmse: 10252.4\n",
      "[1670]\ttraining's rmse: 4722.24\tvalid_1's rmse: 10252.7\n",
      "[1680]\ttraining's rmse: 4714.44\tvalid_1's rmse: 10250.8\n",
      "[1690]\ttraining's rmse: 4707.58\tvalid_1's rmse: 10247\n",
      "[1700]\ttraining's rmse: 4700.23\tvalid_1's rmse: 10244.9\n",
      "[1710]\ttraining's rmse: 4693.67\tvalid_1's rmse: 10243.5\n",
      "[1720]\ttraining's rmse: 4687.55\tvalid_1's rmse: 10242.7\n",
      "[1730]\ttraining's rmse: 4681.59\tvalid_1's rmse: 10241\n",
      "[1740]\ttraining's rmse: 4674.86\tvalid_1's rmse: 10239.7\n",
      "[1750]\ttraining's rmse: 4669.38\tvalid_1's rmse: 10239\n",
      "[1760]\ttraining's rmse: 4663.04\tvalid_1's rmse: 10236.6\n",
      "[1770]\ttraining's rmse: 4655.44\tvalid_1's rmse: 10235\n",
      "[1780]\ttraining's rmse: 4648.62\tvalid_1's rmse: 10233\n",
      "[1790]\ttraining's rmse: 4641.94\tvalid_1's rmse: 10231.9\n",
      "[1800]\ttraining's rmse: 4635.41\tvalid_1's rmse: 10230.9\n",
      "[1810]\ttraining's rmse: 4628.47\tvalid_1's rmse: 10228.9\n",
      "[1820]\ttraining's rmse: 4621.37\tvalid_1's rmse: 10226.5\n",
      "[1830]\ttraining's rmse: 4613.93\tvalid_1's rmse: 10224.1\n",
      "[1840]\ttraining's rmse: 4608.67\tvalid_1's rmse: 10224.3\n",
      "[1850]\ttraining's rmse: 4601.31\tvalid_1's rmse: 10222.8\n",
      "[1860]\ttraining's rmse: 4594.4\tvalid_1's rmse: 10222.3\n",
      "[1870]\ttraining's rmse: 4587.21\tvalid_1's rmse: 10221.3\n",
      "[1880]\ttraining's rmse: 4581.1\tvalid_1's rmse: 10219.9\n",
      "[1890]\ttraining's rmse: 4574.53\tvalid_1's rmse: 10216.2\n",
      "[1900]\ttraining's rmse: 4568.22\tvalid_1's rmse: 10213.8\n",
      "[1910]\ttraining's rmse: 4562.2\tvalid_1's rmse: 10212.5\n",
      "[1920]\ttraining's rmse: 4555.32\tvalid_1's rmse: 10210.5\n",
      "[1930]\ttraining's rmse: 4549.63\tvalid_1's rmse: 10208.7\n",
      "[1940]\ttraining's rmse: 4542.78\tvalid_1's rmse: 10207.3\n",
      "[1950]\ttraining's rmse: 4537.14\tvalid_1's rmse: 10204.9\n",
      "[1960]\ttraining's rmse: 4530.97\tvalid_1's rmse: 10204.5\n",
      "[1970]\ttraining's rmse: 4525.12\tvalid_1's rmse: 10202.6\n",
      "[1980]\ttraining's rmse: 4519.12\tvalid_1's rmse: 10200.4\n",
      "[1990]\ttraining's rmse: 4513.65\tvalid_1's rmse: 10200.7\n",
      "[2000]\ttraining's rmse: 4508.01\tvalid_1's rmse: 10197.9\n",
      "[2010]\ttraining's rmse: 4501.8\tvalid_1's rmse: 10194.8\n",
      "[2020]\ttraining's rmse: 4496.28\tvalid_1's rmse: 10192.7\n",
      "[2030]\ttraining's rmse: 4490.71\tvalid_1's rmse: 10191.8\n",
      "[2040]\ttraining's rmse: 4485.39\tvalid_1's rmse: 10190.6\n",
      "[2050]\ttraining's rmse: 4479.49\tvalid_1's rmse: 10189.4\n",
      "[2060]\ttraining's rmse: 4473.79\tvalid_1's rmse: 10187.8\n",
      "[2070]\ttraining's rmse: 4467.61\tvalid_1's rmse: 10186.5\n",
      "[2080]\ttraining's rmse: 4462.77\tvalid_1's rmse: 10186.4\n",
      "[2090]\ttraining's rmse: 4457.4\tvalid_1's rmse: 10184.6\n",
      "[2100]\ttraining's rmse: 4450.72\tvalid_1's rmse: 10183.6\n",
      "[2110]\ttraining's rmse: 4446.15\tvalid_1's rmse: 10182.4\n",
      "[2120]\ttraining's rmse: 4439.79\tvalid_1's rmse: 10180.4\n",
      "[2130]\ttraining's rmse: 4434.01\tvalid_1's rmse: 10179.6\n",
      "[2140]\ttraining's rmse: 4428.22\tvalid_1's rmse: 10179.6\n",
      "[2150]\ttraining's rmse: 4423.06\tvalid_1's rmse: 10178.8\n",
      "[2160]\ttraining's rmse: 4418.8\tvalid_1's rmse: 10178.1\n",
      "[2170]\ttraining's rmse: 4412.7\tvalid_1's rmse: 10175.7\n",
      "[2180]\ttraining's rmse: 4406.73\tvalid_1's rmse: 10172.6\n",
      "[2190]\ttraining's rmse: 4401.88\tvalid_1's rmse: 10172.2\n",
      "[2200]\ttraining's rmse: 4395.92\tvalid_1's rmse: 10170.3\n",
      "[2210]\ttraining's rmse: 4391.24\tvalid_1's rmse: 10169.3\n",
      "[2220]\ttraining's rmse: 4387.03\tvalid_1's rmse: 10169\n",
      "[2230]\ttraining's rmse: 4381.47\tvalid_1's rmse: 10169.3\n",
      "[2240]\ttraining's rmse: 4375.27\tvalid_1's rmse: 10167.7\n",
      "[2250]\ttraining's rmse: 4370.7\tvalid_1's rmse: 10167.6\n",
      "[2260]\ttraining's rmse: 4365.82\tvalid_1's rmse: 10165.7\n",
      "[2270]\ttraining's rmse: 4359.32\tvalid_1's rmse: 10165.4\n",
      "[2280]\ttraining's rmse: 4354.89\tvalid_1's rmse: 10165.4\n",
      "[2290]\ttraining's rmse: 4350.06\tvalid_1's rmse: 10165\n",
      "[2300]\ttraining's rmse: 4344.88\tvalid_1's rmse: 10164.2\n",
      "[2310]\ttraining's rmse: 4340.61\tvalid_1's rmse: 10163.5\n",
      "[2320]\ttraining's rmse: 4335.32\tvalid_1's rmse: 10161.2\n",
      "[2330]\ttraining's rmse: 4331.21\tvalid_1's rmse: 10160\n",
      "[2340]\ttraining's rmse: 4326.11\tvalid_1's rmse: 10158.2\n",
      "[2350]\ttraining's rmse: 4320.68\tvalid_1's rmse: 10156.6\n",
      "[2360]\ttraining's rmse: 4315.15\tvalid_1's rmse: 10156.4\n",
      "[2370]\ttraining's rmse: 4310.02\tvalid_1's rmse: 10155.3\n",
      "[2380]\ttraining's rmse: 4303.1\tvalid_1's rmse: 10152.8\n",
      "[2390]\ttraining's rmse: 4297.57\tvalid_1's rmse: 10152.5\n",
      "[2400]\ttraining's rmse: 4292.11\tvalid_1's rmse: 10151.8\n",
      "[2410]\ttraining's rmse: 4287.37\tvalid_1's rmse: 10151.1\n",
      "[2420]\ttraining's rmse: 4283.39\tvalid_1's rmse: 10149.7\n",
      "[2430]\ttraining's rmse: 4278.6\tvalid_1's rmse: 10149.6\n",
      "[2440]\ttraining's rmse: 4271.96\tvalid_1's rmse: 10148.6\n",
      "[2450]\ttraining's rmse: 4268.05\tvalid_1's rmse: 10147.6\n",
      "[2460]\ttraining's rmse: 4262.76\tvalid_1's rmse: 10146\n",
      "[2470]\ttraining's rmse: 4258.46\tvalid_1's rmse: 10146.2\n",
      "[2480]\ttraining's rmse: 4253.94\tvalid_1's rmse: 10145.8\n",
      "[2490]\ttraining's rmse: 4249.1\tvalid_1's rmse: 10146.5\n",
      "[2500]\ttraining's rmse: 4244.44\tvalid_1's rmse: 10145.5\n",
      "[2510]\ttraining's rmse: 4239.51\tvalid_1's rmse: 10144.9\n",
      "[2520]\ttraining's rmse: 4235.34\tvalid_1's rmse: 10145.1\n",
      "[2530]\ttraining's rmse: 4230.69\tvalid_1's rmse: 10145\n",
      "[2540]\ttraining's rmse: 4225.38\tvalid_1's rmse: 10142.3\n",
      "[2550]\ttraining's rmse: 4220.16\tvalid_1's rmse: 10140\n",
      "[2560]\ttraining's rmse: 4214.24\tvalid_1's rmse: 10138.5\n",
      "[2570]\ttraining's rmse: 4208.44\tvalid_1's rmse: 10137.2\n",
      "[2580]\ttraining's rmse: 4204.92\tvalid_1's rmse: 10136.6\n",
      "[2590]\ttraining's rmse: 4199.48\tvalid_1's rmse: 10135.1\n",
      "[2600]\ttraining's rmse: 4194.57\tvalid_1's rmse: 10133.3\n",
      "[2610]\ttraining's rmse: 4190.42\tvalid_1's rmse: 10132.6\n",
      "[2620]\ttraining's rmse: 4185.65\tvalid_1's rmse: 10131.8\n",
      "[2630]\ttraining's rmse: 4181.5\tvalid_1's rmse: 10130.9\n",
      "[2640]\ttraining's rmse: 4176.57\tvalid_1's rmse: 10129.3\n",
      "[2650]\ttraining's rmse: 4171.93\tvalid_1's rmse: 10127.9\n",
      "[2660]\ttraining's rmse: 4167.28\tvalid_1's rmse: 10126.5\n",
      "[2670]\ttraining's rmse: 4162.25\tvalid_1's rmse: 10124.1\n",
      "[2680]\ttraining's rmse: 4158.45\tvalid_1's rmse: 10123.7\n",
      "[2690]\ttraining's rmse: 4154.73\tvalid_1's rmse: 10123.1\n",
      "[2700]\ttraining's rmse: 4150.47\tvalid_1's rmse: 10122.3\n",
      "[2710]\ttraining's rmse: 4145.55\tvalid_1's rmse: 10120.2\n",
      "[2720]\ttraining's rmse: 4140.52\tvalid_1's rmse: 10120.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "# TimeSeriesSplit을 사용한 시계열 교차 검증\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.010204448913134991,\n",
    "    'num_leaves': 128,\n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 8,\n",
    "    'subsample': 0.5668227534307944,\n",
    "    'colsample_bytree': 0.6033862727269486,\n",
    "    'reg_alpha': 2.5906244698002496e-06,\n",
    "    'reg_lambda': 2.979573198070953e-05,\n",
    "    'force_row_wise': True,\n",
    "}\n",
    "\n",
    "# K-Fold 교차 검증 실행\n",
    "fold = 1\n",
    "rmse_scores = []\n",
    "\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = lgb.train(params, dtrain, num_boost_round=100000, valid_sets=[dtrain, dval], valid_names=['train', 'valid'], early_stopping_rounds=50, verbose_eval=10)\n",
    "    \n",
    "    # 예측 및 평가\n",
    "    val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "    rmse_scores.append(val_rmse)\n",
    "    print(f\"Fold {fold}: Validation RMSE = {val_rmse}\")\n",
    "    fold += 1\n",
    "\n",
    "print(f\"Mean Validation RMSE: {np.mean(rmse_scores)}\")\n",
    "\n",
    "# 최종 모델 학습 (전체 데이터를 사용)\n",
    "dtrain_final = lgb.Dataset(X, label=y)\n",
    "final_model = lgb.train(params, dtrain_final, num_boost_round=1000)\n",
    "\n",
    "# 미래 데이터 예측 (2023년 7월부터 9월까지)\n",
    "# X_future = pd.read_csv('future_data.csv')\n",
    "# future_preds = final_model.predict(X_future)\n",
    "\n",
    "# 결과 출력\n",
    "# print(\"Future Predictions:\")\n",
    "# print(future_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGBM + Stratified KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGBM + Random seed KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Seed 0 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2887\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.553091\n",
      "[100]\ttraining's rmse: 18200.9\tvalid_1's rmse: 18597.9\n",
      "[200]\ttraining's rmse: 14842.1\tvalid_1's rmse: 15458.4\n",
      "[300]\ttraining's rmse: 13176.1\tvalid_1's rmse: 13920.9\n",
      "[400]\ttraining's rmse: 12136.8\tvalid_1's rmse: 12985.3\n",
      "[500]\ttraining's rmse: 11392.5\tvalid_1's rmse: 12327.1\n",
      "[600]\ttraining's rmse: 10825.9\tvalid_1's rmse: 11851.9\n",
      "[700]\ttraining's rmse: 10328.6\tvalid_1's rmse: 11432.6\n",
      "[800]\ttraining's rmse: 9903.71\tvalid_1's rmse: 11083\n",
      "[900]\ttraining's rmse: 9567.46\tvalid_1's rmse: 10812.9\n",
      "[1000]\ttraining's rmse: 9271.44\tvalid_1's rmse: 10589.7\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.524724\n",
      "[100]\ttraining's rmse: 18225.7\tvalid_1's rmse: 18872.8\n",
      "[200]\ttraining's rmse: 14809.3\tvalid_1's rmse: 15544.2\n",
      "[300]\ttraining's rmse: 13215.3\tvalid_1's rmse: 14024.9\n",
      "[400]\ttraining's rmse: 12191.8\tvalid_1's rmse: 13089.1\n",
      "[500]\ttraining's rmse: 11429.9\tvalid_1's rmse: 12413.6\n",
      "[600]\ttraining's rmse: 10845.4\tvalid_1's rmse: 11927.3\n",
      "[700]\ttraining's rmse: 10360.2\tvalid_1's rmse: 11529.3\n",
      "[800]\ttraining's rmse: 9949.12\tvalid_1's rmse: 11205.1\n",
      "[900]\ttraining's rmse: 9582.72\tvalid_1's rmse: 10918.7\n",
      "[1000]\ttraining's rmse: 9282.88\tvalid_1's rmse: 10695.1\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2885\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.164729\n",
      "[100]\ttraining's rmse: 18169.7\tvalid_1's rmse: 18183.6\n",
      "[200]\ttraining's rmse: 14856.3\tvalid_1's rmse: 15065.3\n",
      "[300]\ttraining's rmse: 13208.9\tvalid_1's rmse: 13577.1\n",
      "[400]\ttraining's rmse: 12148.8\tvalid_1's rmse: 12650.1\n",
      "[500]\ttraining's rmse: 11414.2\tvalid_1's rmse: 12035.8\n",
      "[600]\ttraining's rmse: 10807.5\tvalid_1's rmse: 11535.3\n",
      "[700]\ttraining's rmse: 10334.1\tvalid_1's rmse: 11168.9\n",
      "[800]\ttraining's rmse: 9915.31\tvalid_1's rmse: 10850.4\n",
      "[900]\ttraining's rmse: 9559.5\tvalid_1's rmse: 10592.2\n",
      "[1000]\ttraining's rmse: 9260.89\tvalid_1's rmse: 10373.9\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2887\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91038.836778\n",
      "[100]\ttraining's rmse: 18180.4\tvalid_1's rmse: 18545.6\n",
      "[200]\ttraining's rmse: 14796.6\tvalid_1's rmse: 15381.9\n",
      "[300]\ttraining's rmse: 13199\tvalid_1's rmse: 13898.1\n",
      "[400]\ttraining's rmse: 12183.5\tvalid_1's rmse: 13008.8\n",
      "[500]\ttraining's rmse: 11406.9\tvalid_1's rmse: 12317.6\n",
      "[600]\ttraining's rmse: 10812.4\tvalid_1's rmse: 11812.1\n",
      "[700]\ttraining's rmse: 10335\tvalid_1's rmse: 11417.4\n",
      "[800]\ttraining's rmse: 9921.7\tvalid_1's rmse: 11093.1\n",
      "[900]\ttraining's rmse: 9567.29\tvalid_1's rmse: 10819.5\n",
      "[1000]\ttraining's rmse: 9271.71\tvalid_1's rmse: 10596.5\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2891\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.606423\n",
      "[100]\ttraining's rmse: 18299\tvalid_1's rmse: 18656.6\n",
      "[200]\ttraining's rmse: 14910.1\tvalid_1's rmse: 15441.9\n",
      "[300]\ttraining's rmse: 13232.4\tvalid_1's rmse: 13884.2\n",
      "[400]\ttraining's rmse: 12178.2\tvalid_1's rmse: 12945\n",
      "[500]\ttraining's rmse: 11402\tvalid_1's rmse: 12278.9\n",
      "[600]\ttraining's rmse: 10824.3\tvalid_1's rmse: 11802.1\n",
      "[700]\ttraining's rmse: 10311\tvalid_1's rmse: 11387.6\n",
      "[800]\ttraining's rmse: 9899.48\tvalid_1's rmse: 11067.9\n",
      "[900]\ttraining's rmse: 9525.62\tvalid_1's rmse: 10766.1\n",
      "[1000]\ttraining's rmse: 9215.93\tvalid_1's rmse: 10542.7\n",
      "Seed 0 OOF RMSE: 10560.101000969802\n",
      "-------- Seed 1 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.436011\n",
      "[100]\ttraining's rmse: 18145.7\tvalid_1's rmse: 18431.7\n",
      "[200]\ttraining's rmse: 14843.3\tvalid_1's rmse: 15362.4\n",
      "[300]\ttraining's rmse: 13242.4\tvalid_1's rmse: 13896.5\n",
      "[400]\ttraining's rmse: 12137.5\tvalid_1's rmse: 12886.7\n",
      "[500]\ttraining's rmse: 11386.1\tvalid_1's rmse: 12222.3\n",
      "[600]\ttraining's rmse: 10791.6\tvalid_1's rmse: 11721.5\n",
      "[700]\ttraining's rmse: 10327.2\tvalid_1's rmse: 11351.2\n",
      "[800]\ttraining's rmse: 9898.15\tvalid_1's rmse: 11000.8\n",
      "[900]\ttraining's rmse: 9553.61\tvalid_1's rmse: 10741.2\n",
      "[1000]\ttraining's rmse: 9241.63\tvalid_1's rmse: 10504.5\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.574243\n",
      "[100]\ttraining's rmse: 18157.6\tvalid_1's rmse: 18515.7\n",
      "[200]\ttraining's rmse: 14827.9\tvalid_1's rmse: 15350\n",
      "[300]\ttraining's rmse: 13222.8\tvalid_1's rmse: 13840.9\n",
      "[400]\ttraining's rmse: 12173.8\tvalid_1's rmse: 12875.6\n",
      "[500]\ttraining's rmse: 11418.9\tvalid_1's rmse: 12205.8\n",
      "[600]\ttraining's rmse: 10802.9\tvalid_1's rmse: 11669.9\n",
      "[700]\ttraining's rmse: 10329.1\tvalid_1's rmse: 11274.2\n",
      "[800]\ttraining's rmse: 9912.13\tvalid_1's rmse: 10941.7\n",
      "[900]\ttraining's rmse: 9563.81\tvalid_1's rmse: 10675.7\n",
      "[1000]\ttraining's rmse: 9267.02\tvalid_1's rmse: 10456.2\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2886\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.089785\n",
      "[100]\ttraining's rmse: 18126.2\tvalid_1's rmse: 18821.8\n",
      "[200]\ttraining's rmse: 14742.5\tvalid_1's rmse: 15620\n",
      "[300]\ttraining's rmse: 13192.2\tvalid_1's rmse: 14195.8\n",
      "[400]\ttraining's rmse: 12112.7\tvalid_1's rmse: 13220.9\n",
      "[500]\ttraining's rmse: 11354.6\tvalid_1's rmse: 12563\n",
      "[600]\ttraining's rmse: 10760.5\tvalid_1's rmse: 12061\n",
      "[700]\ttraining's rmse: 10280.9\tvalid_1's rmse: 11678\n",
      "[800]\ttraining's rmse: 9881.5\tvalid_1's rmse: 11343.3\n",
      "[900]\ttraining's rmse: 9533.27\tvalid_1's rmse: 11057.9\n",
      "[1000]\ttraining's rmse: 9227.06\tvalid_1's rmse: 10817.3\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2891\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.508706\n",
      "[100]\ttraining's rmse: 18150\tvalid_1's rmse: 18489.4\n",
      "[200]\ttraining's rmse: 14825.3\tvalid_1's rmse: 15378.8\n",
      "[300]\ttraining's rmse: 13187.8\tvalid_1's rmse: 13891.9\n",
      "[400]\ttraining's rmse: 12116.5\tvalid_1's rmse: 12911.2\n",
      "[500]\ttraining's rmse: 11386.8\tvalid_1's rmse: 12280.2\n",
      "[600]\ttraining's rmse: 10789.3\tvalid_1's rmse: 11765.1\n",
      "[700]\ttraining's rmse: 10300.2\tvalid_1's rmse: 11352\n",
      "[800]\ttraining's rmse: 9904.42\tvalid_1's rmse: 11035.3\n",
      "[900]\ttraining's rmse: 9554.88\tvalid_1's rmse: 10765.8\n",
      "[1000]\ttraining's rmse: 9246.68\tvalid_1's rmse: 10539.9\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.077000\n",
      "[100]\ttraining's rmse: 18258.1\tvalid_1's rmse: 18258.6\n",
      "[200]\ttraining's rmse: 14821.6\tvalid_1's rmse: 14996.1\n",
      "[300]\ttraining's rmse: 13176.1\tvalid_1's rmse: 13499.3\n",
      "[400]\ttraining's rmse: 12140.3\tvalid_1's rmse: 12573.9\n",
      "[500]\ttraining's rmse: 11415.3\tvalid_1's rmse: 11946.5\n",
      "[600]\ttraining's rmse: 10814.5\tvalid_1's rmse: 11451.8\n",
      "[700]\ttraining's rmse: 10350.8\tvalid_1's rmse: 11077.7\n",
      "[800]\ttraining's rmse: 9922.67\tvalid_1's rmse: 10750.3\n",
      "[900]\ttraining's rmse: 9570.02\tvalid_1's rmse: 10486\n",
      "[1000]\ttraining's rmse: 9260.74\tvalid_1's rmse: 10259.1\n",
      "Seed 1 OOF RMSE: 10516.91266241927\n",
      "-------- Seed 2 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.491084\n",
      "[100]\ttraining's rmse: 18207.6\tvalid_1's rmse: 18807.1\n",
      "[200]\ttraining's rmse: 14830.2\tvalid_1's rmse: 15565.2\n",
      "[300]\ttraining's rmse: 13227.7\tvalid_1's rmse: 14070.2\n",
      "[400]\ttraining's rmse: 12179.7\tvalid_1's rmse: 13113.5\n",
      "[500]\ttraining's rmse: 11416.7\tvalid_1's rmse: 12434.2\n",
      "[600]\ttraining's rmse: 10814.7\tvalid_1's rmse: 11906.1\n",
      "[700]\ttraining's rmse: 10368.7\tvalid_1's rmse: 11539.7\n",
      "[800]\ttraining's rmse: 9969.35\tvalid_1's rmse: 11223.5\n",
      "[900]\ttraining's rmse: 9604.2\tvalid_1's rmse: 10934.6\n",
      "[1000]\ttraining's rmse: 9295.83\tvalid_1's rmse: 10701.5\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.123980\n",
      "[100]\ttraining's rmse: 18234\tvalid_1's rmse: 18670.1\n",
      "[200]\ttraining's rmse: 14858.6\tvalid_1's rmse: 15512.5\n",
      "[300]\ttraining's rmse: 13237.8\tvalid_1's rmse: 14040.7\n",
      "[400]\ttraining's rmse: 12188.3\tvalid_1's rmse: 13126.9\n",
      "[500]\ttraining's rmse: 11435.6\tvalid_1's rmse: 12477.3\n",
      "[600]\ttraining's rmse: 10840.4\tvalid_1's rmse: 11974.5\n",
      "[700]\ttraining's rmse: 10353.2\tvalid_1's rmse: 11574\n",
      "[800]\ttraining's rmse: 9948.95\tvalid_1's rmse: 11263.8\n",
      "[900]\ttraining's rmse: 9587.65\tvalid_1's rmse: 10984.6\n",
      "[1000]\ttraining's rmse: 9275.18\tvalid_1's rmse: 10747.1\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.000459\n",
      "[100]\ttraining's rmse: 18202.1\tvalid_1's rmse: 18440.7\n",
      "[200]\ttraining's rmse: 14859.6\tvalid_1's rmse: 15229.8\n",
      "[300]\ttraining's rmse: 13244.9\tvalid_1's rmse: 13744\n",
      "[400]\ttraining's rmse: 12204.4\tvalid_1's rmse: 12827.5\n",
      "[500]\ttraining's rmse: 11444.3\tvalid_1's rmse: 12175.3\n",
      "[600]\ttraining's rmse: 10838.6\tvalid_1's rmse: 11662.2\n",
      "[700]\ttraining's rmse: 10347.6\tvalid_1's rmse: 11269\n",
      "[800]\ttraining's rmse: 9950.48\tvalid_1's rmse: 10947.5\n",
      "[900]\ttraining's rmse: 9612.3\tvalid_1's rmse: 10690.6\n",
      "[1000]\ttraining's rmse: 9307.66\tvalid_1's rmse: 10462.6\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2885\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.354085\n",
      "[100]\ttraining's rmse: 18183.9\tvalid_1's rmse: 18305.8\n",
      "[200]\ttraining's rmse: 14798.3\tvalid_1's rmse: 15216.7\n",
      "[300]\ttraining's rmse: 13196.7\tvalid_1's rmse: 13794.8\n",
      "[400]\ttraining's rmse: 12171\tvalid_1's rmse: 12897.8\n",
      "[500]\ttraining's rmse: 11402.8\tvalid_1's rmse: 12249\n",
      "[600]\ttraining's rmse: 10812.6\tvalid_1's rmse: 11773.1\n",
      "[700]\ttraining's rmse: 10307.6\tvalid_1's rmse: 11362.2\n",
      "[800]\ttraining's rmse: 9896.34\tvalid_1's rmse: 11028.7\n",
      "[900]\ttraining's rmse: 9560.07\tvalid_1's rmse: 10763.3\n",
      "[1000]\ttraining's rmse: 9243.59\tvalid_1's rmse: 10528.8\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2887\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91038.716135\n",
      "[100]\ttraining's rmse: 18180.9\tvalid_1's rmse: 18486.8\n",
      "[200]\ttraining's rmse: 14785.2\tvalid_1's rmse: 15242.3\n",
      "[300]\ttraining's rmse: 13224.1\tvalid_1's rmse: 13819.3\n",
      "[400]\ttraining's rmse: 12166.3\tvalid_1's rmse: 12877.7\n",
      "[500]\ttraining's rmse: 11420.2\tvalid_1's rmse: 12232.6\n",
      "[600]\ttraining's rmse: 10816.4\tvalid_1's rmse: 11729.6\n",
      "[700]\ttraining's rmse: 10319.9\tvalid_1's rmse: 11323.6\n",
      "[800]\ttraining's rmse: 9919.38\tvalid_1's rmse: 11005.4\n",
      "[900]\ttraining's rmse: 9545.72\tvalid_1's rmse: 10722.1\n",
      "[1000]\ttraining's rmse: 9223.12\tvalid_1's rmse: 10483.4\n",
      "Seed 2 OOF RMSE: 10585.326432167583\n",
      "-------- Seed 3 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.281567\n",
      "[100]\ttraining's rmse: 18184.7\tvalid_1's rmse: 18645\n",
      "[200]\ttraining's rmse: 14820.5\tvalid_1's rmse: 15477.5\n",
      "[300]\ttraining's rmse: 13210.3\tvalid_1's rmse: 13996.9\n",
      "[400]\ttraining's rmse: 12173\tvalid_1's rmse: 13076.4\n",
      "[500]\ttraining's rmse: 11402.4\tvalid_1's rmse: 12405.8\n",
      "[600]\ttraining's rmse: 10804.1\tvalid_1's rmse: 11892.8\n",
      "[700]\ttraining's rmse: 10333\tvalid_1's rmse: 11520.2\n",
      "[800]\ttraining's rmse: 9936.74\tvalid_1's rmse: 11219\n",
      "[900]\ttraining's rmse: 9582.68\tvalid_1's rmse: 10944.9\n",
      "[1000]\ttraining's rmse: 9258.87\tvalid_1's rmse: 10705.7\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.707590\n",
      "[100]\ttraining's rmse: 18185.9\tvalid_1's rmse: 18701.6\n",
      "[200]\ttraining's rmse: 14836.4\tvalid_1's rmse: 15529.6\n",
      "[300]\ttraining's rmse: 13256.9\tvalid_1's rmse: 14037\n",
      "[400]\ttraining's rmse: 12139.4\tvalid_1's rmse: 13014.7\n",
      "[500]\ttraining's rmse: 11390.2\tvalid_1's rmse: 12361.3\n",
      "[600]\ttraining's rmse: 10776.6\tvalid_1's rmse: 11842.2\n",
      "[700]\ttraining's rmse: 10294.6\tvalid_1's rmse: 11448.2\n",
      "[800]\ttraining's rmse: 9902.48\tvalid_1's rmse: 11139.5\n",
      "[900]\ttraining's rmse: 9545.23\tvalid_1's rmse: 10867.4\n",
      "[1000]\ttraining's rmse: 9230.66\tvalid_1's rmse: 10619.5\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2886\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.290249\n",
      "[100]\ttraining's rmse: 18146.6\tvalid_1's rmse: 18356.1\n",
      "[200]\ttraining's rmse: 14806.4\tvalid_1's rmse: 15122.2\n",
      "[300]\ttraining's rmse: 13123.1\tvalid_1's rmse: 13581.5\n",
      "[400]\ttraining's rmse: 12085.4\tvalid_1's rmse: 12674.4\n",
      "[500]\ttraining's rmse: 11341.2\tvalid_1's rmse: 12050.8\n",
      "[600]\ttraining's rmse: 10734\tvalid_1's rmse: 11552.1\n",
      "[700]\ttraining's rmse: 10257.7\tvalid_1's rmse: 11180.7\n",
      "[800]\ttraining's rmse: 9870.58\tvalid_1's rmse: 10881.7\n",
      "[900]\ttraining's rmse: 9520.37\tvalid_1's rmse: 10601.6\n",
      "[1000]\ttraining's rmse: 9213.37\tvalid_1's rmse: 10374.8\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.257561\n",
      "[100]\ttraining's rmse: 18160.4\tvalid_1's rmse: 18677.9\n",
      "[200]\ttraining's rmse: 14761.7\tvalid_1's rmse: 15443.4\n",
      "[300]\ttraining's rmse: 13179.5\tvalid_1's rmse: 13960.8\n",
      "[400]\ttraining's rmse: 12144.7\tvalid_1's rmse: 13008.4\n",
      "[500]\ttraining's rmse: 11371.3\tvalid_1's rmse: 12321.9\n",
      "[600]\ttraining's rmse: 10793.5\tvalid_1's rmse: 11831.6\n",
      "[700]\ttraining's rmse: 10301.5\tvalid_1's rmse: 11420.9\n",
      "[800]\ttraining's rmse: 9882.82\tvalid_1's rmse: 11072.3\n",
      "[900]\ttraining's rmse: 9541.58\tvalid_1's rmse: 10794.9\n",
      "[1000]\ttraining's rmse: 9252.8\tvalid_1's rmse: 10574\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2884\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.148776\n",
      "[100]\ttraining's rmse: 18164.9\tvalid_1's rmse: 18231.5\n",
      "[200]\ttraining's rmse: 14800.2\tvalid_1's rmse: 15110.5\n",
      "[300]\ttraining's rmse: 13160.8\tvalid_1's rmse: 13629\n",
      "[400]\ttraining's rmse: 12108.4\tvalid_1's rmse: 12705\n",
      "[500]\ttraining's rmse: 11372.6\tvalid_1's rmse: 12084\n",
      "[600]\ttraining's rmse: 10776.9\tvalid_1's rmse: 11571.9\n",
      "[700]\ttraining's rmse: 10290.1\tvalid_1's rmse: 11170.7\n",
      "[800]\ttraining's rmse: 9890.86\tvalid_1's rmse: 10861.8\n",
      "[900]\ttraining's rmse: 9548.3\tvalid_1's rmse: 10594.4\n",
      "[1000]\ttraining's rmse: 9239.58\tvalid_1's rmse: 10369.5\n",
      "Seed 3 OOF RMSE: 10529.47373589046\n",
      "-------- Seed 4 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2886\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.472697\n",
      "[100]\ttraining's rmse: 18195.9\tvalid_1's rmse: 18328.1\n",
      "[200]\ttraining's rmse: 14878.3\tvalid_1's rmse: 15185.8\n",
      "[300]\ttraining's rmse: 13249.2\tvalid_1's rmse: 13648.9\n",
      "[400]\ttraining's rmse: 12197.1\tvalid_1's rmse: 12695.6\n",
      "[500]\ttraining's rmse: 11435.4\tvalid_1's rmse: 12023.1\n",
      "[600]\ttraining's rmse: 10840.8\tvalid_1's rmse: 11528.6\n",
      "[700]\ttraining's rmse: 10379.7\tvalid_1's rmse: 11148.3\n",
      "[800]\ttraining's rmse: 9969.55\tvalid_1's rmse: 10826.1\n",
      "[900]\ttraining's rmse: 9624.4\tvalid_1's rmse: 10564.5\n",
      "[1000]\ttraining's rmse: 9311.13\tvalid_1's rmse: 10339.4\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2893\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.343606\n",
      "[100]\ttraining's rmse: 18150\tvalid_1's rmse: 18620.2\n",
      "[200]\ttraining's rmse: 14835.3\tvalid_1's rmse: 15515.9\n",
      "[300]\ttraining's rmse: 13225.2\tvalid_1's rmse: 14069.5\n",
      "[400]\ttraining's rmse: 12182.7\tvalid_1's rmse: 13156.3\n",
      "[500]\ttraining's rmse: 11402.8\tvalid_1's rmse: 12493.4\n",
      "[600]\ttraining's rmse: 10811.5\tvalid_1's rmse: 12015.7\n",
      "[700]\ttraining's rmse: 10315.3\tvalid_1's rmse: 11617.1\n",
      "[800]\ttraining's rmse: 9911.66\tvalid_1's rmse: 11302.6\n",
      "[900]\ttraining's rmse: 9555.91\tvalid_1's rmse: 11044.1\n",
      "[1000]\ttraining's rmse: 9235.44\tvalid_1's rmse: 10814\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.147003\n",
      "[100]\ttraining's rmse: 18204.5\tvalid_1's rmse: 18741.7\n",
      "[200]\ttraining's rmse: 14810.9\tvalid_1's rmse: 15471.7\n",
      "[300]\ttraining's rmse: 13220.2\tvalid_1's rmse: 13994.2\n",
      "[400]\ttraining's rmse: 12130.5\tvalid_1's rmse: 13017.2\n",
      "[500]\ttraining's rmse: 11370.5\tvalid_1's rmse: 12340.8\n",
      "[600]\ttraining's rmse: 10772.3\tvalid_1's rmse: 11825.9\n",
      "[700]\ttraining's rmse: 10289\tvalid_1's rmse: 11420\n",
      "[800]\ttraining's rmse: 9903.95\tvalid_1's rmse: 11117.3\n",
      "[900]\ttraining's rmse: 9551.07\tvalid_1's rmse: 10837.6\n",
      "[1000]\ttraining's rmse: 9245.38\tvalid_1's rmse: 10602.9\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91038.976010\n",
      "[100]\ttraining's rmse: 18255.6\tvalid_1's rmse: 18392.1\n",
      "[200]\ttraining's rmse: 14854\tvalid_1's rmse: 15189.5\n",
      "[300]\ttraining's rmse: 13281.7\tvalid_1's rmse: 13755.6\n",
      "[400]\ttraining's rmse: 12253.3\tvalid_1's rmse: 12846.9\n",
      "[500]\ttraining's rmse: 11488.9\tvalid_1's rmse: 12194.8\n",
      "[600]\ttraining's rmse: 10904.9\tvalid_1's rmse: 11702.7\n",
      "[700]\ttraining's rmse: 10397.7\tvalid_1's rmse: 11271.2\n",
      "[800]\ttraining's rmse: 9979.32\tvalid_1's rmse: 10932.5\n",
      "[900]\ttraining's rmse: 9617.59\tvalid_1's rmse: 10651.9\n",
      "[1000]\ttraining's rmse: 9320.36\tvalid_1's rmse: 10437.7\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91038.746429\n",
      "[100]\ttraining's rmse: 18175.1\tvalid_1's rmse: 18661.2\n",
      "[200]\ttraining's rmse: 14811.7\tvalid_1's rmse: 15492.6\n",
      "[300]\ttraining's rmse: 13234.2\tvalid_1's rmse: 14039.2\n",
      "[400]\ttraining's rmse: 12160.1\tvalid_1's rmse: 13089.4\n",
      "[500]\ttraining's rmse: 11389.6\tvalid_1's rmse: 12434\n",
      "[600]\ttraining's rmse: 10789.2\tvalid_1's rmse: 11933.2\n",
      "[700]\ttraining's rmse: 10309.8\tvalid_1's rmse: 11535.9\n",
      "[800]\ttraining's rmse: 9903.78\tvalid_1's rmse: 11209.7\n",
      "[900]\ttraining's rmse: 9566.57\tvalid_1's rmse: 10957.8\n",
      "[1000]\ttraining's rmse: 9262.75\tvalid_1's rmse: 10728.6\n",
      "Seed 4 OOF RMSE: 10585.977730144929\n",
      "-------- Seed 5 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.254361\n",
      "[100]\ttraining's rmse: 18128\tvalid_1's rmse: 18382.6\n",
      "[200]\ttraining's rmse: 14767.9\tvalid_1's rmse: 15142.5\n",
      "[300]\ttraining's rmse: 13153.4\tvalid_1's rmse: 13650.2\n",
      "[400]\ttraining's rmse: 12089.9\tvalid_1's rmse: 12697.1\n",
      "[500]\ttraining's rmse: 11355.6\tvalid_1's rmse: 12075.1\n",
      "[600]\ttraining's rmse: 10772.1\tvalid_1's rmse: 11592.2\n",
      "[700]\ttraining's rmse: 10303.7\tvalid_1's rmse: 11207.1\n",
      "[800]\ttraining's rmse: 9906.33\tvalid_1's rmse: 10894.7\n",
      "[900]\ttraining's rmse: 9567.13\tvalid_1's rmse: 10644.6\n",
      "[1000]\ttraining's rmse: 9255.29\tvalid_1's rmse: 10412.2\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2886\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.882541\n",
      "[100]\ttraining's rmse: 18130.4\tvalid_1's rmse: 18473.2\n",
      "[200]\ttraining's rmse: 14782.9\tvalid_1's rmse: 15352.8\n",
      "[300]\ttraining's rmse: 13192.4\tvalid_1's rmse: 13912.2\n",
      "[400]\ttraining's rmse: 12165.3\tvalid_1's rmse: 12997.8\n",
      "[500]\ttraining's rmse: 11407.4\tvalid_1's rmse: 12326.9\n",
      "[600]\ttraining's rmse: 10811.5\tvalid_1's rmse: 11812.4\n",
      "[700]\ttraining's rmse: 10304.2\tvalid_1's rmse: 11385.8\n",
      "[800]\ttraining's rmse: 9927.5\tvalid_1's rmse: 11082.4\n",
      "[900]\ttraining's rmse: 9582.68\tvalid_1's rmse: 10819.9\n",
      "[1000]\ttraining's rmse: 9281.87\tvalid_1's rmse: 10578\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.198264\n",
      "[100]\ttraining's rmse: 18119.3\tvalid_1's rmse: 18571.7\n",
      "[200]\ttraining's rmse: 14771.5\tvalid_1's rmse: 15439.8\n",
      "[300]\ttraining's rmse: 13198.3\tvalid_1's rmse: 13985.2\n",
      "[400]\ttraining's rmse: 12170.2\tvalid_1's rmse: 13076.8\n",
      "[500]\ttraining's rmse: 11402.8\tvalid_1's rmse: 12423.5\n",
      "[600]\ttraining's rmse: 10802.9\tvalid_1's rmse: 11916.3\n",
      "[700]\ttraining's rmse: 10287.2\tvalid_1's rmse: 11504.6\n",
      "[800]\ttraining's rmse: 9868.61\tvalid_1's rmse: 11172.6\n",
      "[900]\ttraining's rmse: 9513.12\tvalid_1's rmse: 10900.2\n",
      "[1000]\ttraining's rmse: 9221.42\tvalid_1's rmse: 10678.8\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.488706\n",
      "[100]\ttraining's rmse: 18225.9\tvalid_1's rmse: 18768.5\n",
      "[200]\ttraining's rmse: 14795.6\tvalid_1's rmse: 15394.7\n",
      "[300]\ttraining's rmse: 13223.7\tvalid_1's rmse: 13924\n",
      "[400]\ttraining's rmse: 12168.4\tvalid_1's rmse: 12957\n",
      "[500]\ttraining's rmse: 11431.1\tvalid_1's rmse: 12318\n",
      "[600]\ttraining's rmse: 10830.3\tvalid_1's rmse: 11800.7\n",
      "[700]\ttraining's rmse: 10338.6\tvalid_1's rmse: 11382\n",
      "[800]\ttraining's rmse: 9929.84\tvalid_1's rmse: 11059.7\n",
      "[900]\ttraining's rmse: 9568.7\tvalid_1's rmse: 10779.7\n",
      "[1000]\ttraining's rmse: 9262.33\tvalid_1's rmse: 10541.4\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91038.861872\n",
      "[100]\ttraining's rmse: 18219.4\tvalid_1's rmse: 18534.2\n",
      "[200]\ttraining's rmse: 14786.8\tvalid_1's rmse: 15171.6\n",
      "[300]\ttraining's rmse: 13228.7\tvalid_1's rmse: 13701.7\n",
      "[400]\ttraining's rmse: 12178.1\tvalid_1's rmse: 12752.6\n",
      "[500]\ttraining's rmse: 11457.8\tvalid_1's rmse: 12140.5\n",
      "[600]\ttraining's rmse: 10835.9\tvalid_1's rmse: 11616.5\n",
      "[700]\ttraining's rmse: 10354.5\tvalid_1's rmse: 11224.5\n",
      "[800]\ttraining's rmse: 9948.45\tvalid_1's rmse: 10903.6\n",
      "[900]\ttraining's rmse: 9612.79\tvalid_1's rmse: 10629.2\n",
      "[1000]\ttraining's rmse: 9286.58\tvalid_1's rmse: 10384.2\n",
      "Seed 5 OOF RMSE: 10519.484211431583\n",
      "-------- Seed 6 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2885\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.503160\n",
      "[100]\ttraining's rmse: 18189.3\tvalid_1's rmse: 18886.6\n",
      "[200]\ttraining's rmse: 14800.7\tvalid_1's rmse: 15606.2\n",
      "[300]\ttraining's rmse: 13188\tvalid_1's rmse: 14113.7\n",
      "[400]\ttraining's rmse: 12163.7\tvalid_1's rmse: 13184.4\n",
      "[500]\ttraining's rmse: 11396.1\tvalid_1's rmse: 12499.7\n",
      "[600]\ttraining's rmse: 10806.3\tvalid_1's rmse: 11998.9\n",
      "[700]\ttraining's rmse: 10327\tvalid_1's rmse: 11607.4\n",
      "[800]\ttraining's rmse: 9934.99\tvalid_1's rmse: 11286.8\n",
      "[900]\ttraining's rmse: 9598.1\tvalid_1's rmse: 11024.9\n",
      "[1000]\ttraining's rmse: 9288.02\tvalid_1's rmse: 10783.8\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.899179\n",
      "[100]\ttraining's rmse: 18109.6\tvalid_1's rmse: 18493.8\n",
      "[200]\ttraining's rmse: 14763.1\tvalid_1's rmse: 15370.3\n",
      "[300]\ttraining's rmse: 13215.9\tvalid_1's rmse: 13953\n",
      "[400]\ttraining's rmse: 12153\tvalid_1's rmse: 13007.5\n",
      "[500]\ttraining's rmse: 11359.6\tvalid_1's rmse: 12303.9\n",
      "[600]\ttraining's rmse: 10774.9\tvalid_1's rmse: 11814.3\n",
      "[700]\ttraining's rmse: 10306.9\tvalid_1's rmse: 11431.9\n",
      "[800]\ttraining's rmse: 9881.15\tvalid_1's rmse: 11095\n",
      "[900]\ttraining's rmse: 9517.61\tvalid_1's rmse: 10812.3\n",
      "[1000]\ttraining's rmse: 9218.57\tvalid_1's rmse: 10599.7\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2892\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.332715\n",
      "[100]\ttraining's rmse: 18300.2\tvalid_1's rmse: 18583.3\n",
      "[200]\ttraining's rmse: 14873\tvalid_1's rmse: 15303.6\n",
      "[300]\ttraining's rmse: 13258.9\tvalid_1's rmse: 13785.4\n",
      "[400]\ttraining's rmse: 12196.6\tvalid_1's rmse: 12829.9\n",
      "[500]\ttraining's rmse: 11446.3\tvalid_1's rmse: 12158.3\n",
      "[600]\ttraining's rmse: 10847.2\tvalid_1's rmse: 11649.5\n",
      "[700]\ttraining's rmse: 10344.9\tvalid_1's rmse: 11234.7\n",
      "[800]\ttraining's rmse: 9962.28\tvalid_1's rmse: 10941.6\n",
      "[900]\ttraining's rmse: 9598.6\tvalid_1's rmse: 10656.2\n",
      "[1000]\ttraining's rmse: 9280.38\tvalid_1's rmse: 10403.9\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91038.937043\n",
      "[100]\ttraining's rmse: 18193.5\tvalid_1's rmse: 18535.8\n",
      "[200]\ttraining's rmse: 14845.7\tvalid_1's rmse: 15407.8\n",
      "[300]\ttraining's rmse: 13205.5\tvalid_1's rmse: 13912.9\n",
      "[400]\ttraining's rmse: 12169.6\tvalid_1's rmse: 12980.7\n",
      "[500]\ttraining's rmse: 11441.2\tvalid_1's rmse: 12343\n",
      "[600]\ttraining's rmse: 10828.6\tvalid_1's rmse: 11800.4\n",
      "[700]\ttraining's rmse: 10353.3\tvalid_1's rmse: 11401.3\n",
      "[800]\ttraining's rmse: 9942.39\tvalid_1's rmse: 11050\n",
      "[900]\ttraining's rmse: 9594.61\tvalid_1's rmse: 10775.7\n",
      "[1000]\ttraining's rmse: 9294.22\tvalid_1's rmse: 10529.5\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.013648\n",
      "[100]\ttraining's rmse: 18187.2\tvalid_1's rmse: 18251.7\n",
      "[200]\ttraining's rmse: 14847.2\tvalid_1's rmse: 15053.4\n",
      "[300]\ttraining's rmse: 13240.2\tvalid_1's rmse: 13558.8\n",
      "[400]\ttraining's rmse: 12177.5\tvalid_1's rmse: 12633\n",
      "[500]\ttraining's rmse: 11424.2\tvalid_1's rmse: 11980.1\n",
      "[600]\ttraining's rmse: 10809.7\tvalid_1's rmse: 11467.8\n",
      "[700]\ttraining's rmse: 10329\tvalid_1's rmse: 11090\n",
      "[800]\ttraining's rmse: 9932.84\tvalid_1's rmse: 10780.8\n",
      "[900]\ttraining's rmse: 9602.73\tvalid_1's rmse: 10536.8\n",
      "[1000]\ttraining's rmse: 9303.88\tvalid_1's rmse: 10320.6\n",
      "Seed 6 OOF RMSE: 10528.728839811221\n",
      "-------- Seed 7 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2892\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.331127\n",
      "[100]\ttraining's rmse: 18211.9\tvalid_1's rmse: 18529.3\n",
      "[200]\ttraining's rmse: 14845.3\tvalid_1's rmse: 15334.9\n",
      "[300]\ttraining's rmse: 13247.9\tvalid_1's rmse: 13866.1\n",
      "[400]\ttraining's rmse: 12189.4\tvalid_1's rmse: 12917.9\n",
      "[500]\ttraining's rmse: 11441.6\tvalid_1's rmse: 12268\n",
      "[600]\ttraining's rmse: 10853.2\tvalid_1's rmse: 11772.1\n",
      "[700]\ttraining's rmse: 10346.3\tvalid_1's rmse: 11341.2\n",
      "[800]\ttraining's rmse: 9958.85\tvalid_1's rmse: 11042.9\n",
      "[900]\ttraining's rmse: 9618.51\tvalid_1's rmse: 10769.5\n",
      "[1000]\ttraining's rmse: 9298.29\tvalid_1's rmse: 10515.4\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.404371\n",
      "[100]\ttraining's rmse: 18140.7\tvalid_1's rmse: 18494.6\n",
      "[200]\ttraining's rmse: 14828.8\tvalid_1's rmse: 15376.4\n",
      "[300]\ttraining's rmse: 13240.1\tvalid_1's rmse: 13886.8\n",
      "[400]\ttraining's rmse: 12164.9\tvalid_1's rmse: 12924.4\n",
      "[500]\ttraining's rmse: 11421.5\tvalid_1's rmse: 12270.3\n",
      "[600]\ttraining's rmse: 10820.2\tvalid_1's rmse: 11748.5\n",
      "[700]\ttraining's rmse: 10340.2\tvalid_1's rmse: 11355.1\n",
      "[800]\ttraining's rmse: 9949.79\tvalid_1's rmse: 11050.7\n",
      "[900]\ttraining's rmse: 9593.25\tvalid_1's rmse: 10770.5\n",
      "[1000]\ttraining's rmse: 9287.55\tvalid_1's rmse: 10549\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2891\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.470883\n",
      "[100]\ttraining's rmse: 18084.4\tvalid_1's rmse: 18463.5\n",
      "[200]\ttraining's rmse: 14796\tvalid_1's rmse: 15313.4\n",
      "[300]\ttraining's rmse: 13234.2\tvalid_1's rmse: 13835.3\n",
      "[400]\ttraining's rmse: 12217.2\tvalid_1's rmse: 12932.3\n",
      "[500]\ttraining's rmse: 11466.4\tvalid_1's rmse: 12277.3\n",
      "[600]\ttraining's rmse: 10870.9\tvalid_1's rmse: 11795.1\n",
      "[700]\ttraining's rmse: 10357\tvalid_1's rmse: 11377.9\n",
      "[800]\ttraining's rmse: 9935.45\tvalid_1's rmse: 11052.5\n",
      "[900]\ttraining's rmse: 9582.14\tvalid_1's rmse: 10785.7\n",
      "[1000]\ttraining's rmse: 9289.3\tvalid_1's rmse: 10578.2\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2886\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.206099\n",
      "[100]\ttraining's rmse: 18191.3\tvalid_1's rmse: 18397.9\n",
      "[200]\ttraining's rmse: 14805.9\tvalid_1's rmse: 15270.9\n",
      "[300]\ttraining's rmse: 13182.2\tvalid_1's rmse: 13821.9\n",
      "[400]\ttraining's rmse: 12131.5\tvalid_1's rmse: 12891.4\n",
      "[500]\ttraining's rmse: 11400.3\tvalid_1's rmse: 12271.1\n",
      "[600]\ttraining's rmse: 10787.9\tvalid_1's rmse: 11760.8\n",
      "[700]\ttraining's rmse: 10278.2\tvalid_1's rmse: 11334.7\n",
      "[800]\ttraining's rmse: 9863.07\tvalid_1's rmse: 11014.4\n",
      "[900]\ttraining's rmse: 9525.95\tvalid_1's rmse: 10754.5\n",
      "[1000]\ttraining's rmse: 9229.55\tvalid_1's rmse: 10536.3\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2888\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.273264\n",
      "[100]\ttraining's rmse: 18228.8\tvalid_1's rmse: 18829.6\n",
      "[200]\ttraining's rmse: 14823.9\tvalid_1's rmse: 15527.4\n",
      "[300]\ttraining's rmse: 13229.1\tvalid_1's rmse: 14042.1\n",
      "[400]\ttraining's rmse: 12179.9\tvalid_1's rmse: 13104\n",
      "[500]\ttraining's rmse: 11384.5\tvalid_1's rmse: 12400.8\n",
      "[600]\ttraining's rmse: 10803\tvalid_1's rmse: 11922.1\n",
      "[700]\ttraining's rmse: 10330.6\tvalid_1's rmse: 11528\n",
      "[800]\ttraining's rmse: 9951.1\tvalid_1's rmse: 11227.5\n",
      "[900]\ttraining's rmse: 9583.49\tvalid_1's rmse: 10934.1\n",
      "[1000]\ttraining's rmse: 9275.15\tvalid_1's rmse: 10701.4\n",
      "Seed 7 OOF RMSE: 10576.191853329796\n",
      "-------- Seed 8 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2887\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.186914\n",
      "[100]\ttraining's rmse: 18211.5\tvalid_1's rmse: 18480.6\n",
      "[200]\ttraining's rmse: 14847.6\tvalid_1's rmse: 15290.5\n",
      "[300]\ttraining's rmse: 13213\tvalid_1's rmse: 13800.2\n",
      "[400]\ttraining's rmse: 12161.5\tvalid_1's rmse: 12865.2\n",
      "[500]\ttraining's rmse: 11389.4\tvalid_1's rmse: 12194.5\n",
      "[600]\ttraining's rmse: 10797.9\tvalid_1's rmse: 11711.7\n",
      "[700]\ttraining's rmse: 10308.6\tvalid_1's rmse: 11336.8\n",
      "[800]\ttraining's rmse: 9906.6\tvalid_1's rmse: 11016\n",
      "[900]\ttraining's rmse: 9556.52\tvalid_1's rmse: 10755.1\n",
      "[1000]\ttraining's rmse: 9245.5\tvalid_1's rmse: 10525.4\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.709001\n",
      "[100]\ttraining's rmse: 18221\tvalid_1's rmse: 18735.2\n",
      "[200]\ttraining's rmse: 14919.3\tvalid_1's rmse: 15521.4\n",
      "[300]\ttraining's rmse: 13310\tvalid_1's rmse: 14003.8\n",
      "[400]\ttraining's rmse: 12199.4\tvalid_1's rmse: 12961.7\n",
      "[500]\ttraining's rmse: 11478.6\tvalid_1's rmse: 12292.1\n",
      "[600]\ttraining's rmse: 10856.5\tvalid_1's rmse: 11759.1\n",
      "[700]\ttraining's rmse: 10344.6\tvalid_1's rmse: 11338.5\n",
      "[800]\ttraining's rmse: 9937.01\tvalid_1's rmse: 11011.7\n",
      "[900]\ttraining's rmse: 9593.57\tvalid_1's rmse: 10738.1\n",
      "[1000]\ttraining's rmse: 9300.01\tvalid_1's rmse: 10518\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2891\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.243865\n",
      "[100]\ttraining's rmse: 18124\tvalid_1's rmse: 18599.5\n",
      "[200]\ttraining's rmse: 14826.9\tvalid_1's rmse: 15361.9\n",
      "[300]\ttraining's rmse: 13258.8\tvalid_1's rmse: 13892.7\n",
      "[400]\ttraining's rmse: 12186.9\tvalid_1's rmse: 12906.6\n",
      "[500]\ttraining's rmse: 11447.7\tvalid_1's rmse: 12254.4\n",
      "[600]\ttraining's rmse: 10858.9\tvalid_1's rmse: 11745.5\n",
      "[700]\ttraining's rmse: 10359.8\tvalid_1's rmse: 11331.6\n",
      "[800]\ttraining's rmse: 9940.94\tvalid_1's rmse: 11000.8\n",
      "[900]\ttraining's rmse: 9582.61\tvalid_1's rmse: 10718.1\n",
      "[1000]\ttraining's rmse: 9269.07\tvalid_1's rmse: 10475.7\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2890\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.219972\n",
      "[100]\ttraining's rmse: 18138\tvalid_1's rmse: 18270.1\n",
      "[200]\ttraining's rmse: 14839.7\tvalid_1's rmse: 15282.7\n",
      "[300]\ttraining's rmse: 13217.5\tvalid_1's rmse: 13875\n",
      "[400]\ttraining's rmse: 12159\tvalid_1's rmse: 12967.5\n",
      "[500]\ttraining's rmse: 11401.7\tvalid_1's rmse: 12330.5\n",
      "[600]\ttraining's rmse: 10785.3\tvalid_1's rmse: 11827.3\n",
      "[700]\ttraining's rmse: 10277.6\tvalid_1's rmse: 11428\n",
      "[800]\ttraining's rmse: 9887.91\tvalid_1's rmse: 11142.8\n",
      "[900]\ttraining's rmse: 9531.59\tvalid_1's rmse: 10878.4\n",
      "[1000]\ttraining's rmse: 9223.7\tvalid_1's rmse: 10662.4\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2886\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.325992\n",
      "[100]\ttraining's rmse: 18290.5\tvalid_1's rmse: 18710.3\n",
      "[200]\ttraining's rmse: 14900\tvalid_1's rmse: 15458.9\n",
      "[300]\ttraining's rmse: 13300.2\tvalid_1's rmse: 13992.2\n",
      "[400]\ttraining's rmse: 12188.6\tvalid_1's rmse: 12983.1\n",
      "[500]\ttraining's rmse: 11419.6\tvalid_1's rmse: 12301.5\n",
      "[600]\ttraining's rmse: 10838.9\tvalid_1's rmse: 11808.1\n",
      "[700]\ttraining's rmse: 10331.4\tvalid_1's rmse: 11372.7\n",
      "[800]\ttraining's rmse: 9923.15\tvalid_1's rmse: 11043.8\n",
      "[900]\ttraining's rmse: 9567.15\tvalid_1's rmse: 10770.2\n",
      "[1000]\ttraining's rmse: 9265.36\tvalid_1's rmse: 10549.9\n",
      "Seed 8 OOF RMSE: 10546.469830625041\n",
      "-------- Seed 9 --------\n",
      "-------- Fold 0 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2891\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.412037\n",
      "[100]\ttraining's rmse: 18172.3\tvalid_1's rmse: 18776\n",
      "[200]\ttraining's rmse: 14757.9\tvalid_1's rmse: 15598.8\n",
      "[300]\ttraining's rmse: 13157.8\tvalid_1's rmse: 14101.6\n",
      "[400]\ttraining's rmse: 12120.4\tvalid_1's rmse: 13148.3\n",
      "[500]\ttraining's rmse: 11374.7\tvalid_1's rmse: 12493.9\n",
      "[600]\ttraining's rmse: 10778.4\tvalid_1's rmse: 11994.9\n",
      "[700]\ttraining's rmse: 10312.9\tvalid_1's rmse: 11609.2\n",
      "[800]\ttraining's rmse: 9902.41\tvalid_1's rmse: 11285.7\n",
      "[900]\ttraining's rmse: 9527.56\tvalid_1's rmse: 10979\n",
      "[1000]\ttraining's rmse: 9216.26\tvalid_1's rmse: 10742.6\n",
      "-------- Fold 1 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2886\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91034.758625\n",
      "[100]\ttraining's rmse: 18226.7\tvalid_1's rmse: 18634.1\n",
      "[200]\ttraining's rmse: 14814.4\tvalid_1's rmse: 15372.3\n",
      "[300]\ttraining's rmse: 13222.1\tvalid_1's rmse: 13921.8\n",
      "[400]\ttraining's rmse: 12163.5\tvalid_1's rmse: 12980.7\n",
      "[500]\ttraining's rmse: 11413.6\tvalid_1's rmse: 12322.1\n",
      "[600]\ttraining's rmse: 10812.8\tvalid_1's rmse: 11823\n",
      "[700]\ttraining's rmse: 10342.7\tvalid_1's rmse: 11422.9\n",
      "[800]\ttraining's rmse: 9911.59\tvalid_1's rmse: 11079.4\n",
      "[900]\ttraining's rmse: 9570.81\tvalid_1's rmse: 10814.4\n",
      "[1000]\ttraining's rmse: 9256.56\tvalid_1's rmse: 10572.6\n",
      "-------- Fold 2 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91030.298101\n",
      "[100]\ttraining's rmse: 18228\tvalid_1's rmse: 18658.4\n",
      "[200]\ttraining's rmse: 14803.1\tvalid_1's rmse: 15368.9\n",
      "[300]\ttraining's rmse: 13212.8\tvalid_1's rmse: 13913.4\n",
      "[400]\ttraining's rmse: 12143\tvalid_1's rmse: 12958.6\n",
      "[500]\ttraining's rmse: 11386.6\tvalid_1's rmse: 12303\n",
      "[600]\ttraining's rmse: 10784.2\tvalid_1's rmse: 11796.4\n",
      "[700]\ttraining's rmse: 10295.8\tvalid_1's rmse: 11396.8\n",
      "[800]\ttraining's rmse: 9888.88\tvalid_1's rmse: 11067.8\n",
      "[900]\ttraining's rmse: 9540.53\tvalid_1's rmse: 10792.9\n",
      "[1000]\ttraining's rmse: 9238.81\tvalid_1's rmse: 10575.3\n",
      "-------- Fold 3 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91039.260544\n",
      "[100]\ttraining's rmse: 18237.7\tvalid_1's rmse: 18228.2\n",
      "[200]\ttraining's rmse: 14875.3\tvalid_1's rmse: 15055.4\n",
      "[300]\ttraining's rmse: 13254.2\tvalid_1's rmse: 13562.6\n",
      "[400]\ttraining's rmse: 12230.4\tvalid_1's rmse: 12632.3\n",
      "[500]\ttraining's rmse: 11469.2\tvalid_1's rmse: 11961.8\n",
      "[600]\ttraining's rmse: 10812\tvalid_1's rmse: 11385.8\n",
      "[700]\ttraining's rmse: 10343.2\tvalid_1's rmse: 10996.9\n",
      "[800]\ttraining's rmse: 9928.16\tvalid_1's rmse: 10654.7\n",
      "[900]\ttraining's rmse: 9586.09\tvalid_1's rmse: 10395.1\n",
      "[1000]\ttraining's rmse: 9295.03\tvalid_1's rmse: 10180\n",
      "-------- Fold 4 --------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2891\n",
      "[LightGBM] [Info] Number of data points in the train set: 124052, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91038.956438\n",
      "[100]\ttraining's rmse: 18243.4\tvalid_1's rmse: 18565.1\n",
      "[200]\ttraining's rmse: 14814.9\tvalid_1's rmse: 15282.4\n",
      "[300]\ttraining's rmse: 13198.8\tvalid_1's rmse: 13787.1\n",
      "[400]\ttraining's rmse: 12145.4\tvalid_1's rmse: 12866.1\n",
      "[500]\ttraining's rmse: 11366.1\tvalid_1's rmse: 12212.7\n",
      "[600]\ttraining's rmse: 10755.3\tvalid_1's rmse: 11717.9\n",
      "[700]\ttraining's rmse: 10289.4\tvalid_1's rmse: 11353.6\n",
      "[800]\ttraining's rmse: 9882.29\tvalid_1's rmse: 11042.1\n",
      "[900]\ttraining's rmse: 9531.64\tvalid_1's rmse: 10783.5\n",
      "[1000]\ttraining's rmse: 9226.15\tvalid_1's rmse: 10556\n",
      "Seed 9 OOF RMSE: 10526.949753809211\n",
      "최종 OOF RMSE: 10325.503742746154\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2891\n",
      "[LightGBM] [Info] Number of data points in the train set: 155065, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 91035.737149\n",
      "모델 학습이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터 분리\n",
    "y_train = train_data_encoded['target']\n",
    "X_train = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "# 사용할 특성 정의 (모든 열을 사용한다고 가정)\n",
    "features = X_train.columns.tolist()\n",
    "\n",
    "# 여러 시드 설정\n",
    "seeds = [i for i in range(10)]  # [0~9]\n",
    "n_folds = 5\n",
    "\n",
    "all_oofs = []\n",
    "all_total_predicts = []\n",
    "RMSEs = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"-------- Seed {seed} --------\")\n",
    "    \n",
    "    # StratifiedKFold 설정\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # y_train을 구간으로 나누어 분류 레이블 생성\n",
    "    cut_y_train = pd.cut(y_train, 1000, labels=False)\n",
    "    \n",
    "    seed_oofs = np.zeros(len(X_train))\n",
    "    \n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(kf.split(X_train, cut_y_train)):\n",
    "        print(f\"-------- Fold {fold_idx} --------\")\n",
    "        \n",
    "        kfold_X_train = X_train.iloc[train_idx][features]\n",
    "        kfold_y_train = y_train.iloc[train_idx]\n",
    "        kfold_X_valid = X_train.iloc[valid_idx][features]\n",
    "        kfold_y_valid = y_train.iloc[valid_idx]\n",
    "        \n",
    "        dtrain = lgb.Dataset(kfold_X_train, label=kfold_y_train)\n",
    "        dvalid = lgb.Dataset(kfold_X_valid, label=kfold_y_valid, reference=dtrain)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 1,\n",
    "            'random_state': seed\n",
    "        }\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[dtrain, dvalid],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                       lgb.log_evaluation(period=100)]\n",
    "        )\n",
    "        \n",
    "        seed_oofs[valid_idx] = model.predict(kfold_X_valid, num_iteration=model.best_iteration)\n",
    "    \n",
    "    all_oofs.append(seed_oofs)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_train, seed_oofs))\n",
    "    RMSEs.append(fold_rmse)\n",
    "    print(f\"Seed {seed} OOF RMSE: {fold_rmse}\")\n",
    "\n",
    "# 최종 OOF 예측 및 RMSE 계산\n",
    "final_oof_predictions = np.mean(all_oofs, axis=0)\n",
    "final_oof_rmse = np.sqrt(mean_squared_error(y_train, final_oof_predictions))\n",
    "print(f\"최종 OOF RMSE: {final_oof_rmse}\")\n",
    "\n",
    "# 전체 데이터로 최종 모델 학습\n",
    "dtrain_full = lgb.Dataset(X_train, label=y_train)\n",
    "final_model = lgb.train(params, dtrain_full, num_boost_round=model.best_iteration)\n",
    "print(\"모델 학습이 완료되었습니다.\")\n",
    "\n",
    "# 테스트 데이터에 대한 예측 (테스트 데이터가 있다고 가정)\n",
    "# X_test = test_data[features]\n",
    "# final_predictions = final_model.predict(X_test, num_iteration=final_model.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y_train = train_data_encoded['target']\n",
    "X_train = train_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "# Hold out split을 사용해 학습 데이터와 검증 데이터를 8:2 비율로 나누겠습니다.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895057,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project=\"apartment_price_prediction_gan\",\n",
    "    config={\n",
    "        \"g_learning_rate\": 0.01,\n",
    "        \"d_learning_rate\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 100,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 특성 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=wandb.config.batch_size)\n",
    "\n",
    "# 생성자 모델 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 판별자 모델 정의\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, 128),  # input_dim + 1 for price\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 및 옵티마이저 초기화\n",
    "generator = Generator(15)  # 15 features\n",
    "discriminator = Discriminator(15)  # 15 features\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=wandb.config.g_learning_rate)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=wandb.config.d_learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# wandb에 모델 가중치 로깅 설정\n",
    "wandb.watch(generator, log=\"all\", log_freq=10)\n",
    "wandb.watch(discriminator, log=\"all\", log_freq=10)\n",
    "\n",
    "# 훈련 함수\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        for real_features, real_prices in train_dataloader:\n",
    "            batch_size = real_features.size(0)\n",
    "            \n",
    "            # 판별자 훈련\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            fake_prices = generator(real_features)\n",
    "            real_combined = torch.cat((real_features, real_prices), 1)\n",
    "            fake_combined = torch.cat((real_features, fake_prices), 1)\n",
    "            \n",
    "            real_predictions = discriminator(real_combined)\n",
    "            fake_predictions = discriminator(fake_combined.detach())\n",
    "            \n",
    "            real_loss = loss_fn(real_predictions, torch.ones_like(real_predictions))\n",
    "            fake_loss = loss_fn(fake_predictions, torch.zeros_like(fake_predictions))\n",
    "            d_loss = real_loss + fake_loss\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # 생성자 훈련\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            fake_prices = generator(real_features)\n",
    "            fake_combined = torch.cat((real_features, fake_prices), 1)\n",
    "            fake_predictions = discriminator(fake_combined)\n",
    "            \n",
    "            g_loss = loss_fn(fake_predictions, torch.ones_like(fake_predictions))\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        # 검증\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "        val_g_loss = 0\n",
    "        val_d_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_features, val_prices in val_dataloader:\n",
    "                fake_prices = generator(val_features)\n",
    "                real_combined = torch.cat((val_features, val_prices), 1)\n",
    "                fake_combined = torch.cat((val_features, fake_prices), 1)\n",
    "                \n",
    "                real_predictions = discriminator(real_combined)\n",
    "                fake_predictions = discriminator(fake_combined)\n",
    "                \n",
    "                val_d_loss += (loss_fn(real_predictions, torch.ones_like(real_predictions)) + \n",
    "                               loss_fn(fake_predictions, torch.zeros_like(fake_predictions))).item()\n",
    "                val_g_loss += loss_fn(fake_predictions, torch.ones_like(fake_predictions)).item()\n",
    "\n",
    "        val_g_loss /= len(val_dataloader)\n",
    "        val_d_loss /= len(val_dataloader)\n",
    "\n",
    "        # wandb에 로그 기록\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_g_loss\": g_loss.item(),\n",
    "            \"train_d_loss\": d_loss.item(),\n",
    "            \"val_g_loss\": val_g_loss,\n",
    "            \"val_d_loss\": val_d_loss,\n",
    "        })\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train G Loss: {g_loss.item():.4f}, Train D Loss: {d_loss.item():.4f}, \"\n",
    "                  f\"Val G Loss: {val_g_loss:.4f}, Val D Loss: {val_d_loss:.4f}\")\n",
    "\n",
    "# 모델 훈련\n",
    "train(wandb.config.epochs)\n",
    "\n",
    "# 가격 예측\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    test_features = X_val_tensor[:5]  # 검증 데이터의 처음 5개 샘플 사용\n",
    "    predicted_prices = generator(test_features)\n",
    "    actual_prices = y_val_tensor[:5]\n",
    "\n",
    "print(\"실제 가격:\", actual_prices.numpy().flatten())\n",
    "print(\"예측 가격:\", predicted_prices.numpy().flatten())\n",
    "\n",
    "# wandb 실행 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Dump & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델 저장\n",
    "model_filename = 'final_model.pkl'\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(\"모델이 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 불러와졌습니다.\n",
      "테스트 데이터에 대한 예측이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "final_model = joblib.load(model_filename)\n",
    "print(\"모델이 불러와졌습니다.\")\n",
    "\n",
    "# 테스트 데이터 로드 및 전처리\n",
    "X_test = test_data_encoded.drop(['target'], axis=1)\n",
    "\n",
    "# 최적의 반복 횟수로 X_test에 대한 예측 수행\n",
    "real_test_pred = final_model.predict(X_test, num_iteration=final_model.best_iteration)\n",
    "\n",
    "print(\"테스트 데이터에 대한 예측이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([186820.0053029 , 281186.82354398, 342517.38743919, ...,\n",
       "        85472.85710872,  67089.15922442,  62128.35332279])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([177477.82484663, 266527.08642075, 332481.69242302, ...,\n",
       "        78170.09201322,  64776.73065125,  59279.72447601])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Permutation Importance 함수 정의\n",
    "def permutation_importance(model, X, y, metric, num_rounds=5):\n",
    "    baseline_score = metric(y, model.predict(X, num_iteration=model.best_iteration))\n",
    "    importances = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        scores = []\n",
    "        for _ in range(num_rounds):\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[col] = np.random.permutation(X_permuted[col])\n",
    "            score = metric(y, model.predict(X_permuted, num_iteration=model.best_iteration))\n",
    "            scores.append(score)\n",
    "        importances.append(np.mean(scores) - baseline_score)\n",
    "\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "# train 데이터와 test 데이터를 나눕니다.\n",
    "X_train_perm, X_test_perm, y_train_perm, y_test_perm = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 최종 모델 학습 (train 데이터로만 학습)\n",
    "dtrain_perm = lgb.Dataset(X_train_perm, label=y_train_perm)\n",
    "final_model_perm = lgb.train(best_params, dtrain_perm, num_boost_round=model.best_iteration)\n",
    "\n",
    "# Permutation Importance 계산\n",
    "feature_importances = permutation_importance(final_model_perm, X_test_perm, y_test_perm, mean_squared_error)\n",
    "\n",
    "print(\"Permutation Feature Importances:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# 결과 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importances['feature'], feature_importances['importance'])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
